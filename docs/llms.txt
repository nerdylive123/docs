# RunPod Complete Documentation

This file contains the entire RunPod documentation in a single text document for easy reference and LLM processing.

## Table of Contents

- API Endpoints (api/api-endpoints.md)
- Overview (api/custom-apis/_overview.md)
- Using Your API (api/custom-apis/_using-your-api-copy.md)
- Fine Tune a model (fine-tune/index.md)
- API keys (get-started/api-keys.md)
- Billing information (get-started/billing-information.md)
- Connect to RunPod (get-started/connect-to-runpod.md)
- Get started (get-started/get-started.md)
- Manage accounts (get-started/manage-accounts.md)
- Referral programs (get-started/referrals.md)
- Glossary (glossary.md)
- Burn Testing (hosting/burn-testing.md)
- Maintenance and reliability (hosting/maintenance-and-reliability.md)
- Overview (hosting/overview.md)
- partner-requirements.md (hosting/partner-requirements.md)
- Manage Pods with dstack on RunPod (integrations/dstack/dstack.md)
- Running RunPod on Mods (integrations/mods/mods.md)
- Integrations (integrations/overview.md)
- Running RunPod on SkyPilot (integrations/skypilot/skypilot.md)
- Complete Documentation (llms.md)
- Overview (overview.md)
- Choose a Pod (pods/choose-a-pod.md)
- Export data (pods/configuration/export-data.md)
- Expose ports (pods/configuration/expose-ports.md)
- Override public key (pods/configuration/override-public-keys.md)
- Use SSH (pods/configuration/use-ssh.md)
- Connect to a Pod (pods/connect-to-a-pod.md)
- Access Logs (pods/logs.md)
- Manage Pods (pods/manage-pods.md)
- Global Networking (pods/networking.md)
- Overview (pods/overview.md)
- Pod environment variables (pods/references/environment-variables.md)
- Savings plans (pods/savings-plans.md)
- Volumes (pods/storage/_volume.md)
- Create a network volume (pods/storage/create-network-volumes.md)
- Sync a volume to a cloud provider (pods/storage/sync-volumes.md)
- Transfer files (pods/storage/transfer-files.md)
- Storage types (pods/storage/types.md)
- Manage Pod Templates (pods/templates/manage-templates.md)
- Overview (pods/templates/overview.md)
- Secrets (pods/templates/secrets.md)
- API Endpoints (references/_api-endpoints.md)
- Serverless CPU types (references/cpu-types.md)
- FAQ (references/faq/faq.md)
- Manage Payment Card Declines (references/faq/manage-cards.md)
- GPU types (references/gpu-types.md)
- Leaked API Keys (references/troubleshooting/leaked-api-keys.md)
- Storage Full (references/troubleshooting/storage-full.md)
- 502 Errors (references/troubleshooting/troubleshooting-502-errors.md)
- Video resources (references/video-resources.md)
- Installing runpodctl (runpodctl/install-runpodctl.md)
- Overview (runpodctl/overview.md)
- Get started (runpodctl/projects/get-started.md)
- Managing Projects (runpodctl/projects/manage-projects.md)
- Projects (runpodctl/projects/overview.md)
- runpodctl (runpodctl/reference/runpodctl.md)
- Config (runpodctl/reference/runpodctl_config.md)
- Create (runpodctl/reference/runpodctl_create.md)
- Create Pod (runpodctl/reference/runpodctl_create_pod.md)
- Create Pods (runpodctl/reference/runpodctl_create_pods.md)
- Get (runpodctl/reference/runpodctl_get.md)
- Get Cloud (runpodctl/reference/runpodctl_get_cloud.md)
- Get Pod (runpodctl/reference/runpodctl_get_pod.md)
- Project (runpodctl/reference/runpodctl_project.md)
- Project Build (runpodctl/reference/runpodctl_project_build.md)
- Project Create (runpodctl/reference/runpodctl_project_create.md)
- Project Deploy (runpodctl/reference/runpodctl_project_deploy.md)
- Project Dev (runpodctl/reference/runpodctl_project_dev.md)
- Receive (runpodctl/reference/runpodctl_receive.md)
- Remove (runpodctl/reference/runpodctl_remove.md)
- Remove Pod (runpodctl/reference/runpodctl_remove_pod.md)
- Remove Pods (runpodctl/reference/runpodctl_remove_pods.md)
- Send (runpodctl/reference/runpodctl_send.md)
- Ssh (runpodctl/reference/runpodctl_ssh.md)
- Ssh Add Key (runpodctl/reference/runpodctl_ssh_add-key.md)
- Ssh List Keys (runpodctl/reference/runpodctl_ssh_list-keys.md)
- Start (runpodctl/reference/runpodctl_start.md)
- Start Pod (runpodctl/reference/runpodctl_start_pod.md)
- Stop (runpodctl/reference/runpodctl_stop.md)
- Stop Pod (runpodctl/reference/runpodctl_stop_pod.md)
- Update (runpodctl/reference/runpodctl_update.md)
- Version (runpodctl/reference/runpodctl_version.md)
- Endpoints (sdks/go/endpoints.md)
- Overview (sdks/go/overview.md)
- Configurations (sdks/graphql/configurations.md)
- Manage Endpoints (sdks/graphql/manage-endpoints.md)
- Manage Templates (sdks/graphql/manage-pod-templates.md)
- Manage Pods (sdks/graphql/manage-pods.md)
- Endpoints (sdks/javascript/endpoints.md)
- Overview (sdks/javascript/overview.md)
- Overview (sdks/overview.md)
- Loggers (sdks/python/_loggers.md)
- API Wrapper (sdks/python/apis.md)
- Endpoints (sdks/python/endpoints.md)
- Overview (sdks/python/overview.md)
- Get started (serverless/endpoints/get-started.md)
- Job operations (serverless/endpoints/job-operations.md)
- Manage Endpoints (serverless/endpoints/manage-endpoints.md)
- Overview (serverless/endpoints/overview.md)
- Send a request (serverless/endpoints/send-requests.md)
- Get started with Endpoints (serverless/get-started.md)
- Deploying with GitHub (serverless/github-integration.md)
- Overview (serverless/overview.md)
- Quick Deploys (serverless/quick-deploys.md)
- Endpoint configurations (serverless/references/endpoint-configurations.md)
- Job states (serverless/references/job-states.md)
- Endpoint operations (serverless/references/operations.md)
- Package and deploy an image (serverless/workers/deploy/deploy.md)
- Cleanup (serverless/workers/development/cleanup.md)
- Concurrency (serverless/workers/development/concurrency.md)
- Debugging (serverless/workers/development/debugger.md)
- Use environment variables (serverless/workers/development/environment-variables.md)
- Test locally (serverless/workers/development/local-testing.md)
- Local Server Flags (serverless/workers/development/overview.md)
- Test response time (serverless/workers/development/test-response-times.md)
- Input validation (serverless/workers/development/validator.md)
- Additional controls (serverless/workers/handlers/handler-additional-controls.md)
- Asynchronous Handler (serverless/workers/handlers/handler-async.md)
- Concurrent Handlers (serverless/workers/handlers/handler-concurrency.md)
- Handling Errors (serverless/workers/handlers/handler-error-handling.md)
- Generator Handler (serverless/workers/handlers/handler-generator.md)
- Overview (serverless/workers/handlers/overview.md)
- Overview (serverless/workers/overview.md)
- Configurable Endpoints (serverless/workers/vllm/configurable-endpoints.md)
- Environment variables (serverless/workers/vllm/environment-variables.md)
- Get started (serverless/workers/vllm/get-started.md)
- OpenAI compatibility (serverless/workers/vllm/openai-compatibility.md)
- Overview (serverless/workers/vllm/overview.md)
- Dockerfile (tutorials/introduction/containers/create-dockerfiles.md)
- Docker commands (tutorials/introduction/containers/docker-commands.md)
- Containers overview (tutorials/introduction/containers/overview.md)
- Persist data outside of containers (tutorials/introduction/containers/persist-data.md)
- Overview (tutorials/introduction/overview.md)
- Migrate Your Banana Images to RunPod (tutorials/migrations/banana/images.md)
- Overview (tutorials/migrations/banana/overview.md)
- Migrate Your Banana Text to RunPod (tutorials/migrations/banana/text.md)
- Overview (tutorials/migrations/cog/overview.md)
- Overview (tutorials/migrations/openai/overview.md)
- Overview (tutorials/overview.md)
- Build Docker Images on Runpod with Bazel (tutorials/pods/build-docker-images.md)
- How To Connect to a Pod Instance through VSCode (tutorials/pods/connect-to-vscode.md)
- Fine tune an LLM with Axolotl on RunPod (tutorials/pods/fine-tune-llm-axolotl.md)
- Run Fooocus in Jupyter Notebook (tutorials/pods/run-fooocus.md)
- Set up Ollama on your GPU Pod (tutorials/pods/run-ollama.md)
- Run your first Fast Stable Diffusion with Jupyter Notebook (tutorials/pods/run-your-first.md)
- Getting Started with RunPod's Serverless Python SDK (tutorials/sdks/python/101/00_hello.md)
- Creating and Testing a RunPod Serverless Function with Local Server (tutorials/sdks/python/101/01_local-server-testing.md)
- Building a Generator Handler for Text-to-Speech Simulation (tutorials/sdks/python/101/02_generator.md)
- Building an Async Generator Handler for Weather Data Simulation (tutorials/sdks/python/101/03_async.md)
- Implementing Error Handling and Logging in RunPod Serverless Functions (tutorials/sdks/python/101/04_error.md)
- Aggregating Outputs in RunPod Serverless Functions (tutorials/sdks/python/101/05_aggregate.md)
- Using Hugging Face Models with RunPod (tutorials/sdks/python/102/07-huggingface-models.md)
- Text-to-Image Generation with Stable Diffusion on RunPod (tutorials/sdks/python/102/08-stable-diffusion-text-to-image.md)
- Running Code Locally (tutorials/sdks/python/102/_handler-basics.md)
- Random Number Generator with RunPod (tutorials/sdks/python/102/_random-number-generator.md)
- Introduction to RunPod Python SDK (tutorials/sdks/python/get-started/01-introduction.md)
- Prerequisites (tutorials/sdks/python/get-started/01-prerequisites.md)
- Hello World with RunPod (tutorials/sdks/python/get-started/02-hello-world.md)
- Running Code Locally (tutorials/sdks/python/get-started/03-running-locally.md)
- Run an Ollama Server on a RunPod CPU (tutorials/serverless/cpu/run-ollama-inference.md)
- Generate images with SDXL Turbo (tutorials/serverless/gpu/generate-sdxl-turbo.md)
- Run Google's Gemma model (tutorials/serverless/gpu/run-gemma-7b.md)
- Run your first serverless endpoint with Stable Diffusion (tutorials/serverless/gpu/run-your-first.md)


# api-endpoints.md

File: api/api-endpoints.md

:::note

You will need a RunPod API key which can be generated under your user settings.
This API key will identify you for billing purposes, so guard it well!
You must retrieve your results via the status endpoint within 30 minutes.
We don't keep your inputs or outputs longer than that to protect your privacy!

:::

API Endpoints are Endpoints managed by RunPod that you can use to interact with your favorite models without managing the pods yourself.
These Endpoints are available to all users.

## Overview

The API Endpoint implementation works asynchronously as well as synchronous.

Let's take a look at the differences between the two different implementations.

### Asynchronous Endpoints

Asynchronous endpoints are useful for long-running jobs that you don't want to wait for. You can submit a job and then check back later to see if it's done.
When you fire an Asynchronous request with the API Endpoint, your input parameters are sent to our endpoint and you immediately get a response with a unique job ID.
You can then query the response by passing the job ID to the status endpoint. The status endpoint will give you the job results when completed.

### Synchronous Endpoints

Synchronous endpoints are useful for short-running jobs that you want to wait for.
You can submit a job and get the results back immediately.

Let's take the Stable Diffusion v1 inference endpoint, for example.

### Start your job

You would first make a request like the following (remember to replace the "xxxxxx"s with your real API key:

```curl
curl -X POST https://api.runpod.ai/v2/stable-diffusion-v1/run \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' \
    -d '{"input": {"prompt": "a cute magical flying dog, fantasy art drawn by disney concept artists"}}'
```

You would get an immediate response that looks like this:

```json
{
  "id": "c80ffee4-f315-4e25-a146-0f3d98cf024b",
  "status": "IN_QUEUE"
}
```

In this example, your job ID would be "c80ffee4-f315-4e25-a146-0f3d98cf024b".
You get a new one for each job, and it is a unique identifier for your job.

### Check the status of your job

You haven't gotten any output, so you must make an additional call to the status endpoint after some time. Your status endpoint uses the job ID to route to the correct job status. In this case, the status endpoint is

```command
https://api.runpod.ai/v1/stable-diffusion-v1/status/c80ffee4-f315-4e25-a146-0f3d98cf024b
```

Note how the last part of the URL is your job ID. You could request that endpoint like so.
Remember to use your API key for this request too!

```curl
curl https://api.runpod.ai/v2/stable-diffusion-v1/status/c80ffee4-f315-4e25-a146-0f3d98cf024b \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
```

If your job hasn't been completed, you may get something that looks like this back:

```json
{
  "delayTime": 2624,
  "id": "c80ffee4-f315-4e25-a146-0f3d98cf024b",
  "input": {
    "prompt": "a cute magical flying dog, fantasy art drawn by disney concept artists"
  },
  "status": "IN_PROGRESS"
}
```

This means to wait a bit longer before you query the status endpoint again.

## Get completed job status

Eventually, you will get the final results of your job. They would look something like this:

```json
{
  "delayTime": 123456, // (milliseconds) time in queue
  "executionTime": 1234, // (milliseconds) time it took to complete the job
  "gpu": "24", // gpu type used to run the job
  "id": "c80ffee4-f315-4e25-a146-0f3d98cf024b",
  "input": {
    "prompt": "a cute magical flying dog, fantasy art drawn by disney concept artists"
  },
  "output": [
    {
      "image": "https://job.results1",
      "seed": 1
    },
    {
      "image": "https://job.results2",
      "seed": 2
    }
  ],
  "status": "COMPLETED"
}
```

:::note

You must retrieve your results via the status endpoint within 1 hour. We do not keep your inputs or outputs longer than that to protect your privacy!

:::

### Get your stuff

Note how you don't get the images directly in the output. The output contains the URLs to the cloud storage that will let you download each image.

You've successfully generated your first images with our Stable Diffusion API!

### Rate Limits

Rate limits are enforced per-user basis.
Exceeding limits returns a `429` error.

`/run` - 1000 requests/10s, max 200 concurrent
`/runsync` - 2000 requests/10s, max 400 concurrent

For more information, see [Job operations](/serverless/references/operations).

---

# _overview.md

File: api/custom-apis/_overview.md

You can create Custom APIs for your specific use cases.
If you have a custom use case, you can use RunPod's custom API support to stand up your serverless API.
You can bring your own container image, and RunPod will handle the scaling and other aspects.

To create a custom API, you can navigate to the RunPod Serverless console and click "New Template" to add your container image. Once the template is created, you can convert it into an API endpoint by navigating to the APIs section and clicking "New API".

Please note that running 1 minimum worker is great for debugging purposes, but you will be charged for that worker whether or not you are making requests to your endpoint.

Once everything is set up, you can invoke your API using the "run" endpoint on your API dashboard. RunPod services are currently asynchronous, so you must use the "status" endpoint to get the status/results of each run using the ID present in the run response payload.

---

# _using-your-api-copy.md

File: api/custom-apis/_using-your-api-copy.md

Once everything above is configured, you will be able to invoke your API using the "run" endpoint on your API dashboard. Our services are currently asynchronous, so you must use the "status" endpoint to get the status/results of each run using the ID present in the run response payload. You can also pass in a webhook URL when invoking "run" within the JSON body.

Our own APIs are built using the same tools, so you can take a look at the RunPod API overview. The only difference is that your custom API endpoint only accepts requests using your own account's API key, not any RunPod API key.

We offer two different kinds of run mechanisms: synchronous responses and asynchronous responses.

## Running your API

### /runsync

<!-- dprint-ignore-start -->
```curl cURL
curl -X POST https://api.runpod.ai/v2/<your-api-id>/runsync \
-H 'Content-Type: application/json'                             \
-H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'    \
-d '{"input": {<your-input-json}}'
```
```python
# this requires the installation of runpod-python
# with `pip install runpod-python` beforehand

import runpod

runpod.api_key = "xxxxxxxxxxxxxxxxxxxxxx"  # you can find this in settings

endpoint = runpod.Endpoint("ENDPOINT_ID")

run_request = endpoint.run_sync({"your_model_input_key": "your_model_input_value"})
```
<!-- dprint-ignore-end -->

here's a possible example request (taken from our stable diffusion image)

<!-- dprint-ignore-start -->
```curl cURL
curl -X POST https://api.runpod.ai/v2/<your-api-id>/runsync \
-H 'Content-Type: application/json'                             \
-H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'    \
-d '{"input": {"prompt": "a cute magical flying dog, fantasy art drawn by disney concept artists"}}'
```
```python
# this requires the installation of runpod-python
# with `pip install runpod-python` beforehand

import runpod

runpod.api_key = "xxxxxxxxxxxxxxxxxxxxxx"  # you can find this in settings

endpoint = runpod.Endpoint("ENDPOINT_ID")

run_request = endpoint.run_sync(
    {"prompt": "a cute magical flying dog, fantasy art drawn by disney concept artists"}
)

print(run_request)
```
<!-- dprint-ignore-end -->

this should give a direct response if the code runs for \< 90 seconds, or else it'll give a status response (which you can see below)

**Sample response**

```json
{
  "delayTime": 123456, // (milliseconds) time in queue
  "executionTime": 1234, // (milliseconds) time it took to complete the job
  "gpu": "24", // gpu type used to run the job
  "id": "c80ffee4-f315-4e25-a146-0f3d98cf024b",
  "input": {
    "prompt": "a cute magical flying dog, fantasy art drawn by disney concept artists"
  },
  "output": [
    {
      "image": "https://job.results1",
      "seed": 1
    },
    {
      "image": "https://job.results2",
      "seed": 2
    }
  ],
  "status": "COMPLETED"
}
```

### /run

<!-- dprint-ignore-start -->
```curl cURL
curl -X POST https://api.runpod.ai/v2/<your-api-id>/run \
-H 'Content-Type: application/json'                             \
-H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'    \
-d '{"input": {<your-input-json}}'
```
```python
# this requires the installation of runpod-python
# with `pip install runpod-python` beforehand

import runpod

runpod.api_key = "xxxxxxxxxxxxxxxxxxxxxx"  # you can find this in settings

endpoint = runpod.Endpoint("ENDPOINT_ID")

run_request = endpoint.run({"your_model_input_key": "your_model_input_value"})

print(run_request.status())
```
<!-- dprint-ignore-end -->

here's a possible example request

<!-- dprint-ignore-start -->
```curl cURL
curl -X POST https://api.runpod.ai/v2/<your-api-id>/run \
-H 'Content-Type: application/json'                             \
-H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'    \
-d '{"input": {"prompt": "a cute magical flying dog, fantasy art drawn by disney concept artists"}}'
```
```python
# this requires the installation of runpod-python
# with `pip install runpod-python` beforehand

import runpod

runpod.api_key = "xxxxxxxxxxxxxxxxxxxxxx"  # you can find this in settings

endpoint = runpod.Endpoint("ENDPOINT_ID")

run_request = endpoint.run(
    {"prompt": "a cute magical flying dog, fantasy art drawn by disney concept artists"}
)

print(run_request.status())
```
<!-- dprint-ignore-end -->

running your api via **/run** runs the code asynchronously, here's a sample response

**sample response (for curl)**

```json
{
  "id": "c80ffee4-f315-4e25-a146-0f3d98cf024b",
  "status": "IN_QUEUE"
}
```

### /status

**Sample request**

<!-- dprint-ignore-start -->
```Text cURL
curl https://api.runpod.ai/v2/<your-api-id>/status/<your-status-id>
```
```python Start a job and return a status
# this requires the installation of runpod-python
# with `pip install runpod-python` beforehand

import runpod

runpod.api_key = "xxxxxxxxxxxxxxxxxxxxxx"  # you can find this in settings

endpoint = runpod.Endpoint("ENDPOINT_ID")

run_request = endpoint.run(
    {"prompt": "a cute magical flying dog, fantasy art drawn by disney concept artists"}
)

print(run_request.status())
```
```python Get the status of a running job
# Prerequisite: Install runpod-python using `pip install runpod-python`
import runpod

runpod.api_key = "xxxxxxxxxxxxxxxxxxxxxx"  # Replace with your API key
client = runpod.endpoint.runner.RunPodClient()


job = runpod.endpoint.Job(
    endpoint_id="your_endpoint_id", job_id="your_job_id", client=client
)

print(job.status())
```
<!-- dprint-ignore-end -->

**sample response (for job in progress)**

```json JSON
{
  "delayTime": 2624,
  "id": "c80ffee4-f315-4e25-a146-0f3d98cf024b",
  "input": {
    "prompt": "a cute magical flying dog, fantasy art drawn by disney concept artists"
  },
  "status": "IN_PROGRESS"
}
```

**sample response (for completed job)**

```json JSON
{
  "delayTime": 123456, // (milliseconds) time in queue
  "executionTime": 1234, // (milliseconds) time it took to complete the job
  "gpu": "24", // gpu type used to run the job
  "id": "c80ffee4-f315-4e25-a146-0f3d98cf024b",
  "input": {
    "prompt": "a cute magical flying dog, fantasy art drawn by disney concept artists"
  },
  "output": [
    {
      "image": "https://job.results1",
      "seed": 1
    },
    {
      "image": "https://job.results2",
      "seed": 2
    }
  ],
  "status": "COMPLETED"
}
```

---

# index.md

File: fine-tune/index.md

This guide explains how to Fine Tune a large language model with RunPod.

## Before you begin

Make sure you have:

- An account with access to the Fine Tuning feature
- (Optional) A HuggingFace access token for gated models

## Select a base model

1. Go to the **Fine Tuning** section in the sidebar.
2. In the **Base Model** field, enter the Hugging Face model ID you want to Fine Tune. 
    For example: `NousResearch/Meta-Llama-3-8B`

3. If your selected model is gated (requires special access):
   1. Generate a Hugging Face token with the necessary permissions.
   2. Add the token in the designated field.

## Select a dataset (Optional)

1. Choose a dataset from Hugging Face for Fine Tuning.

   Example: `tatsu-lab/alpaca`

2. Enter the dataset identifier in the **Dataset** field.

:::note 

For a list of datasets, see the [Hugging Face dataset](https://huggingface.co/datasets?task_categories=task_categories:text-generation&sort=trending).

:::

## Deploy the Fine Tuning pod

1. Click **Deploy the Fine Tuning Pod**.
2. Select an appropriate GPU instance based on your model's requirements:
   - For smaller models: Choose GPUs with less memory
   - For larger models/datasets: Choose GPUs with higher memory capacity
3. The system will deploy your pod and initialize the container.
4. Monitor the system logs for deployment progress.
5. Look for the success message: `"You've successfully configured your training environment!"`

## Connect to your training environment

After your Fine Tuning pod is deployed and active:

1. Go to your Fine Tuning pod dashboard.
2. Click **Connect** to view connection options:
   - **Jupyter Notebook**: Launch a browser-based notebook interface
   - **Web Terminal**: Open a terminal in your browser
   - **SSH**: Connect from your local machine terminal

     Note: To use SSH, ensure you've added your public SSH key in your account settings. The system automatically adds your key to the pod's `authorized_keys` file.

## Configure your environment

When you connect to your environment, you'll find this directory structure in `/workspace/fine-tuning/`:

- `examples/`: Contains sample configurations and scripts
- `outputs/`: Stores training results and model outputs
- `config.yaml`: Defines training parameters for your model

The system generates an initial `config.yaml` based on your selected base model and dataset.

## Review and modify the configuration

1. Open the `config.yaml` file using a text editor:

   ```bash
   nano config.yaml
   ```

2. Review the configuration parameters and modify them as needed.

   Note: It's recommended to adjust parameters based on your specific use case.

Example configuration:

```yaml
base_model: NousResearch/Meta-Llama-3.1-8B

load_in_8bit: false
load_in_4bit: false
strict: false

datasets:
  - path: tatsu-lab/alpaca
    type: alpaca
dataset_prepared_path: last_run_prepared
val_set_size: 0.05
output_dir: ./outputs/out

sequence_len: 8192
sample_packing: true
pad_to_sequence_len: true

wandb_project:
wandb_entity:
wandb_watch:
wandb_name:
wandb_log_model:

gradient_accumulation_steps: 8
micro_batch_size: 1
num_epochs: 1
optimizer: paged_adamw_8bit
lr_scheduler: cosine
learning_rate: 2e-5

train_on_inputs: false
group_by_length: false
bf16: auto
fp16:
tf32: false

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
early_stopping_patience:
resume_from_checkpoint:
logging_steps: 1
xformers_attention:
flash_attention: true

warmup_steps: 100
evals_per_epoch: 2
eval_table_size:
saves_per_epoch: 1
debug:
deepspeed:
weight_decay: 0.0
fsdp:
fsdp_config:
special_tokens:
  pad_token: <|end_of_text|>
```

:::note

The `config.yaml` file contains all the hyper parameters needed to fine tune a model.
You will make your adjustments in the `config.yaml`.

Changes to this and running the `axolotl train config.yaml` command might require iteration.

:::

For more configuration examples, see the [Axolotl examples repository](https://github.com/axolotl-ai-cloud/axolotl/tree/main/examples).

## Start the Fine Tuning process

When your configuration is ready:

1. Run the following command to start Fine Tuning:

   ```bash
   axolotl train config.yaml
   ```

2. The training process will begin, and progress will be displayed in your terminal.

When you've finished, push your changes to Hugging Face:

1. Log in to Hugging Face:

   ```bash
   huggingface-cli login
   ```

2. Create a new repository on Hugging Face if needed.

3. Push your model to Hugging Face:

   ```bash
   huggingface-cli upload <your-username>/<model-name> ./output
   ```

Replace `<your-username>` with your Hugging Face username and `<model-name>` with what you want to name your Fine Tuned model.

## Resources

For more information, see:

- [Axolotl Documentation](https://github.com/OpenAccess-AI-Collective/axolotl)

---

# api-keys.md

File: get-started/api-keys.md

API keys authenticate requests to RunPod.
You can generate an API key with **Read/Write** permission, **Restricted** permission, or **Read Only** permission.

:::note

Legacy API keys generated before November 11, 2024 have either Read/Write or Read Only access to GraphQL based on what was set for that key. All legacy keys have full access to AI API. To improve security, generate a new key with Restricted permission and select the minimum permission needed for your use case.

:::

## Generate

To create an API key:

1. From the console, select **Settings**.
2. Under **API Keys**, choose **+ Create API Key**.
3. Select the permission. If you choose **Restricted** permission, you can customize access for each API:
   - **None**: No access
   - (AI API only) **Restricted**: Custom access to specific endpoints. No access is default.
   - **Read/Write**: Full access
   - **Read Only**: Read access without write access
   <!-- dprint-ignore-start -->
    :::warning

   Select the minimum permission needed for your use case. Only allow full access to GraphQL when absolutely necessary for automations like creating or managing RunPod resources outside of Serverless endpoints.

   :::
   <!-- dprint-ignore-end -->
4. Choose **Create**.

:::note

Once your API key is generated, keep it secure. Treat it like a password and avoid sharing it in insecure environments.

:::

## Edit permission

To edit an API key:

1. From the console, select **Settings**.
2. Under **API Keys**, select the pencil icon and select the permission.
3. Choose **Update**.

## Disable

To disable an API key:

1. From the console, select **Settings**.
2. Under **API Keys**, select the toggle and select **Yes**.

## Enable

To enable an API key:

1. From the console, select **Settings**.
2. Under **API Keys**, select the toggle and select **Yes**.

## Revoke

To delete an API key:

1. From the console, select **Settings**.
2. Under **API Keys**, select the trash can icon and select **Revoke Key**.

---

# billing-information.md

File: get-started/billing-information.md

All billing, including per-hour compute and storage billing, is charged per minute.

For more information on billing questions, see [Billing FAQ](docs/references/faq/faq.md#billing).

### Payment methods

RunPod accepts several payment methods for funding your account:

1. **Credit Card**: You can use your credit card to fund your RunPod account. However, be aware that card declines are more common than you might think, and the reasons for them might not always be clear. If you're using a prepaid card, it's recommended to deposit in transactions of at least $100 to avoid unexpected blocks due to Stripe's minimums for prepaid cards.
   For more information, review [cards accepted by Stripe](https://stripe.com/docs/payments/cards/supported-card-brands?ref=blog.runpod.io).

<!-- [source](https://blog.runpod.io/how-to-manage-funding-your-runpod-account#my-card-keeps-getting-declined) -->

2. **Crypto Payments**: RunPod also accepts crypto payments. It's recommended to set up a [crypto.com](https://crypto.com/?ref=blog.runpod.io) account and go through any KYC checks they may require ahead of time. This provides an alternate way of funding your account in case you run into issues with card payment.

<!-- [source](https://blog.runpod.io/how-to-manage-funding-your-runpod-account#crypto-payments) -->

3. **Business Invoicing**: For large transactions (over $5,000), RunPod offers business invoicing through ACH, credit card, Coinbase, and local and international wire transfers.

<!-- [source](https://blog.runpod.io/how-to-manage-funding-your-runpod-account#invoicing) -->

If you're having trouble with your card payments, you can contact [RunPod support](https://www.runpod.io/contact) for assistance.

---

# connect-to-runpod.md

File: get-started/connect-to-runpod.md

There are many ways to interact with RunPod:

- Web interface
- CLI
- SDKs

## Web interface

[Create an account](/get-started/connect-to-runpod) and then log into the web interface at the following address: [runpod.io/console/signup](https://www.runpod.io/console/signup).

## CLI

You can use RunPod's CLI [runpodctl](https://github.com/runpod/runpodctl) to manage Pods and for development.

All Pods come with `runpodctl` installed with a Pod-scoped API key, which makes managing your Pods easier through the command line.

## SDKs

RunPod provides SDKs for the following programming languages:

- [GraphQL](/sdks/graphql/manage-pods)
- [JavaScript](/sdks/javascript/overview)
- [Python](/sdks/python/overview)

---

# get-started.md

File: get-started/get-started.md

Welcome to RunPod, a versatile cloud computing platform tailored for artificial intelligence, machine learning applications, and broad computing needs.

### Creating an account

Start by creating your own RunPod account to manage and access various resources.

[Sign up here](https://www.runpod.io/console/signup) to create an account. Once registered, log in to start using our services.

### Adding funds to your Account

Before deploying resources, you'll need to add funds to your account. RunPod accepts payments via credit card or cryptocurrency.

[Add funds here](https://www.runpod.io/console/user/billing).

For more details on payment methods and billing, visit our [Billing Information](/get-started/billing-information) page or check the [FAQ](/references/faq/manage-cards).

### Running your resources

Once your account is funded, you're ready to deploy computing resources. Follow this step-by-step guide to launch your first pod.

<iframe 
  src="https://app.tango.us/app/embed/e494032e-b628-45d6-a134-fd86bb76b668"
  sandbox="allow-scripts allow-top-navigation-by-user-activation allow-popups allow-same-origin"
  security="restricted"
  title="Deploy your first pod"
  width="100%" 
  height="600px" 
  referrerpolicy="strict-origin-when-cross-origin"
  frameborder="0"
  allowfullscreen
></iframe>

---

# manage-accounts.md

File: get-started/manage-accounts.md

You will need to create an account or get invited by a team member to use RunPod.

## Create an account

Sign up for an account at [RunPod.io](https://www.runpod.io/console/signup).

### Convert personal account to a team account

You can convert a personal account to a team account at anytime.

1. From the console, select **Convert to Team Account**.
2. Set a team name and confirm the conversion.

## Get invited by a team member

1. Accept the link sent by a team member.
2. Select **Join Team**.

For information on how to send an invitation, see [Invite a user](#invite-a-user).

### Invite a user

To invite a user to your team on RunPod, you'll need a team account.

1. To invite users, navigate to your Team page, and select the "Invite New Member" button at the top of the "Members" section.
2. Select the role you want to provide the user.
3. After you create the invite, you can copy the invite link from the pending invites section.
4. Send a user your invite link and they will be able to go to it to join your team.

## Role types

The following roles and permissions are available to you:

### Basic role

Limited access, primarily for account usage and existing pod connections.

**Permissions**:

- Use the account.
- Connect to and use Pods.
- Restricted from viewing billing information.
- No permissions to start, stop, or create Pods.

### Billing role

Specialized role focused on managing billing aspects.

**Permissions**:

- Access and manage billing information.
- Restricted from other account features and Pod management.

### Dev role

Enhanced privileges suitable for development tasks.

**Permissions**:

- All "Basic" role permissions (use account, connect to Pods).
- Start, stop, and create Pods.
- No access to billing information.

### Admin role

Full control over the account, ideal for administrators.

**Permissions**:

- Complete access to all account features and settings.
- Manage billing and access billing information.
- Start, stop, and create Pods.
- Modify account settings and user permissions.
- Full control over all account resources and users.

## Audit logs

RunPod includes audit logs to help you understand which actions were used.
Go to the [Audit logs](https://www.runpod.io/console/user/audit-logs) settings.

You can view and filter the audit logs by date range, user, resource, resource ID, and action.

---

# referrals.md

File: get-started/referrals.md

RunPod offers two referral programs and a template program that allow users to earn additional revenue in the form of RunPod Credits. This document provides an overview of these programs and instructions on how to participate.

## Referral Programs

### 1. Serverless Referral Program (BETA)

The Serverless Referral Program rewards the referrer with RunPod Credits when the referred user spends a certain amount on Serverless.

#### Rewards

- Referrer: Earns 5% in RunPod Credits for Serverless spend

#### Eligibility

- Till December 31st, 2024.

### 2. Pod Referral Program (BETA)

The Pod Referral Program allows users to earn a percentage of the money spent by their referred users for the lifetime of their account.

#### Rewards

- Referrer: Earns 3% in RunPod Credits for every penny spent by the referred user on GPU Pods.

#### Example

- If 20 referrals spend $100 each on GPU Pods, the referrer earns $60.

#### Eligibility

- Till December 31st, 2024.

### 3. Template Referral Program (BETA)

The Template Program allows users to earn a percentage of the money spent by users who use their Pod Template.

#### Rewards

- Template Creator: Earns 1% for runtime in RunPod Credits for every penny spent using their template.

#### Example

- If 20 users use a Pod Template at $0.54/hr for a week, the template creator earns $18.14.

#### Eligibility

- The template must have at least 1 day of runtime.

## How to Participate

1. Access your [referral dashboard](https://www.runpod.io/console/user/referrals).
2. Locate your unique referral link. For example, `https://runpod.io?ref=5t99c9je`.
3. Share your referral link with potential users.

#### Important Notes

1. Commissions are calculated as a percentage of credits used, not credits purchased. So if your friend purchases $1000 in credits, you won't receive a commission until they use them.
2. Referrals are only valid when the referred user creates a new account using your referral link. If your referral already had a RunPod account, you won't receive a commission for referring them.

## Support

If you have any questions or need assistance with the referral or template programs, [please contact](https://contact.runpod.io/hc/en-us/requests/new) the RunPod support team.

RunPod allows referrers to keep the earnings made before activation once they meet the eligibility criteria and are accepted into the program.

---

# glossary.md

File: glossary.md

# Serverless

Serverless is a pay-per-second computing solution designed for dynamic autoscaling in production environments. It automatically adjusts computational resources based on your request traffic, ensuring cost-effective usage.

We offer both GPU and CPU serverless options:

- GPU Serverless: Each worker is equipped with a dedicated GPU, ideal for AI/ML workloads.
- CPU Serverless: Workers come with high-clock-speed CPU cores, suited for general-purpose workloads.

## Worker

A single compute resource that processes requests. Each endpoint can have multiple workers, enabling parallel processing of multiple requests simultaneously

### Total Workers

Total Workers refers to the maximum number of workers available to your account. The sum of the max workers assigned across all your endpoints cannot exceed this limit. If you run out of total workers, please reach out to us by [creating a support ticket](https://contact.runpod.io/).

### Max Workers

Max workers set the upper limit on the number of workers your endpoint can run simultaneously.

Default: 3

### Active (Min) Workers

“Always on” workers. Setting active workers to 1 or more ensures that a worker is always ready to respond to job requests without cold start delays.

Default: 0

:::note

Active workers incur charges as soon as you enable them (set to >0), but they come with a discount of up to 30% off the regular price.

:::

### Flex Workers

Flex Workers are “sometimes on” workers that help scale your endpoint during traffic surges. They are often referred to as idle workers since they spend most of their time in an idle state. Once a flex worker completes a job, it transitions to idle or sleep mode to save costs. You can adjust the idle timeout to keep them running a little longer, reducing cold start delays when new requests arrive.

Default: Max Workers(3) - Active Workers(0) = 3

### Extra Workers

RunPod caches your worker’s Docker image on our host servers, ensuring faster scalability. If you experience a traffic spike, you can increase the max number of workers, and extra workers will be immediately added as part of the flex workers to handle the increased demand.

Default: 2

### Worker States

#### Initializing

When you create a new endpoint or release an update, RunPod needs to download and prepare the Docker image for your workers. During this process, workers remain in an initializing state until they are fully ready to handle requests.

#### Idle

A worker is ready to handle new requests but is not actively processing any. There is no charge while a worker is idle.

#### Running

A running worker is actively processing requests, and you are billed every second it runs. If a worker runs for less than a full second, it will be rounded up to the next whole second. For example, if a worker runs for 2.5 seconds, you will be billed for 3 seconds.

#### Throttled

Sometimes, the machine where the worker is cached may be fully occupied by other workloads. In this case, the worker will show as throttled until resources become available.

#### Outdated

When you update your endpoint configuration or deploy a new Docker image, existing workers are marked as outdated. These workers will continue processing their current jobs but will be gradually replaced through a rolling update, replacing 10% of max workers at a time. This ensures a smooth transition without disrupting active workloads.

#### Unhealthy

When your container crashes, it’s usually due to a bad Docker image, an incorrect start command, or occasionally a machine issue. When this happens, the worker is marked as unhealthy. The system will automatically retry the unhealthy worker after 1 hour, using exponential backoff for up to 7 days. Be sure to check the container logs and fix any issues causing the crash to prevent repeated failures.

## Endpoint

An Endpoint refers to a specific REST API (URL) provided by RunPod that your applications or services can interact with. These endpoints enable standard functionality for submitting jobs and retrieving their outputs.

## Handler

A Handler is a function you create that takes in submitted inputs, processes them (like generating images, text, or audio), and returns the final output.

## Serverless [SDK](https://github.com/runpod/runpod-python?tab=readme-ov-file#--serverless-worker-sdk)

A Python package used when creating a handler function. This package helps your code receive requests from our serverless system, triggers your handler function to execute, and returns the function’s result back to the serverless system.

## Endpoint Settings

### Idle Timeout

The amount of time a worker remains running after completing its current request. During this period, the worker stays active, continuously checking the queue for new jobs, and continues to incur charges. If no new requests arrive within this time, the worker will go to sleep.

Default: 5 seconds

### Execution Timeout

The maximum time a job can run before the system terminates the worker. This prevents “bad” jobs from running indefinitely and draining your credit.

You can disable this setting, but we highly recommend keeping it enabled. The default maximum value is 24 hours, but if you need a longer duration, you can use job TTL to override it.

Default: 600 seconds (10 minutes)

### Job [TTL](/serverless/endpoints/send-requests#execution-policies)(Time-To-Live)

Defines the maximum time a job can remain in the queue before it's automatically terminated. This parameter ensures that jobs don't stay in the queue indefinitely. You should set this if your job runs longer than 24 hours or if you want to remove job data as soon as it is finished.

Minimum value: 10,000 milliseconds (10 seconds)
Default value: 86,400,000 milliseconds (24 hours)

### Flashboot

FlashBoot is RunPod’s magic solution for reducing the average cold-start times on your endpoint. It works probabilistically. When your endpoint has consistent traffic, your workers have a higher chance of benefiting from FlashBoot for faster spin-ups. However, if your endpoint isn’t receiving frequent requests, FlashBoot has fewer opportunities to optimize performance. There’s no additional cost associated with FlashBoot.

### Scale Type

- Queue Delay scaling strategy adjusts worker numbers based on request wait times. With zero workers initially, the first request adds one worker. Subsequent requests add workers only after waiting in the queue for the defined number of delay seconds.
- Request Count scaling strategy adjusts worker numbers according to total requests in the queue and in progress. It automatically adds workers as the number of requests increases, ensuring tasks are handled efficiently.

### Expose HTTP/TCP Ports

We allow direct communication with your worker using its public IP and port. This is especially useful for real-time applications that require minimal latency. Check out this [WebSocket example](https://github.com/runpod-workers/worker-websocket) to see how it works!

## Endpoint Metrics

### Requests

Displays the total number of requests received by your endpoint, along with the number of completed, failed, and retried requests.

### Execution Time

Displays the P70, P90, and P98 execution times for requests on your endpoint. These percentiles help analyze execution time distribution and identify potential performance bottlenecks.

### Delay Time

Delay time is the duration a request spends waiting in the queue before being picked up by a worker. Displays the P70, P90, and P98 delay times for requests on your endpoint. These percentiles help assess whether your endpoint is scaling efficiently.

### Cold Start Time

Cold start time measures how long it takes to wake up a worker. This includes the time needed to start the container, load the model into GPU VRAM, and get the worker ready to process a job. Displays the P70, P90, and P98 cold start times for your endpoint.

### Cold Start Count

Displays the number of cold starts your endpoint has during a given period. The fewer, the better, as fewer cold starts mean faster response times.

### WebhookRequest Responses

Displays the number of webhook requests sent and their corresponding responses, including success and failure counts.

# Pod

## Secure Cloud

GPU instances that run in T3/T4 data centers, providing high reliability and security.

## Community Cloud

GPU instances connect individual compute providers to consumers through a vetted, secure peer-to-peer system.

## Datacenter

A data center is a secure location where RunPod's cloud computing services, such as Secure Cloud and GPU Instances, are hosted. These data centers are equipped with redundancy and data backups to ensure the safety and reliability of your data.

## GPU Instance

GPU Instance is a container-based GPU instance that you can deploy.
These instances spin up in seconds using both public and private repositories.
They are available in two different types:

- Secure Cloud
- Community Cloud

## Template

A RunPod template is a Docker container image paired with a configuration.

## SDKs

RunPod provides several Software Development Kits (SDKs) you can use to interact with the RunPod platform.
These SDKs enable you to create serverless functions, manage infrastructure, and interact with APIs.

---

# burn-testing.md

File: hosting/burn-testing.md

Machines should be thoroughly tested before they are listed on the RunPod platform.
Here is a simple guide to running a burn test for a few days.

Stop the RunPod agent by running:

```command
sudo systemctl stop runpod
```

Then you can kick off a gpu-burn run by typing:

```command
docker run --gpus all --rm jorghi21/gpu-burn-test 172800
```

You should also verify that your memory, CPU, and disk are up to the task.
You can use the [ngstress library](https://wiki.ubuntu.com/Kernel/Reference/stress-ngstress) to accomplish this.

When everything is verified okay, start the RunPod agent again by running

```command
sudo systemctl start runpod
```

Then, on your [machine dashboard](https://www.runpod.io/console/host/machines), self rent your machine to ensure it's working well with most popular templates.

---

# maintenance-and-reliability.md

File: hosting/maintenance-and-reliability.md

## Maintenance

Hosts must currently schedule maintenance at least one week in advance and are able to program immediate maintenance _only_ in the case that their server is unrented.
Users will get email reminders of upcoming maintenance that will occur on their active pods.
Please contact RunPod on Discord or Slack if you are:

- scheduling maintenance on more than a few machines, and/or
- performing operations that could affect user data

Please err on the side of caution and aim to overcommunicate.

Here are some things to keep in mind.

- Uptime/reliability will not be affected during scheduled maintenance.
- ALL other events that may impact customer workloads will result in a reliability score decrease. This includes unlisted machines.
- All machines that have maintenance scheduled will be automatically unlisted 4 days prior to the scheduled maintenance start time to minimize disruption for clients.
- Excessive maintenance will result in further penalties.
- You are allowed to bring down machines that have active users on them provided that you are in a maintenance window.
- Immediate maintenance: this option is only for quick repairs/updates that are absolutely necessary. Unrented servers can still house user data, any operations that can result in potential data loss SHOULD NOT be performed in this maintenance mode.

## Reliability calculations

RunPod aims to partner with datacenters that offer **99.99%** uptime.
Reliability is currently calculated as follows:

`( total minutes + small buffer ) / total minutes in interval`

This means that if you have 30 minutes of network downtime on the first of the month, your reliability will be calculated as:

`( 43200 - 30 + 10 ) / 43200 = 99.95%`

Based on approximately 43200 minutes per month and a 10 minute buffer.
We include the buffer because we do incur small single-minute uptime dings once in a while due to agent upgrades and such.
It will take an entire month to regenerate back to 100% uptime given no further downtimes in the month, considering it it calculated based on a 30 days rolling window.

Machines with less than **98%** reliability are **automatically removed** from the available GPU pool and can only be accessed by clients that already had their data on it.

---

# overview.md

File: hosting/overview.md

## RunPod GPU hosting opportunity

RunPod offers a diverse range of GPUs, made possible through proprietary servers and collaboration with trusted community members.
If you're interested in integrating your hardware into the RunPod ecosystem, follow the steps below.

## How to join as a host

1. Check Eligibility: Make sure you adhere to our [minimum requirements](/hosting/partner-requirements).
2. Connect with us: Currently, we onboard hosts through a manual vetting process.
   If you have high-quality machines that satisfy our hosting requirements, and at least 20 GPUs in total, please fill out [this form](https://share.hsforms.com/1GYpMeNlSQc6n11toAlgNngecykq).

## Additional hosting information

- Service Fee: RunPod charges a 24% service fee. This encompasses:
  - Approximately 4% for Stripe payment fees.
  - 2% for our referral program and 1% for our template program. For more information, see [Refer a friend](https://www.runpod.io/refer-a-friend).

- Pricing: While GPU on-demand prices are consistent, hosts can define a minimum bid price for spot rental. Even though we try as much as possible to maintain stable prices over time, we need to adjust to market trends.

- Safety & Trust: We mandate KYC (Know Your Customer) verification for all hosts to safeguard our users and combat fraud. For larger providers, we require a Provider Agreement and a Service Level Agreement to be completed.

- Hosting Experience: As one of our trusted providers, you have access to a fully customized dashboard to manage your resources that you can leverage to deploy hardware and plan your expansion.

- Rental Rates: We do not make utilization data publicly available.
  However, we are more than happy to provide statistics and information about popular GPU models when directly discussing with you.
  Furthermore, lots of different variables can impact occupancy.
  We are more than happy to provide you with in-dept data about how different hardware quality levels can impact your revenue.

---

# partner-requirements.md

File: hosting/partner-requirements.md

# RunPod Secure Cloud Partner Requirements - Release 2025

# Introduction

This document outlines the specifications required to be a RunPod secure cloud partner. These requirements establish the baseline, however for new partners, RunPod will perform a due diligence process prior to selection encompassing business health, prior performance, and corporate alignment.

Meeting these technical and operational requirements does not guarantee selection.

_New partners_

- All specifications will apply to new partners on November 1, 2024.

_Existing partners_

- Hardware specifications (Sections 1, 2, 3, 4) will apply to new servers deployed by existing partners on December 15, 2024.
- Compliance specification (Section 5) will apply to existing partners on April 1, 2025.

A new revision will be released in October 2025 on an annual basis. Minor mid-year revisions may be made as needed to account for changes in market, roadmap, or customer needs.

## Minimum deployment size

100kW of GPU server capacity is the minimum deployment size.

## 1. Hardware Requirements

### 1.1 GPU Compute Server Requirements

#### GPU Requirements

NVIDIA GPUs no older than Ampere generation.

### CPU

| Requirement      | Specification                                                                                                                          |
| ---------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| Cores            | Minimum 4 physical CPU cores per GPU + 2 for system operations                                                                         |
| Clock Speed      | Minimum 3.5 GHz base clock, with boost clock of at least 4.0 GHz                                                                       |
| Recommended CPUs | AMD EPYC 9654 (96 cores, up to 3.7 GHz), Intel Xeon Platinum 8490H (60 cores, up to 4.8 GHz), AMD EPYC 9474F (48 cores, up to 4.1 GHz) |

### Bus Bandwidth

| GPU VRAM          | Minimum Bandwidth |
| ----------------- | ----------------- |
| 8/10/12/16 GB     | PCIe 3.0 x16      |
| 20/24/32/40/48 GB | PCIe 4.0 x16      |
| 80 GB             | PCIe 5.0 x16      |

Exceptions list:

1. PCIe 4.0 x16 - A100 80GB PCI-E

### Memory

Main system memory must have ECC.

| GPU Configuration | Recommended RAM  |
| ----------------- | ---------------- |
| 8x 80 GB VRAM     | >= 2048 GB DDR5  |
| 8x 40/48 GB VRAM  | >= 1024 GB DDR5  |
| 8x 24 GB VRAM     | >= 512 GB DDR4/5 |
| 8x 16 GB VRAM     | >= 256 GB DDR4/5 |

### Storage

There are two types of required storage, boot and working arrays. These are two separate arrays of hard drives which provide isolation between host operating system activity (boot array) and customer workloads (working array).

### Boot array

| **Requirement**                    | **Specification**         |
| ---------------------------------- | ------------------------- |
| Redundancy                         | >= 2n redundancy (RAID 1) |
| Size                               | >= 500GB (Post RAID)      |
| Disk Perf - Sequential read        | 2,000 MB/s                |
| Disk Perf - Sequential write       | 2,000 MB/s                |
| Disk Perf - Random Read (4K QD32)  | 100,000 IOPS              |
| Disk Perf - Random Write (4K QD32) | 10,000 IOPS               |

### Working array

| Component                          | Requirement                                                                         |
| ---------------------------------- | ----------------------------------------------------------------------------------- |
| Redundancy                         | >= 2n redundancy (RAID 1 or RAID 10)                                                |
| Size                               | 2 TB+ NVME per GPU for 24/48 GB GPUs; 4 TB+ NVME per GPU for 80 GB GPUs (Post RAID) |
| Disk Perf - Sequential read        | 6,000 MB/s                                                                          |
| Disk Perf - Sequential write       | 5,000 MB/s                                                                          |
| Disk Perf - Random Read (4K QD32)  | 400,000 IOPS                                                                        |
| Disk Perf - Random Write (4K QD32) | 40,000 IOPS                                                                         |

### 1.2 Storage Cluster Requirements

Each datacenter must have a storage cluster which provides shared storage between all GPU servers. The hardware is provided by the partner, storage cluster licensing is provided by RunPod. All storage servers must be accessible by all GPU compute machines.

### Baseline Cluster Specifications

| Component            | Requirement                         |
| -------------------- | ----------------------------------- |
| Minimum Servers      | 4                                   |
| Minimum Storage size | 200 TB raw (100 TB usable)          |
| Connectivity         | 200 Gbps between servers/data-plane |
| Network              | Private subnet                      |

### Server Specifications

| Component | Requirement                                                                                                            |
| --------- | ---------------------------------------------------------------------------------------------------------------------- |
| CPU       | AMD Genoa: EPYC 9354P (32-Core, 3.25-3.8 GHz), EPYC 9534 (64-Core, 2.45-3.7 GHz), or EPYC 9554 (64-Core, 3.1-3.75 GHz) |
| RAM       | 256 GB or higher, DDR5/ECC                                                                                             |

### Storage Cluster Server Boot Array

| Requirement                        | Specification             |
| ---------------------------------- | ------------------------- |
| Redundancy                         | >= 2n redundancy (RAID 1) |
| Size                               | >= 500GB (Post RAID)      |
| Disk Perf - Sequential read        | 2,000 MB/s                |
| Disk Perf - Sequential write       | 2,000 MB/s                |
| Disk Perf - Random Read (4K QD32)  | 100,000 IOPS              |
| Disk Perf - Random Write (4K QD32) | 10,000 IOPS               |

### Storage Cluster Server Working Array

| Component                          | Requirement                                                                      |
| ---------------------------------- | -------------------------------------------------------------------------------- |
| Redundancy                         | None (JBOD) - RunPod will assemble into array. 7 to 14TB disk sizes recommended. |
| Disk Perf - Sequential read        | 6,000 MB/s                                                                       |
| Disk Perf - Sequential write       | 5,000 MB/s                                                                       |
| Disk Perf - Random Read (4K QD32)  | 400,000 IOPS                                                                     |
| Disk Perf - Random Write (4K QD32) | 40,000 IOPS                                                                      |

Servers should have spare disk slots for future expansion without deployment of new servers.

Even distribution among machines (e.g., 7 TB x 8 disks x 4 servers = 224 TB total space).

### Dedicated Metadata Server for Large-Scale Clusters

Once a storage cluster exceeds 90% single core CPU on the leader node during peak hours, a dedicated metadata server is required. Metadata tracking is a single process operation, and single threaded performance is the most important metric.

| Component | Requirement                                          |
| --------- | ---------------------------------------------------- |
| CPU       | AMD Ryzen Threadripper 7960X (24-Cores, 4.2-5.3 GHz) |
| RAM       | 128 GB or higher, DDR5/ECC                           |
| Boot disk | >= 500 GB, RAID 1                                    |

## 2. Software Requirements

### Operating System

Ubuntu Server 22.04 LTS
Linux kernel 6.5.0-15 or later production version (Ubuntu HWE Kernel)
SSH remote connection capability

### BIOS Configuration

IOMMU disabled for non-VM systems
Update server BIOS/firmware to latest stable version

### Drivers and Software

| Component          | Requirement                                   |
| ------------------ | --------------------------------------------- |
| NVIDIA Drivers     | Version 550.54.15 or later production version |
| CUDA               | Version 12.4 or later production version      |
| NVIDIA Persistence | Activated for GPUs of 48 GB or more           |

### HGX SXM System Addendum

- NVIDIA Fabric Manager installed, activated, running, and tested
- Fabric Manager version must match NVIDIA drivers and Kernel drivers headers
- CUDA Toolkit, NVIDIA NSCQ, and NVIDIA DCGM installed
- Verify NVLINK switch topology using nvidia-smi and dcgmi
- Ensure SXM performance using dcgmi diagnostic tool

## 3. Data Center Power Requirements

| Requirement             | Specification                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ----------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Utility Feeds           | - Minimum of two independent utility feeds from separate substations <br /> - Each feed capable of supporting 100% of the data center's power load<br />- Automatic transfer switches (ATS) for seamless switchover between feeds with UL 1008 certification (or regional equivalent)                                                                                                                                                     |
| UPS                     | - N+1 redundancy for UPS systems<br />- Minimum of 15 minutes runtime at full load                                                                                                                                                                                                                                                                                                                                                        |
| Generators              | - N+1 redundancy for generator systems<br />- Generators must be able to support 100% of the data center's power load<br />- Minimum of 48 hours of on-site fuel storage at full load<br />- Automatic transfer to generator power within 10 seconds of utility failure                                                                                                                                                                   |
| Power Distribution      | - Redundant power distribution paths (2N) from utility to rack level<br />- Redundant Power Distribution Units (PDUs) in each rack<br />- Remote power monitoring and management capabilities at rack level                                                                                                                                                                                                                               |
| Testing and Maintenance | - Monthly generator tests under load for a minimum of 30 minutes<br />- Quarterly full-load tests of the entire backup power system, including UPS and generators<br />- Annual full-facility power outage test (coordinated with RunPod)<br />- Regular thermographic scanning of electrical systems<br />- Detailed maintenance logs for all power equipment<br />- 24/7 on-site facilities team for immediate response to power issues |
| Monitoring and Alerting | - Real-time monitoring of all power systems<br />- Automated alerting for any power anomalies or threshold breaches                                                                                                                                                                                                                                                                                                                       |
| Capacity Planning       | - Maintain a minimum of 20% spare power capacity for future growth<br />- Annual power capacity audits and forecasting                                                                                                                                                                                                                                                                                                                    |
| Fire Suppression        | - Maintain datacenter fire suppression systems in compliance with NFPA 75 and 76 (or regional equivalent)                                                                                                                                                                                                                                                                                                                                 |

## 4. Network Requirements

| Requirement             | Specification                                                                                                                                                                                                                                                                                                                                                                  |
| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Internet Connectivity   | - Minimum of two diverse and redundant internet circuits from separate providers<br />- Each connection should be capable of supporting 100% of the data center's bandwidth requirements<br />- BGP routing implemented for automatic failover between circuit providers<br />- 100 Gbps minimum total bandwidth capacity                                                      |
| Core Infrastructure     | - Redundant core switches in a high-availability configuration (e.g., stacking, VSS, or equivalent)                                                                                                                                                                                                                                                                            |
| Distribution Layer      | - Redundant distribution switches with multi-chassis link aggregation (MLAG) or equivalent technology<br />- Minimum 100 Gbps uplinks to core switches                                                                                                                                                                                                                         |
| Access Layer            | - Redundant top-of-rack switches in each cabinet<br />- Minimum 100 Gbps server connections for high-performance compute nodes                                                                                                                                                                                                                                                 |
| DDoS Protection         | - Must have a DDoS mitigation solution, either on-premises or on-demand cloud-based                                                                                                                                                                                                                                                                                            |
| Quality of service      | Maintain network performance within the following parameters:<br /> * Network utilization levels must remain below 80% on any link during peak hours<br /> * Packet loss must not exceed 0.1% (1 in 1000) on any network segment<br /> * P95 round-trip time (RTT) within the data center should not exceed 4ms<br /> * P95 jitter within the datacenter should not exceed 3ms |
| Testing and Maintenance | - Regular failover testing of all redundant components (minimum semi-annually)<br />- Annual full-scale disaster recovery test<br />- Maintenance windows for network updates and patches, with minimal service disruption scheduled at least 1 week in advance                                                                                                                |
| Capacity Planning       | - Maintain a minimum of 40% spare network capacity for future growth<br />- Regular network performance audits and capacity forecasting                                                                                                                                                                                                                                        |

## 5. Compliance Requirements

To qualify as a RunPod secure cloud partner, the parent organization must adhere to at least one of the following compliance standards:

- SOC 2 Type I (System and Organization Controls)
- ISO/IEC 27001:2013 (Information Security Management Systems)
- PCI DSS (Payment Card Industry Data Security Standard)

Additionally, partners must comply with the following operational standards:

| Requirement       | Description                                                                                                                                                                                            |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Data Center Tier  | Abide by Tier III+ Data Center Standards                                                                                                                                                               |
| Security          | 24/7 on-site security and technical staff                                                                                                                                                              |
| Physical security | RunPod servers must be held in an isolated secure rack or cage in an area that is not accessible to any non-partner or approved DC personnel. Physical access to this area must be tracked and logged. |
| Maintenance       | All maintenance resulting in disruption or downtime must be scheduled at least 1 week in advance. Large disruptions must be coordinated with RunPod at least 1 month in advance.                       |

RunPod will review evidence of:

- Physical access logs
- Redundancy checks
- Refueling agreements
- Power system test results and maintenance logs
- Power monitoring and capacity planning reports
- Network infrastructure diagrams and configurations
- Network performance and capacity reports
- Security audit results and incident response plans

For detailed information on maintenance scheduling, power system management, and network operations, please refer to our documentation.

### Release log

- 2025-11-01: Initial release.

---

# dstack.md

File: integrations/dstack/dstack.md


[dstack](https://dstack.ai/) is an open-source tool that simplifies the orchestration of Pods for AI and ML workloads. By defining your application and resource requirements in YAML configuration files, it automates the provisioning and management of cloud resources on RunPod, allowing you to focus on your application logic rather than the infrastructure.

In this guide, we'll walk through setting up [dstack](https://dstack.ai/) with RunPod to deploy [vLLM](https://github.com/vllm-project/vllm). We'll serve the `meta-llama/Llama-3.1-8B-Instruct` model from Hugging Face using a Python environment.

## Prerequisites

- [A RunPod account with an API key](/get-started/api-keys)
- On your local machine:
  - Python 3.8 or higher
  - `pip` (or `pip3` on macOS)
  - Basic utilities: `curl`
- These instructions are applicable for macOS, Linux, and Windows systems.

:::note
**Windows Users**

- It's recommended to use [WSL (Windows Subsystem for Linux)](https://docs.microsoft.com/en-us/windows/wsl/install) or tools like [Git Bash](https://gitforwindows.org/) to follow along with the Unix-like commands used in this tutorial
- Alternatively, Windows users can use PowerShell or Command Prompt and adjust commands accordingly
  :::

## Installation

### Setting Up the dstack Server

1. **Prepare Your Workspace**

   Open a terminal or command prompt and create a new directory for this tutorial:

   ```bash
   mkdir runpod-dstack-tutorial
   cd runpod-dstack-tutorial
   ```

2. **Set Up a Python Virtual Environment**

   ```bash
   ```
   ```bash
   ```
   ```cmd
   ```
   ```powershell
   ```
   ```bash
   ```
   ```bash
   ```
   ```cmd
   ```
     ```bash
     ```
     ```bash
     ```
     ```cmd
     ```
     ```bash
     ```
     ```bash
     ```
     ```cmd
     ```
     ```yaml
     ```
   ```bash
   ```
```
```
  ```bash
  ```
  ```bash
  ```
  ```bash
  ```
  ```cmd
  ```
  ```powershell
  ```
```bash
```
  ```yaml
  ```
```bash
```
```bash
```
  ```
  ```
  ```bash
  ```
  ```
  ```
```bash
```
```bash
```
```cmd
```
```powershell
```
```json
```
```bash
```
```bash
```
```yaml
```
```bash
```
```yaml
```

---

# mods.md

File: integrations/mods/mods.md

[Mods](https://github.com/charmbracelet/mods) is an AI-powered tool designed for the command line and built to seamlessly integrate with pipelines.
It provides a convenient way to interact with language models directly from your terminal.

## How Mods Works

Mods operates by reading standard input and prefacing it with a prompt supplied in the Mods arguments.
It sends the input text to a language model (LLM) and prints out the generated result.
Optionally, you can ask the LLM to format the response as Markdown.
This allows you to "question" the output of a command, making it a powerful tool for interactive exploration and analysis. Additionally, Mods can work with standard input or an individually supplied argument prompt.

## Getting Started

To start using Mods, follow these step-by-step instructions:

1. **Obtain Your API Key**:
   - Visit the [RunPod Settings](https://www.runpod.io/console/user/settings) page to retrieve your API key.
   - If you haven't created an account yet, you'll need to sign up before obtaining the key.

2. **Install Mods**:
   - Refer to the different installation methods for [Mods](https://github.com/charmbracelet/mods) based on your preferred approach.

3. **Configure RunPod**:
   - Update the `config_template.yml` file to use your RunPod configuration. Here's an example:

     ```yml
     runpod:
       # https://docs.runpod.io/serverless/workers/vllm/openai-compatibility
       base-url: https://api.runpod.ai/v2/${YOUR_ENDPOINT}/openai/v1
       api-key:
       api-key-env: RUNPOD_API_KEY
       models:
         # Add your model name
         openchat/openchat-3.5-1210:
           aliases: ["openchat"]
           max-input-chars: 8192
     ```

   - `base-url`: Update your base-url with your specific endpoint.
   - `api-key-env`: Add your RunPod API key.
   - `openchat/openchat-3.5-1210`: Replace with the name of the model you want to use.
   - `aliases: ["openchat"]`: Replace with your preferred model alias.
   - `max-input-chars`: Update the maximum input characters allowed for your model.

4. **Verify Your Setup**:
   - To ensure everything is set up correctly, pipe any command line output and pass it to `mods`.
   - Specify the RunPod API and model you want to use.

     ```bash
     ls ~/Downloads | mods --api runpod --model openchat -f "tell my fortune based on these files" | glow
     ```

   - This command will list the files in your `~/Downloads` directory, pass them to Mods using the RunPod API and the specified model, and format the response as a fortune based on the files. The output will then be piped to `glow` for a visually appealing display.

---

# overview.md

File: integrations/overview.md


# Integrations

RunPod integrates with various tools that enable you to automate interactions, such as managing containers with infrastructure-as-code and interacting with serverless endpoints without using the WebUI or API. These integrations provide flexibility and automation for streamlining complex workflows and scaling your operations.

<DocCardList />

---

# skypilot.md

File: integrations/skypilot/skypilot.md

[SkyPilot](https://skypilot.readthedocs.io/en/latest/) is a framework for executing LLMs, AI, and batch jobs on any cloud, offering maximum cost savings, highest GPU availability, and managed execution.

This integration leverages the RunPod CLI infrastructure, streamlining the process of spinning up on-demand pods and deploying serverless endpoints with SkyPilot.

## Getting started

To begin using RunPod with SkyPilot, follow these steps:

1. **Obtain Your API Key**: Visit the [RunPod Settings](https://www.runpod.io/console/user/settings) page to get your API key. If you haven't created an account yet, you'll need to do so before obtaining the key.

2. **Install RunPod**: Use the following command to install the latest version of RunPod:
   ```
   pip install "runpod>=1.6"
   ```

3. **Configure RunPod**: Enter `runpod config` in your CLI and paste your API key when prompted.

4. **Install SkyPilot RunPod Cloud**: Execute the following command to install the [SkyPilot RunPod cloud](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html#runpod):
   ```
   pip install "skypilot-nightly[runpod]"
   ```

5. **Verify Your Setup**: Run `sky check` to ensure your credentials are correctly set up and you're ready to proceed.

## Running a Project

After setting up your environment, you can seamlessly spin up a cluster in minutes:

1. **Create a New Project Directory**: Run `mkdir hello-sky` to create a new directory for your project.

2. **Navigate to Your Project Directory**: Change into your project directory with `cd hello-sky`.

3. **Create a Configuration File**: Enter `cat > hello_sky.yaml` and input the following configuration details:

   ```yml
   resources:
     cloud: runpod

   # Working directory (optional) containing the project codebase.
   # Its contents are synced to ~/sky_workdir/ on the cluster.
   workdir: .

   # Setup commands (optional).
   # Typical use: pip install -r requirements.txt
   # Invoked under the workdir (i.e., can use its files).
   setup: |
     echo "Running setup."

   # Run commands.
   # Typical use: make use of resources, such as running training.
   # Invoked under the workdir (i.e., can use its files).
   run: |
     echo "Hello, SkyPilot!"
     conda env list
   ```

4. **Launch Your Project**: With your configuration file created, launch your project on the cluster by running `sky launch -c mycluster hello_sky.yaml`.

5. **Confirm Your GPU Type**: You should see the available GPU options on Secure Cloud appear in your command line. Once you confirm your GPU type, your cluster will start spinning up.

With this integration, you can leverage the power of RunPod and SkyPilot to efficiently run your LLMs, AI, and batch jobs on any cloud.

---

# llms.md

File: llms.md

# RunPod Documentation

The complete documentation is available as a text file for LLM processing.

You can access the raw text file directly at [/llms.txt](/llms.txt).

---

# overview.md

File: overview.md

**RunPod** is a cloud computing platform designed for AI, machine learning applications, and general computing needs.

Leverage our platform to execute your code using both GPU and CPU resources through our [Pods](/pods/overview) and [Serverless](/serverless/overview) computing options.

Start today by [signing up for an account](https://www.runpod.io/console/signup).

## What are Pods?

**Pods** allow you to run your code on GPU and CPU instances using containers.

Pods are available in two types: [Secure Cloud and Community Cloud](references/faq/#secure-cloud-vs-community-cloud). Secure Cloud operates in T3/T4 data centers, providing high reliability and security. Community Cloud connects individual compute providers to consumers through a vetted, secure peer-to-peer system.

## What is Serverless?

**Serverless** offers pay-per-second serverless computing with autoscaling capabilities for your production environment.

Define a Worker, create a REST API Endpoint for it, queue jobs, and enjoy autoscaling to meet demand.
This service, part of our Secure Cloud offering, ensures low cold-start times and robust security measures.

Get started with:

- [Building your own Worker image](/serverless/workers/overview)
- [Using any LLM with the vLLM worker](/serverless/workers/vllm/overview)

## Command Line Interface (CLI)

RunPod also provides a Command Line Interface (CLI) tool for quickly developing and deploying custom endpoints on the RunPod serverless platform.

For more information, see the following:

- [GitHub](https://github.com/runpod/runpodctl)
- [Reference documentation](/runpodctl/reference/runpodctl)

### Our mission

RunPod aims to make cloud computing accessible and affordable for everyone, without compromising on features, usability, or experience. We empower individuals and enterprises with cutting-edge technology to unlock the potential of AI and cloud computing.

For general inquiries, browse our documentation or reach out to us on [Discord](https://discord.gg/cUpRmau42V), our support chat, or by [email](mailto:help@runpod.io). More information is available on our [contact page](https://www.runpod.io/contact).

## Where to go next?

Learn more about RunPod by:

- [Creating an account](/get-started/manage-accounts)
- [Adding funds to your account](/get-started/billing-information)
- [Running your first tutorial](/tutorials/introduction/overview)

---

# choose-a-pod.md

File: pods/choose-a-pod.md

Selecting the appropriate Pod instance is a critical step in planning your RunPod deployment. The choice of VRAM, RAM, vCPU, and storage, both Temporary and Persistent, can significantly impact the performance and efficiency of your project.

This page gives guidance on how to choose your Pod configuration. However, these are general guidelines. Keep your specific requirements in mind and plan accordingly.

### Overview

It's essential to understand the specific needs of your model. You can normally find detailed information in the model card’s description on platforms like Hugging Face or in the `config.json` file of your model.

There are tools that can help you assess and calculate your model’s specific requirements, such as:

- [Hugging Face's Model Memory Usage Calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage)
- [Vokturz’ Can it run LLM calculator](https://huggingface.co/spaces/Vokturz/can-it-run-llm)
- [Alexander Smirnov’s VRAM Estimator](https://vram.asmirnov.xyz)

Using these resources should give you a clearer picture of what to look for in a Pod.

When transitioning to the selection of your Pod, you should focus on the following main factors:

- **GPU**
- **VRAM**
- **Disk Size**

Each of these components plays a crucial role in the performance and efficiency of your deployment. By carefully considering these elements along with the specific requirements of your project as shown in your initial research, you will be well-equipped to determine the most suitable Pod instance for your needs.

### GPU

The type and power of the GPU directly affect your project's processing capabilities, especially for tasks involving graphics processing and machine learning.

### Importance

The GPU in your Pod plays a vital role in processing complex algorithms, particularly in areas like data science, video processing, and machine learning. A more powerful GPU can significantly speed up computations and enable more complex tasks.

### Selection criteria

- **Task Requirements**: Assess the intensity and nature of the GPU tasks in your project.
- **Compatibility**: Ensure the GPU is compatible with your software and frameworks.
- **Energy Efficiency**: Consider the power consumption of the GPU, especially for long-term deployments.

### VRAM

VRAM (Video RAM) is crucial for tasks that require heavy graphical processing and rendering. It is the dedicated memory used by your GPU to store image data that is displayed on your screen.

### Importance

VRAM is essential for intensive tasks. It serves as the memory for the GPU, allowing it to store and access data quickly. More VRAM can handle larger textures and more complex graphics, which is crucial for high-resolution displays and advanced 3D rendering.

### Selection criteria

- **Graphics Intensity**: More VRAM is needed for graphically intensive tasks such as 3D rendering, gaming, or AI model training that involves large datasets.
- **Parallel Processing Needs**: Tasks that require simultaneous processing of multiple data streams benefit from more VRAM.
- **Future-Proofing**: Opting for more VRAM can make your setup more adaptable to future project requirements.

### Storage

Adequate storage, both temporary and persistent, ensures smooth operation and data management.

### Importance

Disk size, including both temporary and persistent storage, is critical for data storage, caching, and ensuring that your project has the necessary space for its operations.

### Selection criteria

- **Data Volume**: Estimate the amount of data your project will generate and process.
- **Speed Requirements**: Faster disk speeds can improve overall system performance.
- **Data Retention Needs**: Determine the balance between temporary (volatile) and persistent (non-volatile) storage based on your data retention policies.

---

# export-data.md

File: pods/configuration/export-data.md

You can export your Pod's data to any of the following cloud providers:

- Amazon S3
- Google Cloud Storage
- Microsoft Azure Blob Storage
- Dropbox
- Backblaze B2 Cloud Storage

Remember to keep your keys and access tokens confidential to maintain the security of your resources.

## Amazon S3

You can review a video guide on the process [here](https://www.youtube.com/watch?v=2ZuOKwFR9pc&t=1s).

### Creating a Bucket within Amazon S3

1. **Access the Bucket Creation Form:**
   - Navigate to the Amazon S3 bucket creation form by visiting [this link](https://s3.console.aws.amazon.com/s3/bucket/create?region=us-east-1).

2. **Name Your Bucket:**
   - Provide a descriptive name for your bucket. Choose a name that is easy to remember and reflects the contents or purpose of the bucket.

3. **Select AWS Region:**
   - Ensure you select your preferred AWS Region. This is important for data storage locations and can affect access speeds.

4. **Adjust Public Access Settings:**
   - Uncheck the **Block All Public Access** option at the bottom of the form if you need your bucket to be publicly accessible.

5. **Access Key and Secret Access Key:**
   - Go to Security Credentials in your AWS account.
   - Create an Access Key on the Security Credentials page.
   - Note that your Secret Access Key will be displayed during this process. Keep it secure.


### Sending Data from RunPod to AWS S3

1. **Access CloudSync in RunPod:**
   - In RunPod, navigate to the CloudSync section.

2. **Enter Key IDs and Bucket Information:**
   - Enter your Access Key and Secret Access Key.
   - Specify the AWS Region where your bucket is located.
   - Provide the path of your bucket as shown in the interface.

3. **Initiate Data Transfer:**
   - Select the **Copy to AWS S3** option.
   - This action will start copying your pod contents to the specified Amazon S3 bucket.

4. **Monitor Transfer:**
   - Once you select Copy, your pod contents should begin copying over to Amazon S3.
   - You can monitor the transfer process through RunPod’s interface to ensure that the data transfer completes successfully.


Remember to keep your Access Key and Secret Access Key confidential to maintain the security of your AWS resources.

## Google Cloud Storage

### Creating a Bucket within Google Cloud Storage

1. **Access the Bucket Creation Interface:**
   - Navigate to the Google Cloud Storage dashboard and click on "Buckets -> Create" to access the bucket creation interface.

2. **Name Your Bucket:**
   - Assign a unique, descriptive name to your bucket that reflects its contents or purpose.

3. **Configure Bucket Settings:**
   - Leave most options as default. Important: Uncheck "Enforce Public Access Prevention On This Bucket" if you need your bucket to be publicly accessible.

4. **Organize Your Bucket:**
   - Once the bucket is created, consider creating a folder within the bucket for better organization, especially if managing multiple pods.


### Transferring Data from RunPod to Google Cloud Storage

1. **Access CloudSync in RunPod:**
   - Within RunPod, go to the CloudSync section and select "Google Cloud Storage -> Copy to Google Cloud Storage."

2. **Service Account JSON Key:**
   - Obtain your Service Account JSON key. If unsure how to do this, consult [this guide](https://cloud.google.com/iam/docs/keys-create-delete).
   - In the provided field on RunPod, paste the entire contents of your Service Account JSON key.

3. **Specify Transfer Details:**
   - Enter the destination path in your bucket.
   - Choose the folder from your pod that you wish to copy.

4. **Initiate and Monitor Transfer:**
   - Start the data transfer process by selecting the relevant options.
   - Monitor the transfer in the RunPod interface to ensure successful completion.


### Troubleshooting

- If your bucket is not publicly viewable and you encounter errors, refer to [Google Cloud Storage's documentation on making data public](https://cloud.google.com/storage/docs/access-control/making-data-public) for necessary adjustments.

Remember to keep your Service Account JSON key confidential to maintain the security of your Google Cloud resources.

## Azure Blob Storage Setup and Data Transfer with RunPod

### Creating a Storage Account in Azure

1. **Create a Resource Group in Azure:**
   - Go to [Resource Groups](https://portal.azure.com/#view/HubsExtension/BrowseResourceGroups) and click the Create button.
   - Name the resource group, which will be used to organize your Azure resources.

2. **Set Up a Storage Account:**
   - Under [Storage Accounts](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Storage%2FStorageAccounts), click Create.
   - Provide a name for your storage account and assign it to the newly created resource group.

3. **Retrieve Access Key:**
   - Navigate to Access Keys under Security + Networking in your storage account to get the key needed for authentication.

4. **Create a Blob Container:**
   - In the Storage Browser, select Blob Containers, then click Add Container.
   - Optionally, create folders within this container for better organization.


### Transferring Data from RunPod to Azure Blob Storage

1. **Access Cloud Sync in RunPod:**
   - Go to your pod in My Pods on RunPod.
   - Select Cloud Sync, then choose "Azure Blob Storage" and "Copy to Azure Blob Storage."

2. **Input Storage Details:**
   - Enter your Azure account name and account key.
   - Specify the desired path in the blob storage where the data will be transferred.

3. **Initiate Transfer:**
   - Click on "Copy to Azure Blob Storage" to start the process.
   - Your RunPod data will begin copying over to the specified location in Azure Blob Storage.


Ensure secure handling of your Azure account key to maintain the integrity and security of your data during the transfer process.

## Backblaze B2 Cloud Storage Setup

### Creating a Bucket in Backblaze B2

1. **Navigate to Bucket Creation:**
   - Go to [B2 Cloud Storage Buckets](https://secure.backblaze.com/b2_buckets.htm) and click "Create a Bucket."
   - Make sure to set the bucket visibility to Public.

2. **Generate Application Key:**
   - Visit [App Keys](https://secure.backblaze.com/app_keys.htm) to create a new application key. This key will be used for authenticating access to your bucket.


### Transferring Data from RunPod to Backblaze B2

1. **Access CloudSync in RunPod:**
   - On your My Pods screen in RunPod, select Cloud Sync, then choose "Backblaze B2."

2. **Enter Credentials:**
   - Input your KeyID in the first field.
   - Enter your applicationKey in the second field.
   - Specify your bucket name as illustrated in the interface.

3. **Initiate Transfer:**
   - Click "Copy to Backblaze B2" to start the transfer process. Your pod's contents will begin transferring to the specified Backblaze B2 bucket.


Remember to securely manage your KeyID and applicationKey to ensure the safety of your data in Backblaze B2 Cloud Storage.

## Dropbox Setup and Data Transfer with RunPod

### Setting Up Dropbox

1. **Create an App on Dropbox:**
   - Go to the [DBX Platform](https://www.dropbox.com/developers/apps/create) and create an app.
   - Choose "Scoped Access" under API options and "Full Dropbox" for the type of access. Then, name your app.

2. **Configure App Permissions:**
   - In the Dropbox App Console, under the Permissions tab, make sure to enable the required checkboxes for reading and writing access.

3. **Generate Access Token:**
   - Return to the Settings tab of your app.
   - In the OAuth2 section, click "Generate" under Generated Access Token to create an access key.
   - Save this key securely, as it is crucial for integrating with RunPod and will not be visible after leaving the page.

4. **Create a Dropbox Folder (Optional):**
   - Although not mandatory, it's advisable to create a dedicated folder in Dropbox for organizing the data synced from RunPod.


### Transferring Data from RunPod to Dropbox

1. **Access Cloud Sync in RunPod:**
   - In RunPod, navigate to the Cloud Sync option and select Dropbox.

2. **Enter Access Token and Path:**
   - Input your Dropbox Access Token.
   - Specify the remote path in Dropbox where you want to send the data.

3. **Start Data Sync:**
   - Click "Copy to Dropbox" to initiate the data syncing process. Your RunPod data will begin transferring to the specified location in Dropbox.


Ensure the safekeeping of your Dropbox Access Token to maintain the security of your data during the sync process.

---

# expose-ports.md

File: pods/configuration/expose-ports.md

There are a few ways to expose ports on your pod to the outside world. The first thing that you should understand is that the publicly exposed port is most likely NOT going to be the same as the port that you expose on your container. Let's look at an example to illustrate this.

Let's say that I want to run a public API on my pod using uvicorn with the following command:

```
uvicorn main:app --host 0.0.0.0 --port 4000
```

This means that uvicorn would be listening on all interfaces on port 4000. Let's now expose this port to the public internet using two different methods.

### Through RunPod's Proxy

In this case, you would want to make sure that the port you want to expose (4000 in this case) is set on the [Template](https://www.runpod.io/console/user/templates) or [Pod](https://www.runpod.io/console/pods) configuration page. You can see here that I have added 4000 to the HTTP port list in my pod config. You can also do this on your template definition.


Once you have done this, and your server is running, you should be able to hit your server using the pod's proxy address, which is formed in this programmatic way, where the pod ID is the unique ID of your pod, and the internal port in this case is 4000:

```text
https://{POD_ID}-{INTERNAL_PORT}.proxy.runpod.net
```

Keep in mind that this exposed to the public internet. While your pod ID can act as a password of sorts, it's not a replacement for real authentication, which should be implemented at your API level.

#### Important Proxy Behavior Notes

When users access their pods via the RunPod proxy, the connection follows this chain:

```
User -> Cloudflare -> RunPod Loadbalancer -> Pod
```

It's crucial to be aware of the following behavior:

- Cloudflare has a 100-second limit for a connection to remain open.
- If your service does not respond within 100 seconds of a request, the connection will be closed.
- In such cases, the user will receive a `524` error code.

This timeout limit is particularly important for long-running operations or services that might take more than 100 seconds to respond.
Make sure to design your applications with this limitation in mind, potentially implementing progress updates or chunked responses for longer operations.

### Through TCP Public IP

If your pod supports a public IP address, you can also expose your API over public TCP. In this case, you would add the port to the TCP side of the configuration.


The only difference here is that you will receive an external port mapping and a public IP address to access your service.
For example, your connect menu may look something like this:


In this case, you would be hitting your service running on 4000 with the following ip:port combination

```text
73.10.226.56:10027
```

Be aware that the public IP could potentially change when using Community Cloud, but should not change when using Secure Cloud. The port will change if your pod gets reset.

### Requesting a Symmetrical Port Mapping

For some applications, asymmetrical port mappings are not ideal. In the above case, we have external port 10027 mapping to internal port 4000. If you need to have a symmetrical port mapping, you can request them by putting in ports above 70000 in your TCP port field.


Of course, 70000 isn't a valid port number, but what this does is it tells RunPod that you don't care what the actual port number is on launch, but to rather give you a symmetrical mapping. You can inspect the actual mapping via your connect menu:


In this case, I have requested two symmetrical ports and they ended up being 10030:10030 and 10031:10031. If you need programmatic access to these in your pod, you can access them via environment variable:

```text
RUNPOD_TCP_PORT_70001=10031
RUNPOD_TCP_PORT_70000=10030
```

---

# override-public-keys.md

File: pods/configuration/override-public-keys.md

We attempt to inject the public key that you configure in your account's settings page for authentication using basic terminal.
If you want to override this at a pod level, you can manually supply a public key as the `RUNPOD_SSH_PUBLIC_KEY` environment variable.

---

# use-ssh.md

File: pods/configuration/use-ssh.md

The basic terminal SSH access that RunPod exposes is not a full SSH connection and, therefore, does not support commands like SCP. If you want to have full SSH capabilities, then you will need to rent an instance that has public IP support and run a full SSH daemon in your Pod.

## Setup

1. Generate your public/private SSH key pair on your local machine with `ssh-keygen -t ed25519 -C "your_email@example.com"`. This will save your public/private key pair to `~/.ssh/id_ed25519.pub` and `~/.ssh/id_ed25519`, respectively.\
   :::note
   if you're using command prompt in Windows rather than the Linux terminal or WSL, your public/private key pair will be saved to `C:\users\{yourUserAccount}\.ssh\id_ed25519.pub` and `C:\users\{yourUserAccount}\.ssh\id_ed25519`, respectively.
   :::


2. Add your public key to your [RunPod user settings](https://www.runpod.io/console/user/settings).



3. Start your Pod. Make sure of the following things:

- Your Pod supports a public IP, if you're deploying in Community Cloud.
- An SSH daemon is started. If you're using a RunPod official template such as RunPod Stable Diffusion, you don't need to take any additional steps. If you're using a custom template, make sure your template has TCP port 22 exposed and use the following Docker command. If you have an existing start command, replace `sleep infinity` at the end with your existing command:

```bash
bash -c 'apt update;DEBIAN_FRONTEND=noninteractive apt-get install openssh-server -y;mkdir -p ~/.ssh;cd $_;chmod 700 ~/.ssh;echo "$PUBLIC_KEY" >> authorized_keys;chmod 700 authorized_keys;service ssh start;sleep infinity'
```


Once your Pod is done initializing, you'll be able to SSH into it by running the SSH over exposed TCP command in the Pod's Connection Options menu on your local machine.

:::note

- if you're using the Windows Command Prompt rather than the Linux terminal or WSL, and you've used the default key location when generating your public/private key pair (i.e., you didn't specify a different file path when prompted), you'll need to modify the file path in the provided SSH command after the `-i` flag to `C:\users\{yourUserAccount}\.ssh\id_ed25519`.
- If you've saved your key to a location other than the default, specify that path you chose when generating your key pair after the `-i` flag instead.
  :::



## What's the SSH password?

If you're being prompted for a password when you attempt to connect, something is amiss. We don't require a password for SSH connections. Some common mistakes that cause your SSH client to prompt for a password include:

- Copying and pasting the key _fingerprint_ (beginning with `SHA256:`) into your RunPod user settings instead of the public key itself (the contents of the `id_ed25519.pub` file when viewed from a text editor)
- Omitting the encryption type from the beginning of the key when copying and pasting into your RunPod user settings (i.e., copying the random text, but not the `ssh-ed25519` which precedes it)
- Not separating different public keys in your RunPod user settings with a newline between each one (this would result in the first public/private key pair functioning as expected, but each subsequent key pair would not work)
- Specifying an incorrect file path to your private key file:


- Attempting to use a private key that other users on the machine have permissions for:


- Incorrect Private Key being used locally in SSH config file.
  There should be a config file on your local machine in your ~/.ssh folder. You want to ensure that the IdentityFile in the config file points to the private key of the public key you used to make this connection. If you are not pointing to the correct private key in the config file, when you make a connection request using your public key, you will get a mismatch and be prompted for a password. Once the correct private key is set in your config file, you can connect without a password.


---

# connect-to-a-pod.md

File: pods/connect-to-a-pod.md

You can connect to a Pod through various methods, depending on your requirements, preferences, and templates used.

## SSH terminal

Connecting to a Pod using an SSH terminal is a secure and reliable method, suitable for long-running processes and critical tasks.

Every Pod contains the ability to connect through SSH.

To do this, you need to have an SSH client installed on your local machine.

1. Open the terminal on your local machine.
2. Choose one of the following commands and then enter it into your machine's terminal:

```bash
# No support for SCP & SFTP
ssh <username>@<pod-ssh-hostname> -i <path-to-ssh-key>

# Supports SCP & SFTP
ssh <username>@<pod-ip-address> -p <ssh-port> -i <path-to-ssh-key>
```

Replace the placeholders with the following:

- `<username>`: Your assigned username for the Pod
- `<pod-ssh-hostname>`: The SSH hostname provided for your Pod
- `<pod-ip-address>`: The IP address of your Pod
- `<ssh-port>`: The designated SSH port for your Pod
- `<path-to-ssh-key>`: The path to your SSH private key file

You now have a secure SSH terminal to your Pod.

## Web terminal

:::note

Depending on your Pod's template will provide the ability to connect to the web terminal.

:::

The web terminal is a convenient, web-based terminal for quickly connecting to your Pod and running commands.

This shouldn't be relied on for long-running process such as training an LLM or other critical tasks.

The web terminal is useful for quickly logging in to your Pod and running commands.

1. On your Pod's page, select **Connect**.
2. Select **Start Web Terminal** then choose **Connect to Web Terminal** in a new window.
3. Enter the **Username** and **Password**.

:::note

You can find the **Username** and **Password** for your web terminal once select **Start Web Terminal**.


:::

---

# logs.md

File: pods/logs.md

Pods provide two types of logs.

- **Container logs** include anything typically sent to your console standard out.

- **System logs** include information on your container's formation and current status, including download, extraction, start, and stop.

To access your logs, go the Pods dashboard and click the **Logs** button on your Pod.

---

# manage-pods.md

File: pods/manage-pods.md

Learn how to start, stop, and manage Pods with RunPod, including creating and terminating Pods, and using the command line interface to manage your Pods.


### Prerequisites

If you are using the [RunPod CLI](/runpodctl/install-runpodctl), you'll need to set your API key in the configuration.

```bash
runpodctl config --apiKey $RUNPOD_API_KEY
```

Replace `$RUNPOD_API_KEY` with your RunPod API key.

Once your API key is set, you can manage your infrastructure.

If you're not sure which Pod meets your needs, see [Choose a Pod](/pods/choose-a-pod).

## Create Pods

```bash
```
    ```bash
    ```
          ```bash
          ```
          ```bash
          ```
```bash
```
```bash
```
```bash
```
```bash
```

---

# networking.md

File: pods/networking.md

This private networking feature enables secure communication between all Pods within your RunPod account. It creates a private, virtual network that connects your Pods, allowing them to communicate with each other as if they were on the same local network, regardless of their physical location.

## How to use it?

**Enable Global Networking**

1. Go to [Pods](https://www.runpod.io/console/pods) section and select **+ Deploy**.
2. Toggle the **Global Networking** to select Pods that have Global Networking enabled.
3. Configure your GPUs and select **Deploy**.

**Access the Private Network**

- Each Pod with Global Networking enabled will be assigned a private IP address.
- The Private IP Address is referenced via the DNS record for the pod using the pattern: `$podid.runpod.internal`
- The DNS record will displayed on the pod details card after the pod is created.

**Run Services**

Start your services on the Pods as usual. They will be accessible to other Pods on the private network without needing to explicitly expose ports.
To test connectivity between pods, open a terminal and run: `ping podid.runpod.internal`

**Public Access** (if needed)

For services that need to be publicly accessible, you still need to expose ports as usual when creating the Pod.
Consider using a "gateway" Pod that exposes public endpoints and then communicates with your private Pods.

**Security Best Practices**

Keep sensitive services (like databases) on private Pods without exposing any public ports.

:::note

Global Networking doesn't increase throughput between Pods, but it does allow for secure, private communication within your account's infrastructure.

:::

For more detailed information or support, please [contact our customer service team](https://contact.runpod.io/hc/en-us/requests/new).

## Current Limitations

- Available only on NVIDIA GPU Pods
- Not yet available for CPU Pods

The following datacenters can take advantage of Global Networking:

- CA-MTL-3
- US-GA-1
- US-GA-2
- US-KS-2

---

# overview.md

File: pods/overview.md

Pods are running container instances.
You can pull an instance from a container registry such as Docker Hub, GitHub Container Registry, Amazon Elastic Container Registry, or another compatible registry.

:::note

When building an image for RunPod on a Mac (Apple Silicon), use the flag `--platform linux/amd64` to ensure your image is compatible with the platform. This flag is necessary because RunPod currently only supports the `linux/amd64` architecture.

:::

### Understanding Pod components and configuration

A Pod is a server container created by you to access the hardware, with a dynamically generated assigned identifier.
For example, `2s56cp0pof1rmt` identifies the instance.

A Pod comprises a container volume with the operating system and temporary storage, a disk volume for permanent storage, an Ubuntu Linux container, allocated vCPU and system RAM, optional GPUs or CPUs for specific workloads, a pre-configured template for easy software access, and a proxy connection for web access.

Each Pod encompasses a variety of components:

- A container volume that houses the operating system and temporary storage.
  - This storage is volatile and will be lost if the Pod is halted or rebooted.
- A disk volume for permanent storage, preserved for the duration of the Pod's lease, akin to a hard disk.
  - This storage is persistent and will be available even if the Pod is halted or rebooted.
- Network storage, similar to a volume but can be moved between machines.
  - When using network storage, you can only delete the Pod.
- An Ubuntu Linux container, capable of running almost any software that can be executed on Ubuntu.
- Assigned vCPU and system RAM dedicated to the container and any processes it runs.
- Optional GPUs or CPUs, tailored for specific workloads like CUDA or AI/ML tasks, though not mandatory for starting the container.
- A pre-configured template that automates the installation of software and settings upon Pod creation, offering straightforward, one-click access to various packages.
- A proxy connection for web access, allowing connectivity to any open port on the container.
  - For example, `https://[pod-id]-[port number].proxy.runpod.net`, or `https://2s56cp0pof1rmt-7860.proxy.runpod.net/`).

To get started, see how to [Choose a Pod](/pods/choose-a-pod) then see the instructions on [Manage Pods](/pods/manage-pods).

## Learn more

You can jump straight to a running Pod by starting from a [template](/pods/templates/overview). For more customization, you can configure the following:

- [GPU Type](/references/gpu-types) and quantity
- System Disk Size
- Start Command
- [Environment Variables](/pods/references/environment-variables)
- [Expose HTTP/TCP ports](/pods/configuration/expose-ports)
- [Persistent Storage Options](/category/storage)

To get started, see how to [Choose a Pod](/pods/choose-a-pod) then see the instructions on [Manage Pods](/pods/manage-pods).

---

# environment-variables.md

File: pods/references/environment-variables.md

You can store the following environment variables in your Pods.

| Variable              | Description                                                                               |
| :-------------------- | :---------------------------------------------------------------------------------------- |
| `RUNPOD_POD_ID`       | The unique identifier for your pod.                                                       |
| `RUNPOD_API_KEY`      | Used to make RunPod API calls to the specific pod. It's limited in scope to only the pod. |
| `RUNPOD_POD_HOSTNAME` | Name of the host server the pod is running on.                                            |
| `RUNPOD_GPU_COUNT`    | Number of GPUs available to the pod.                                                      |
| `RUNPOD_CPU_COUNT`    | Number of CPUs available to the pod.                                                      |
| `RUNPOD_PUBLIC_IP`    | If available, the publicly accessible IP for the pod.                                     |
| `RUNPOD_TCP_PORT_22`  | The public port SSH port 22.                                                              |
| `RUNPOD_ALLOW_IP`     | You can whitelist specific IPs to access your pod, like 192.168.0.12/32, 172.16.0.16/32.  |
| `RUNPOD_DC_ID`        | The data center where the pod is located.                                                 |
| `RUNPOD_VOLUME_ID`    | The ID of the volume connected to the pod.                                                |
| `CUDA_VERSION`        | The installed CUDA version.                                                               |
| `PWD`                 | Current working directory.                                                                |
| `PYTORCH_VERSION`     | Installed PyTorch Version.                                                                |
| `PUBLIC_KEY`          | The SSH public keys to access the pod over SSH.                                           |

---

# savings-plans.md

File: pods/savings-plans.md

Savings Plans are a powerful cost-saving feature designed to optimize your RunPod experience.
Take advantage of upfront payments to unlock discounts on uninterrupted instances, maximize cost efficiency, and get the most out of specific card types on Secure Cloud.

## Get started

To start saving with RunPod's Savings Plans, ensure you have sufficient RunPod credits in your account.

There are two ways to create a savings plan.

- Add a Savings Plan to your existing Pod from the Pod dashboard.
- Initiate a Savings Plan during Pod deployment.

Regularly check the **Savings Plans** section to track your Savings Plans and associated pods.


## Benefits

**Reduced Costs**: By paying upfront for a Savings Plan, you can enjoy discounted rates on uninterrupted instances. This means significant cost savings for your RunPod deployments.

**Flexible Savings**: When you stop a Pod, the Savings Plan associated with it applies to your next deployment of the same card. This means you continue to benefit from your savings commitment even after temporary pauses in your pod usage.

:::warning

Stopping your Pod does not extend your Savings Plan.
Each Savings Plan has a fixed expiration date, which you set at when you buy the plan.

:::

**Instant Activation**: Savings Plans kick in immediately upon activation and remain active for the duration of your committed period. You can start saving from the moment you initiate a Savings Plan.

**Easy Management**: Adding a Savings Plan to your existing running Pod is a breeze through the Pod dashboard. Alternately, Savings Plans are automatically started when you deploy a new Pod, simplifying the process and ensuring you don't miss out on potential savings.

**Clear Visibility**: Stay on top of your savings commitments and associated Pods by navigating to the **Savings Plan** page. This shows a comprehensive overview of your Savings Plans for effective monitoring and management.

---

# _volume.md

File: pods/storage/_volume.md

Volumes can be:

## Persistent

## Ephemeral

---

# create-network-volumes.md

File: pods/storage/create-network-volumes.md

Network Volumes are a feature specific to Secure Cloud that allows you to create a volume that multiple pods can interact with. This gives you an extra amount of flexibility to keep working--especially if you are working with a high demand GPU pool that may not always be available--as you can simply create a pod in a different pool while you wait for an option to free up. This can also save you time by downloading frequently used models or other large files to a volume and holding them for later use, rather than having to re-download them every time you spin up a new pod.

**How to Create a Volume**

Under the Secure Cloud page, click the option to create a volume.


The pricing for the volume will be shown, and you can select a data center and provide a name and the requested size.


Once you create the volume, it will appear in your list.


Once you click the Deploy option, your container size will be locked to the size of your network volume. Note that there will be a nominal cost for network volume storage, in lieu of the disk cost normally quoted. Also note that you can link many pods to one singular network volume, and you will enjoy an overall cost saving even with just two pods sharing one volume (as opposed to setting up two pods with separate volumes), despite the presence of this additional cost.


**What's the infrastructure behind Network Volume**

When you choose to create a Network Volume, you gain access to our robust infrastructure, which includes state-of-the-art storage servers located in the same datacenters where you rent GPU servers from us. These servers are connected via a high-speed 25Gbps local network, up to 200Gbps in some locations, guaranteeing efficient data transfer and minimal latency. Everything is stored on high-speed NVME SSDs to ensure best performance.

If you're interested in harnessing the advantages of Network Volume and its cost-effective storage solutions, we invite you to read our detailed [blog article](https://blog.runpod.io/four-reasons-to-set-up-a/). It explores the benefits and features of Network Volume, helping you make an informed decision about your storage needs.

**Please note that if your machine-based storage or network volume is terminated due to lack of funds, that disk space is immediately freed up for use by other clients, and RunPod is unable to assist in recovering lost storage.**

---

# sync-volumes.md

File: pods/storage/sync-volumes.md

You can sync your volume to a cloud provider by clicking the **Cloud Sync** option under your **My Pods** page. For detailed instructions on connecting to AWS S3, Google Cloud Storage, Azure, Backblaze, Dropbox, and configuring these services, please refer to this [configuration guide](../configuration/export-data.md).

---

# transfer-files.md

File: pods/storage/transfer-files.md

Learn to transfer files to and from RunPod.

## Prerequisites

- If you intend to use `runpodctl`, make sure it's installed on your machine, see [install runpodctl](../../runpodctl/install-runpodctl.md)

- If you intend to use `scp`, make sure your Pod is configured to use real SSH.
  For more information, see [use SSH](/pods/configuration/use-ssh).

- If you intend to use `rsync`, make sure it's installed on both your local machine and your Pod with `apt install rsync`.

- Note the public IP address and external port from the SSH over exposed TCP command (you'll need these for the SCP/rsync commands).

## Transferring with [runpodctl](../../runpodctl/overview.md#data-transfer)

The RunPod CLI (runpodctl) provides simple commands for transferring data between your machine and RunPod. **It’s preinstalled on all RunPod Pods** and uses one-time codes for secure authentication, so no API keys are required.

#### Sending a File

To send a file from source machine:

```bash
runpodctl send data.txt
```

Example output:

```bash
Sending 'data.txt' (5 B)
Code is: 8338-galileo-collect-fidel
On the other computer run

runpodctl receive 8338-galileo-collect-fidel
```

#### Receiving a File

To receive a file on destination machine:

```bash
runpodctl receive 8338-galileo-collect-fidel
```

Example output:

```bash
Receiving 'data.txt' (5 B)

Receiving (<-149.36.0.243:8692)
data.txt 100% |████████████████████| ( 5/ 5B, 0.040 kB/s)
```

## Transferring with SCP

The general syntax for sending files to a Pod with SCP is as follows (execute this on your local machine, and replace the x's with your Pod's external TCP port and IP; for this example, they are 43201 and 194.26.196.6, respectively):

```shell
scp -P 43201 -i ~/.ssh/id_ed25519 /local/file/path root@194.26.196.6:/destination/file/path
```

:::note

If your private key file is in a location other than `~/.ssh/id_ed25519` or you're using the Windows Command Prompt, make sure you update this path accordingly in your command.

:::

Example of sending a file to a Pod:

```shell
scp -P 43201 -i ~/.ssh/id_ed25519 ~/documents/example.txt root@194.26.196.6:/root/example.txt
```

If you want to receive a file from your Pod, switch the source and destination arguments:

```shell
scp -P 43201 -i ~/.ssh/id_ed25519 root@194.26.196.6:/root/example.txt ~/documents/example.txt
```

If you need to transfer a directory, use the `-r` flag to recursively copy files and subdirectories (this will follow any symbolic links encountered as well):

```shell
scp -r -P 43201 -i ~/.ssh/id_ed25519 ~/documents/example_dir root@194.26.196.6:/root/example_dir
```

## Transferring with rsync

:::note

Your local machine must be running Linux or a [WSL instance](https://learn.microsoft.com/en-us/windows/wsl/about) in order to use rsync.

:::

The general syntax for sending files to a Pod with rsync is as follows (execute this on your local machine, and replace the x's with your Pod's external TCP port and IP):

```shell
rsync -e "ssh -p 43201" /source/file/path root@194.26.196.6:/destination/file/path
```

Some helpful flags include:

- `-a`/`--archive` - archive mode (ensures that permissions, timestamps, and other attributes are preserved during the transfer; use this when transferring directories or their contents)
- `-d`/`--delete` - deletes files in the destination directory that are not present in the source
- `-p`/`--progress` - displays file transfer progress
- `-v`/`--verbose` - verbose output
- `-z`/`--compress` - compresses data as it's being sent and uncompresses as it's received (heavier on your CPU, but easier on your network connection)

Example of sending a file to a Pod using rsync:

```shell
rsync -avz -e "ssh -p 43201" ~/documents/example.txt root@194.26.196.6:/root/example.txt
```

If you want to receive a file from your Pod, switch the source and destination arguments:

```shell
rsync -avz -e "ssh -p 43201" root@194.26.196.6:/root/example.txt ~/documents/example.txt
```

To transfer the contents of a directory (without transferring the directory itself), use a trailing slash in the file path:

```shell
rsync -avz -e "ssh -p 43201" ~/documents/example_dir/ root@194.26.196.6:/root/example_dir/
```

Without a trailing slash, the directory itself is transferred:

```shell
rsync -avz -e "ssh -p 43201" ~/documents/example_dir root@194.26.196.6:/root/
```

An advantage of rsync is that files that already exist at the destination aren't transferred again if you attempt to copy them twice (note the minimal data transfer after the second execution):

```shell
rsync -avz -e "ssh -p 43201" ~/documents/example.txt root@194.26.196.6:/root/example.txt
sending incremental file list
example.txt
             119 100%    0.00kB/s    0:00:00 (xfr#1, to-chk=0/1)

sent 243 bytes  received 35 bytes  185.33 bytes/sec
total size is 119  speedup is 0.43

$ rsync -avz -e "ssh -p 43201" ~/documents/example.txt root@194.26.196.6:/root/example.txt
sending incremental file list

sent 120 bytes  received 12 bytes  88.00 bytes/sec
total size is 119  speedup is 0.90
```

## Sync a volume to a cloud provider

You can sync your volume to a cloud provider by clicking the **Cloud Sync** option under your **My Pods** page, For detailed instructions on connecting to AWS S3, Google Cloud Storage, Azure, Backblaze, Dropbox, and configuring these services, please refer to this [configuration guide](../configuration/export-data.md).

---

# types.md

File: pods/storage/types.md

The following section describes the different types of storage and volume options.

## Container volume

A container volume is a type of storage that houses the operating system and provides temporary storage for a Pod.
It is created when a Pod is launched and is tightly coupled with the Pod's lifecycle.

**Key characteristics:**

- Volatile storage that is lost if the Pod is halted or rebooted
- Suitable for storing temporary data or files that are not required to persist beyond the Pod's lifecycle
- Capacity is determined by the selected Pod configuration
- Provides fast read and write speeds as it is locally attached to the Pod

## Disk volume

A disk volume is a type of persistent storage that is preserved for the duration of the Pod's lease.
It functions similarly to a hard disk, allowing you to store data that needs to be retained even if the Pod is halted or rebooted.

**Key characteristics:**

- Persistent storage that remains available throughout the Pod's lease period
- Suitable for storing data, models, or files that need to be preserved across Pod restarts or reconfigurations
- Capacity can be selected based on storage requirements
- Provides reliable data persistence but may have slightly slower read and write speeds compared to container volumes

## Network storage

Network storage is a type of storage that is similar to a disk volume but offers the flexibility to be moved between different machines.
It provides a way to store and access data across multiple Pods or instances.

**Key characteristics:**

- Persistent storage that can be attached to different Pods or machines
- Suitable for scenarios where data needs to be shared or accessed by multiple Pods
- Allows for data portability and facilitates collaboration between different instances
- Provides data persistence and the ability to move storage between Pods
- When using network storage, you can only delete the Pod, as the storage is managed separately

---

# manage-templates.md

File: pods/templates/manage-templates.md

## Explore Templates

You can explore Templates managed by RunPod and Community Templates in the **[Explore](https://www.runpod.io/console/explore)** section of the Web interface.

You can explore Templates managed by you or your team in the **[Templates](https://www.runpod.io/console/user/templates)** section of the Web interface.

Learn to create your own Template in the following section.

## Creating a Template

Templates are used to launch images as a Pod: within a template, you define the required container disk size, volume, volume path, and ports needed.

### Web interface


### cURL

You can also create or modify a template using the RunPod API.

```curl
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "mutation { saveTemplate(input: { containerDiskInGb: 5, dockerArgs: \"sleep infinity\", env: [ { key: \"key1\", value: \"value1\" }, { key: \"key2\", value: \"value2\" } ], imageName: \"ubuntu:latest\", name: \"Generated Template\", ports: \"8888/http,22/tcp\", readme: \"## Hello, World!\", volumeInGb: 15, volumeMountPath: \"/workspace\" }) { containerDiskInGb dockerArgs env { key value } id imageName name ports readme volumeInGb volumeMountPath } }"}'
```

### Environment variables

Environment variables in RunPod templates are key-value pairs that are accessible within your pod. Define a variable by setting a name with the `key` and then what it should contain with the `value`.

Use environment variables to pass configuration settings and secrets to your container. For example, environment variables can store the path to a database or API keys used by your application.


RunPod also provides a set of predefined [environment variables](/pods/references/environment-variables) that provide information about the pod, such as the unique identifier for your pod (`RUNPOD_POD_ID`), the API key used to make RunPod API calls to the specific pod (`RUNPOD_API_KEY`), the name of the host server the pod is running on (`RUNPOD_POD_HOSTNAME`), and more.

You can references [Secrets](/pods/templates/secrets) in your Pod templates.

---

# overview.md

File: pods/templates/overview.md

Templates are Docker containers images paired with a configuration.

They are used to launch images as Pods, define the required container disk size, volume, volume paths, and ports needed.

You can also define environment variables within the Template.

## Template types

There a few types of Templates:

- **Managed by RunPod**: Also known as official Templates; these templates are created and maintained by RunPod.
- **Custom Templates**:
  - **Community Templates**: Custom Templates shared by the community.
  - **Private Templates**: Custom Templates created by you or if using a team account, shared inside your team.

### Custom Templates

### Customizing Container Start Command

You can customize the Docker command to run additional commands or modify the default behavior.

The Docker command is specified in the **Container Start Command** field.

**Default Docker Command**

The default Docker command is:

```bash
bash -c '/start.sh'
```

This command runs the `/start.sh` script at the end of the container startup process.

You can customize the Docker command to run additional commands or modify the default behavior.

For example, you can add a command to run before `/start.sh`:

```bash
bash -c 'mkdir /testdir1 && /start.sh'
```

This command creates a directory `/testdir1` before running `/start.sh`.

**Using the `entrypoint` Field**

You can also specify a JSON string with `cmd` and `entrypoint` as the keys.

The `entrypoint` field allows you to specify a command to run at the beginning of the container startup process. For example:

```json
{ "cmd": ["echo foo && /start.sh"], "entrypoint": ["bash", "-c"] }
```

This command runs the `echo` command and then runs `/start.sh`.

**Important Considerations**

When using the `entrypoint` field, be aware that the command will run twice: once as the entrypoint and again as part of the `cmd` field.

This can cause issues if the command errors when run a second time.
For example:

```json
{ "cmd": ["mkdir /testdir11 && /start.sh"], "entrypoint": ["bash", "-c"] }
```

This command will run `mkdir` twice, which can cause errors if the directory already exists.

**Tips and Examples**

Here are some working examples to try in dev:

- Command only: `bash -c 'mkdir /testdir1 && /start.sh'`
- Command only: `{"cmd": ["bash", "-c", "mkdir /testdir8 && /start.sh"]}`
- Command and Entrypoint: `{"cmd": ["test-echo-test-echo"], "entrypoint": ["echo"]}`
- Command and Entrypoint: `{"cmd": ["mkdir -p /testdir12 && /start.sh"], "entrypoint": ["bash", "-c"]}`

:::note

Remember to use `mkdir -p` to avoid errors when creating directories.

:::

---

# secrets.md

File: pods/templates/secrets.md

You can add Secrets to your Pods and templates.
Secrets are encrypted strings of text that are used to store sensitive information, such as passwords, API keys, and other sensitive data.

## Create a Secret

You can create a Secret using the RunPod Web interface or the RunPod API.

1. Login into the RunPod Web interface and select [Secrets](https://www.runpod.io/console/user/secrets).
2. Choose **Create Secret** and provide the following:
   1. **Secret Name**: The name of the Secret.
   2. **Secret Value**: The value of the Secret.
   3. **Description**: (optional) A description of the Secret.
3. Select **Create Secret**.

:::note

Once a Secret is created, its value cannot be viewed.
If you need to change the Secret, you must create a new one or [modify the Secret Value](#modify-a-secret).

:::

## Modify a Secret

You can modify an existing Secret using the RunPod Web interface.

1. Login into the RunPod Web interface and select [Secrets](https://www.runpod.io/console/user/secrets).
2. Select the name of the Secret you want to modify.
3. Select the configuration icon and choose **Edit Secret Value**.
   1. Enter your new Secret Value.
4. Select **Save Changes**.

## View Secret details

You can view the details of an existing Secret using the RunPod Web interface.
You can't view the Secret Value.

1. Login into the RunPod Web interface and select [Secrets](https://www.runpod.io/console/user/secrets).
2. Select the name of the Secret you want to view.
3. Select the configuration icon and choose **View Secret**.

## Use a Secret in a Pod

With your Secrets setup, you can now reference them in your Pods.

You can reference your Secret directly or select it from the Web interface when creating or modifying a Pod template.

**Reference your Secret directly**

You can reference your Secret directly in the [Environment Variables](/pods/references/environment-variables) section of your Pod template.
To reference your Secret, reference it's key appended to the `RUNPOD_SECRET_` prefix.
For example:

```yml
{{ RUNPOD_SECRET_hello_world }}
```

Where `hello_world` is the value of your Secret Name.

**Select your Secret from the Web interface**

Alternatively, you can select your Secret from the Web interface when creating or modifying a Pod template.

## Delete a Secret

You can delete an existing Secret using the RunPod Web interface.

1. Login into the RunPod Web interface and select [Secrets](https://www.runpod.io/console/user/secrets).
2. Select the name of the Secret you want to delete.
3. Select the configuration icon and choose **Delete Secret**.
4. Enter the name of the Secret to confirm deletion.
5. Select **Confirm Delete**.

---

# _api-endpoints.md

File: references/_api-endpoints.md



---

# cpu-types.md

File: references/cpu-types.md

The following list contains all CPU types available on RunPod.

<!--
Table last generated: 2024-12-27
-->

| displayName                                     | cores | threadsPerCore |
| :---------------------------------------------- | ----: | -------------: |
| 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz   |     6 |              2 |
| 11th Gen Intel(R) Core(TM) i5-11400F @ 2.60GHz  |     6 |              2 |
| 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz  |     2 |              1 |
| 11th Gen Intel(R) Core(TM) i7-11700 @ 2.50GHz   |     8 |              2 |
| 11th Gen Intel(R) Core(TM) i7-11700F @ 2.50GHz  |     8 |              2 |
| 11th Gen Intel(R) Core(TM) i7-11700K @ 3.60GHz  |     8 |              2 |
| 11th Gen Intel(R) Core(TM) i7-11700KF @ 3.60GHz |     8 |              2 |
| 11th Gen Intel(R) Core(TM) i9-11900K @ 3.50GHz  |     8 |              2 |
| 11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz |     8 |              2 |
| 12th Gen Intel(R) Core(TM) i3-12100             |     4 |              2 |
| 12th Gen Intel(R) Core(TM) i7-12700F            |    12 |              1 |
| 12th Gen Intel(R) Core(TM) i7-12700K            |    12 |              1 |
| 13th Gen Intel(R) Core(TM) i3-13100F            |     4 |              2 |
| 13th Gen Intel(R) Core(TM) i5-13600K            |    14 |              1 |
| 13th Gen Intel(R) Core(TM) i7-13700K            |    16 |              1 |
| 13th Gen Intel(R) Core(TM) i7-13700KF           |    16 |              1 |
| 13th Gen Intel(R) Core(TM) i9-13900F            |    24 |              1 |
| 13th Gen Intel(R) Core(TM) i9-13900K            |    24 |              1 |
| 13th Gen Intel(R) Core(TM) i9-13900KF           |    24 |              1 |
| AMD EPYC 4564P 16-Core Processor                |    16 |              2 |
| AMD EPYC 7251 8-Core Processor                  |     8 |              2 |
| AMD EPYC 7252 8-Core Processor                  |     8 |              2 |
| AMD EPYC 7272 12-Core Processor                 |    12 |              2 |
| AMD EPYC 7281 16-Core Processor                 |    16 |              2 |
| AMD EPYC 7282 16-Core Processor                 |    16 |              2 |
| AMD EPYC 7302 16-Core Processor                 |    16 |              2 |
| AMD EPYC 7302P 16-Core Processor                |    16 |              2 |
| AMD EPYC 7313 16-Core Processor                 |    16 |              2 |
| AMD EPYC 7313P 16-Core Processor                |    16 |              2 |
| AMD EPYC 7343 16-Core Processor                 |    16 |              2 |
| AMD EPYC 7351P 16-Core Processor                |    16 |              2 |
| AMD EPYC 7352 24-Core Processor                 |    24 |              2 |
| AMD EPYC 7371 16-Core Processor                 |    16 |              2 |
| AMD EPYC 7402 24-Core Processor                 |    24 |              2 |
| AMD EPYC 7402P 24-Core Processor                |    24 |              2 |
| AMD EPYC 7413 24-Core Processor                 |    24 |              2 |
| AMD EPYC 7443 24-Core Processor                 |    48 |              1 |
| AMD EPYC 7443P 24-Core Processor                |    24 |              2 |
| AMD EPYC 7452 32-Core Processor                 |    32 |              2 |
| AMD EPYC 7453 28-Core Processor                 |    28 |              1 |
| AMD EPYC 74F3 24-Core Processor                 |    24 |              2 |
| AMD EPYC 7502 32-Core Processor                 |    32 |              1 |
| AMD EPYC 7502P 32-Core Processor                |    32 |              1 |
| AMD EPYC 7513 32-Core Processor                 |    32 |              2 |
| AMD EPYC 7532 32-Core Processor                 |    32 |              2 |
| AMD EPYC 7542 32-Core Processor                 |    32 |              2 |
| AMD EPYC 7543 32-Core Processor                 |    28 |              1 |
| AMD EPYC 7543P 32-Core Processor                |    32 |              2 |
| AMD EPYC 7551 32-Core Processor                 |    32 |              2 |
| AMD EPYC 7551P 32-Core Processor                |    32 |              2 |
| AMD EPYC 7552 48-Core Processor                 |    48 |              2 |
| AMD EPYC 75F3 32-Core Processor                 |    32 |              2 |
| AMD EPYC 7601 32-Core Processor                 |    32 |              2 |
| AMD EPYC 7642 48-Core Processor                 |    48 |              2 |
| AMD EPYC 7643 48-Core Processor                 |    48 |              2 |
| AMD EPYC 7663 56-Core Processor                 |    56 |              2 |
| AMD EPYC 7702 64-Core Processor                 |    64 |              2 |
| AMD EPYC 7702P 64-Core Processor                |    64 |              2 |
| AMD EPYC 7713 64-Core Processor                 |    64 |              1 |
| AMD EPYC 7742 64-Core Processor                 |    64 |              2 |
| AMD EPYC 7763 64-Core Processor                 |    64 |              2 |
| AMD EPYC 7773X 64-Core Processor                |    64 |              2 |
| AMD EPYC 7B12 64-Core Processor                 |    64 |              2 |
| AMD EPYC 7B13 64-Core Processor                 |    64 |              1 |
| AMD EPYC 7C13 64-Core Processor                 |    64 |              2 |
| AMD EPYC 7F32 8-Core Processor                  |     8 |              2 |
| AMD EPYC 7F52 16-Core Processor                 |    16 |              2 |
| AMD EPYC 7F72 24-Core Processor                 |    24 |              2 |
| AMD EPYC 7H12 64-Core Processor                 |    64 |              2 |
| AMD EPYC 7K62 48-Core Processor                 |    48 |              2 |
| AMD EPYC 7R32 48-Core Processor                 |    48 |              2 |
| AMD EPYC 7T83 64-Core Processor                 |   127 |              1 |
| AMD EPYC 7V13 64-Core Processor                 |    24 |              1 |
| AMD EPYC 9124 16-Core Processor                 |    16 |              2 |
| AMD EPYC 9254 24-Core Processor                 |    24 |              2 |
| AMD EPYC 9354 32-Core Processor                 |    32 |              2 |
| AMD EPYC 9354P 32-Core Processor                |    32 |              2 |
| AMD EPYC 9374F 32-Core Processor                |    32 |              1 |
| AMD EPYC 9454 48-Core Processor                 |    48 |              2 |
| AMD EPYC 9474F 48-Core Processor                |    48 |              2 |
| AMD EPYC 9534 64-Core Processor                 |    64 |              2 |
| AMD EPYC 9554 64-Core Processor                 |   126 |              1 |
| AMD EPYC 9654 96-Core Emb Processor             |    96 |              1 |
| AMD EPYC 9654 96-Core Processor                 |    96 |              2 |
| AMD EPYC 9754 128-Core Processor                |   128 |              2 |
| AMD EPYC Processor                              |     1 |              1 |
| AMD EPYC Processor (with IBPB)                  |    16 |              1 |
| AMD EPYC-Rome Processor                         |    16 |              1 |
| AMD Eng Sample: 100-000000053-04_32/20_N        |    48 |              1 |
| AMD Ryzen 3 2200G with Radeon Vega Graphics     |     4 |              1 |
| AMD Ryzen 3 3200G with Radeon Vega Graphics     |     4 |              1 |
| AMD Ryzen 3 4100 4-Core Processor               |     4 |              2 |
| AMD Ryzen 5 1600 Six-Core Processor             |     6 |              2 |
| AMD Ryzen 5 2600 Six-Core Processor             |     6 |              2 |
| AMD Ryzen 5 2600X Six-Core Processor            |     6 |              2 |
| AMD Ryzen 5 3600 6-Core Processor               |     6 |              2 |
| AMD Ryzen 5 3600X 6-Core Processor              |     6 |              2 |
| AMD Ryzen 5 5500                                |     6 |              2 |
| AMD Ryzen 5 5600G with Radeon Graphics          |     6 |              2 |
| AMD Ryzen 5 7600 6-Core Processor               |     6 |              2 |
| AMD Ryzen 5 PRO 2600 Six-Core Processor         |     6 |              2 |
| AMD Ryzen 7 1700 Eight-Core Processor           |     8 |              2 |
| AMD Ryzen 7 1700X Eight-Core Processor          |     8 |              2 |
| AMD Ryzen 7 5700G with Radeon Graphics          |     8 |              2 |
| AMD Ryzen 7 5700X 8-Core Processor              |     8 |              2 |
| AMD Ryzen 7 5800X 8-Core Processor              |     8 |              2 |
| AMD Ryzen 7 7700 8-Core Processor               |     8 |              2 |
| AMD Ryzen 7 PRO 3700 8-Core Processor           |     8 |              2 |
| AMD Ryzen 9 3900X 12-Core Processor             |    12 |              2 |
| AMD Ryzen 9 5950X 16-Core Processor             |    16 |              2 |
| AMD Ryzen 9 7900 12-Core Processor              |    12 |              2 |
| AMD Ryzen 9 7950X 16-Core Processor             |    16 |              2 |
| AMD Ryzen Threadripper 1900X 8-Core Processor   |     8 |              2 |
| AMD Ryzen Threadripper 1920X 12-Core Processor  |    12 |              2 |
| AMD Ryzen Threadripper 1950X 16-Core Processor  |    16 |              2 |
| AMD Ryzen Threadripper 2920X 12-Core Processor  |    12 |              2 |
| AMD Ryzen Threadripper 2950X 16-Core Processor  |    16 |              2 |
| AMD Ryzen Threadripper 2970WX 24-Core Processor |    24 |              1 |
| AMD Ryzen Threadripper 2990WX 32-Core Processor |    32 |              2 |
| AMD Ryzen Threadripper 3960X 24-Core Processor  |    24 |              2 |
| AMD Ryzen Threadripper 7960X 24-Cores           |    24 |              2 |
| AMD Ryzen Threadripper PRO 3975WX 32-Cores      |    32 |              2 |
| AMD Ryzen Threadripper PRO 3995WX 64-Cores      |    64 |              2 |
| AMD Ryzen Threadripper PRO 5945WX 12-Cores      |    12 |              2 |
| AMD Ryzen Threadripper PRO 5955WX 16-Cores      |    16 |              2 |
| AMD Ryzen Threadripper PRO 5965WX 24-Cores      |    24 |              2 |
| AMD Ryzen Threadripper PRO 5975WX 32-Cores      |    32 |              2 |
| AMD Ryzen Threadripper PRO 5995WX 64-Cores      |    18 |              1 |
| AMD Ryzen Threadripper PRO 7985WX 64-Cores      |   112 |              1 |
| Common KVM processor                            |    28 |              1 |
| Genuine Intel(R) CPU $0000%@                    |    24 |              2 |
| Genuine Intel(R) CPU @ 2.20GHz                  |    14 |              2 |
| INTEL(R) XEON(R) PLATINUM 8558                  |    48 |              2 |
| INTEL(R) XEON(R) PLATINUM 8568Y+                |    48 |              2 |
| Intel Xeon Processor (Icelake)                  |    40 |              2 |
| Intel(R) Celeron(R) CPU G3900 @ 2.80GHz         |     2 |              1 |
| Intel(R) Celeron(R) G5905 CPU @ 3.50GHz         |     2 |              1 |
| Intel(R) Core(TM) i3-10100F CPU @ 3.60GHz       |     4 |              2 |
| Intel(R) Core(TM) i3-10105F CPU @ 3.70GHz       |     4 |              2 |
| Intel(R) Core(TM) i3-6100 CPU @ 3.70GHz         |     2 |              2 |
| Intel(R) Core(TM) i3-9100F CPU @ 3.60GHz        |     4 |              1 |
| Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz        |     6 |              2 |
| Intel(R) Core(TM) i5-10400F CPU @ 2.90GHz       |     6 |              2 |
| Intel(R) Core(TM) i5-10600 CPU @ 3.30GHz        |     6 |              2 |
| Intel(R) Core(TM) i5-14500                      |    14 |              2 |
| Intel(R) Core(TM) i5-14600KF                    |    14 |              2 |
| Intel(R) Core(TM) i5-4570 CPU @ 3.20GHz         |     4 |              1 |
| Intel(R) Core(TM) i5-6400 CPU @ 2.70GHz         |     4 |              1 |
| Intel(R) Core(TM) i5-6500 CPU @ 3.20GHz         |     4 |              1 |
| Intel(R) Core(TM) i5-7400 CPU @ 3.00GHz         |     4 |              1 |
| Intel(R) Core(TM) i5-9400F CPU @ 2.90GHz        |     6 |              1 |
| Intel(R) Core(TM) i7-10700F CPU @ 2.90GHz       |     8 |              2 |
| Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz       |     8 |              2 |
| Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz         |     4 |              2 |
| Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz         |     4 |              2 |
| Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz         |     4 |              2 |
| Intel(R) Core(TM) i7-6700K CPU @ 4.00GHz        |     4 |              2 |
| Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz        |     6 |              2 |
| Intel(R) Core(TM) i7-7700K CPU @ 4.20GHz        |     4 |              2 |
| Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz         |     6 |              2 |
| Intel(R) Core(TM) i7-9700 CPU @ 3.00GHz         |     8 |              1 |
| Intel(R) Core(TM) i9-10940X CPU @ 3.30GHz       |    14 |              2 |
| Intel(R) Core(TM) i9-14900K                     |    24 |              1 |
| Intel(R) Pentium(R) CPU G3260 @ 3.30GHz         |     2 |              1 |
| Intel(R) Pentium(R) CPU G4560 @ 3.50GHz         |     2 |              2 |
| Intel(R) Xeon(R) Bronze 3204 CPU @ 1.90GHz      |     6 |              1 |
| Intel(R) Xeon(R) CPU X5660 @ 2.80GHz            |     6 |              2 |
| Intel(R) Xeon(R) CPU E3-1220 v3 @ 3.10GHz       |     4 |              1 |
| Intel(R) Xeon(R) CPU E3-1225 V2 @ 3.20GHz       |     4 |              1 |
| Intel(R) Xeon(R) CPU E5-1650 v4 @ 3.60GHz       |     6 |              2 |
| Intel(R) Xeon(R) CPU E5-2603 v3 @ 1.60GHz       |     6 |              1 |
| Intel(R) Xeon(R) CPU E5-2609 0 @ 2.40GHz        |     4 |              1 |
| Intel(R) Xeon(R) CPU E5-2609 v3 @ 1.90GHz       |     1 |              1 |
| Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz       |     8 |              2 |
| Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz        |     6 |              2 |
| Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz       |     6 |              2 |
| Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz       |     8 |              2 |
| Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz       |    10 |              2 |
| Intel(R) Xeon(R) CPU E5-2637 v2 @ 3.50GHz       |     4 |              2 |
| Intel(R) Xeon(R) CPU E5-2643 0 @ 3.30GHz        |     4 |              1 |
| Intel(R) Xeon(R) CPU E5-2648L v3 @ 1.80GHz      |    12 |              2 |
| Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz       |    10 |              2 |
| Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz       |    12 |              2 |
| Intel(R) Xeon(R) CPU E5-2660 v2 @ 2.20GHz       |    10 |              2 |
| Intel(R) Xeon(R) CPU E5-2667 v2 @ 3.30GHz       |     1 |              1 |
| Intel(R) Xeon(R) CPU E5-2667 v3 @ 3.20GHz       |     8 |              2 |
| Intel(R) Xeon(R) CPU E5-2667 v4 @ 3.20GHz       |     1 |              1 |
| Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz        |     8 |              2 |
| Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz       |    10 |              2 |
| Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz       |    20 |              2 |
| Intel(R) Xeon(R) CPU E5-2678 v3 @ 2.50GHz       |    12 |              2 |
| Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz       |    12 |              2 |
| Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz       |    14 |              2 |
| Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz       |    16 |              2 |
| Intel(R) Xeon(R) CPU E5-2690 0 @ 2.90GHz        |     8 |              2 |
| Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz       |    14 |              2 |
| Intel(R) Xeon(R) CPU E5-2695 v4 @ 2.10GHz       |    18 |              2 |
| Intel(R) Xeon(R) CPU E5-2696 v3 @ 2.30GHz       |    18 |              2 |
| Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20GHz       |    22 |              2 |
| Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz       |    16 |              2 |
| Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz       |    20 |              2 |
| Intel(R) Xeon(R) CPU E5-2699 v3 @ 2.30GHz       |     1 |              1 |
| Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz       |    22 |              2 |
| Intel(R) Xeon(R) CPU E5-4667 v3 @ 2.00GHz       |    16 |              2 |
| Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz        |    12 |              2 |
| Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz       |    20 |              2 |
| Intel(R) Xeon(R) Gold 5320 CPU @ 2.20GHz        |    26 |              2 |
| Intel(R) Xeon(R) Gold 6133 CPU @ 2.50GHz        |    40 |              1 |
| Intel(R) Xeon(R) Gold 6136 CPU @ 3.00GHz        |    12 |              2 |
| Intel(R) Xeon(R) Gold 6138 CPU @ 2.00GHz        |    20 |              2 |
| Intel(R) Xeon(R) Gold 6150 CPU @ 2.70GHz        |    18 |              2 |
| Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz        |    12 |              2 |
| Intel(R) Xeon(R) Gold 6238R CPU @ 2.20GHz       |    28 |              2 |
| Intel(R) Xeon(R) Gold 6240R CPU @ 2.40GHz       |    24 |              2 |
| Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz       |    16 |              1 |
| Intel(R) Xeon(R) Gold 6252 CPU @ 2.10GHz        |    24 |              1 |
| Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz       |    22 |              2 |
| Intel(R) Xeon(R) Gold 6342 CPU @ 2.80GHz        |    24 |              2 |
| Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz        |    28 |              2 |
| Intel(R) Xeon(R) Gold 6448Y                     |    32 |              2 |
| Intel(R) Xeon(R) Platinum 8160 CPU @ 2.10GHz    |    24 |              2 |
| Intel(R) Xeon(R) Platinum 8171M CPU @ 2.60GHz   |    26 |              2 |
| Intel(R) Xeon(R) Platinum 8173M CPU @ 2.00GHz   |    28 |              2 |
| Intel(R) Xeon(R) Platinum 8176M CPU @ 2.10GHz   |    28 |              2 |
| Intel(R) Xeon(R) Platinum 8180 CPU @ 2.50GHz    |    28 |              2 |
| Intel(R) Xeon(R) Platinum 8352Y CPU @ 2.20GHz   |    32 |              2 |
| Intel(R) Xeon(R) Platinum 8452Y                 |    36 |              2 |
| Intel(R) Xeon(R) Platinum 8462Y+                |    32 |              2 |
| Intel(R) Xeon(R) Platinum 8468                  |    48 |              2 |
| Intel(R) Xeon(R) Platinum 8468V                 |    44 |              2 |
| Intel(R) Xeon(R) Platinum 8470                  |    52 |              2 |
| Intel(R) Xeon(R) Platinum 8480+                 |    56 |              2 |
| Intel(R) Xeon(R) Platinum 8480C                 |    56 |              2 |
| Intel(R) Xeon(R) Platinum 8480CL                |    56 |              2 |
| Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz      |    10 |              2 |
| Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz      |    10 |              2 |
| Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz      |    24 |              1 |
| Intel(R) Xeon(R) Silver 4310T CPU @ 2.30GHz     |    10 |              2 |
| Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz      |    16 |              2 |
| Intel(R) Xeon(R) W-2223 CPU @ 3.60GHz           |     4 |              2 |
| QEMU Virtual CPU version 2.5+                   |    16 |              1 |
| Ryzen 5 5600X                                   |     6 |              2 |
| Ryzen 9 5900X                                   |    12 |              2 |
| Ryzen Threadripper PRO 3955WX                   |    16 |              2 |

---

# faq.md

File: references/faq/faq.md

## Secure Cloud vs Community Cloud

RunPod provides two cloud computing services: [Secure Cloud](https://www.runpod.io/console/gpu-secure-cloud) and [Community Cloud.](https://www.runpod.io/console/gpu-cloud)

**Secure Cloud** runs in T3/T4 data centers by our trusted partners. Our close partnership comes with high-reliability with redundancy, security, and fast response times to mitigate any downtimes. For any sensitive and enterprise workloads, we highly recommend Secure Cloud.

**Community Cloud** brings power in numbers and diversity spanning the whole world. Through our decentralized platform, we can offer peer-to-peer GPU computing that connects individual compute providers to compute consumers. Our Community Cloud hosts are invite-only and vetted by us, and still have to abide by our standards. Even though their associated infrastructure might not offer as much redundancy for power and networking, they still offer good servers that combine quality and affordability.

Both solutions offer far more competitive prices than large cloud providers such as AWS or GCP.

## On-Demand vs. Spot Pod

**On-Demand Pods** can run forever without interruptions with resources dedicated to your Pod. They do incur higher costs than Spot Pods.

**Spot Pods** use spare compute capacity, allowing you to bid for those compute resources. Resources are dedicated to your Pod, but someone else can bid higher or start an On-Demand Pod that will stop your Pod. When this happens, your Pod is given a signal to stop 5 seconds prior with SIGTERM, and eventually, the kill signal SIGKILL after 5 seconds. You can use volumes to save any data to the disk in that 5s period or push data to the cloud periodically.

### How does RunPod work?

RunPod leverages technologies like [Docker](https://www.docker.com/) to containerize and isolate guest workloads on a host machine. We have built a decentralized platform where thousands of servers can be connected to offer a seamless experience for all users.

### Where can I go for help?

We'd be happy to help! Join our community on [Discord](https://discord.gg/pJ3P2DbUUq), message us in our support chat, or email us at [help@runpod.io](mailto:help@runpod.io).

### What is RunPod's policy on refunds and credits?

If you aren't sure if RunPod is for you, feel free to hang out in our [Discord](https://discord.gg/cUpRmau42V) to ask questions or email [help@runpod.io](mailto:help@runpod.io) You can load as little as $10 into your account to try things out. We don't currently offer refunds or trial credits due to the overhead of processing these requests. Please plan accordingly!

## What are Pods?

---

### What is an On-Demand instance?

**On-Demand instances** are for non-interruptible workloads.
You pay the On-Demand price and cannot be displaced by other users if you have funds to keep your Pod running.

### What is a Spot instance?

A **Spot instance** is an interruptible instance that can generally be rented much cheaper than an On-Demand one.
Spot instances are great for stateless workloads like an API or for workloads you can periodically save to a volume disk.
Your volume disk is retained even if your Spot instance is interrupted.

### What is a Savings Plan?

Savings Plans are a way for you to pay up-front and get a discount for it.
This is great for when you know you will need prolonged access to compute.
You can learn more on the about [Savings Plans here](/pods/savings-plans).

## Billing

All billing, including per-hour compute and storage billing, is charged per minute.

### How does Pod billing work?

Every Pod has an hourly cost based on GPU type. Your RunPod credits are charged for the Pod every minute as long as the Pod is running. If you ever run out of credits, your Pods will be automatically stopped, and you will get an email notification. Eventually, Pods will be terminated if you don't refill your credit. **We pre-emptively stop all of your Pods if you get down to 10 minutes of remaining run time. This gives your account enough balance to keep your data volumes around in the case you need access to your data. Please plan accordingly.**

Once a balance has been completely drained, all pods are subject to deletion at the discretion of the service.
An attempt will be made to hold the pods for as long as possible, but this should not be relied upon!
We highly recommend setting up [automatic payments](https://www.runpod.io/console/user/billing) to ensure balances are automatically topped up as needed.

:::note

You must have at least one hour's worth of time in your balance to rent a Pod at your given spec.
If your balance is insufficient to rent a Pod, then consider renting the Pod on Spot, depositing additional funds, or lowering your GPU spec requirements.

:::

### How does storage billing work?

We currently charge $0.10 GB per month for all storage on running Pods and $0.20 GB per month for volume storage on stopped Pods.
Storage is tied to compute servers, and we want to ensure active users have enough space to run their workloads.
Storage is charged per minute, and we never charge users if the host machine is down or unavailable from the public internet.

### How does Network Volume billing work?

For storage requirements below 1TB, we charge a competitive rate of $0.07/GB/Month. If your storage requirements exceed 1TB, we provide a cost-effective pricing of $0.05/GB/Month. This ensures that you receive significant savings as your data storage scales.

When you choose to create a Network Volume, you gain access to our robust infrastructure, which includes state-of-the-art storage servers located in the same datacenters where you rent GPU servers from us. These servers are connected via a high-speed 25Gbps local network, up to 200 Gbps in some locations, guaranteeing efficient data transfer and minimal latency. Everything is stored on high-speed NVME SSDs to ensure best performance.

Network volumes are billed on a per-hour basis. Please note that if your machine-based storage or network volume is terminated due to lack of funds, that disk space is immediately freed up for use by other clients, and RunPod is unable to assist in recovering lost storage.
RunPod is also not designed to be a cloud storage system; storage is provided in the pursuit of running tasks using its GPUs, and not meant to be a long-term backup solution.
It is highly advisable to continually back up anything you want to save offsite locally or to a cloud provider.

### Can I withdraw my unused balance?

No, RunPod does not offer the option to withdraw your unused balance after depositing funds into your account. When you add funds to your RunPod account, these credits are non-refundable and can only be used for RunPod services.

:::important

When depositing funds into your RunPod account, please be aware that you cannot withdraw your balance once it has been added. Only deposit the amount you intend to use for RunPod services.

:::

We recommend carefully considering the amount you wish to deposit based on your expected usage of our services. If you have any questions about billing or need assistance in planning your RunPod expenses, please don't hesitate to contact our support team at [help@runpod.io](mailto:help@runpod.io).

## Security

---

### Is my data protected from other clients?

Yes. Your data is run in a multi-tenant environment where other clients can't access your pod. For sensitive workloads requiring the best security, please use Secure Cloud.

### Is my data protected from the host of the machine my Pod is running on?

Data privacy is important to us at RunPod.
Our Terms of Service prohibit hosts from trying to inspect your Pod data or usage patterns in any way.
If you want the highest level of security, use Secure Cloud.

## Usability

---

### What can I do in a RunPod Pod?

You can run any Docker container available on any publicly reachable container registry. If you are not well versed in containers, we recommend sticking with the default run templates like our RunPod PyTorch template. However, if you know what you are doing, you can do a lot more!

### Can I run my own Docker daemon on RunPod?

You can't currently spin up your own instance of Docker, as we run Docker for you! Unfortunately, this means that you cannot currently build Docker containers on RunPod or use things like Docker Compose. Many use cases can be solved by creating a custom template with the Docker image that you want to run.

### My Pod is stuck on initializing. What gives?

Usually, this happens for one of several reasons. If you can't figure it out, [contact us](https://www.runpod.io/contact), and we'll gladly help you.

1. You are trying to run a Pod to SSH into, but you did not give the Pod an idle job to run like "sleep infinity."
2. You have given your Pod a command that it doesn't know how to run. Check the logs to make sure that you don't have any syntax errors, etc.

### Can I run Windows?

We don't currently support Windows.
We want to do this in the future, but we do not have a solid timeframe for Windows support.

### How do I find a reliable server in Community Cloud?

RunPod needs to provide you with reliable servers. All of our listed servers must meet minimum reliability, and most are running in a data center! However, if you want the highest level of reliability and security, use Secure Cloud. RunPod calculates server reliability by maintaining a heartbeat with each server in real-time.

### Why do I have zero GPUs assigned to my Pod?

If you want to avoid this, using network volumes is the best choice. [Read about it here.](https://blog.runpod.io/four-reasons-to-set-up-a/)

[Learn how to use them here](https://docs.runpod.io/pods/storage/create-network-volumes).

Most of our machines have between 4 and 8 GPUs per physical machine. When you start a Pod, it is locked to a specific physical machine. If you keep it running (On-Demand), then that GPU cannot be taken from you. However, if you stop your Pod, it becomes available for a different user to rent. When you want to start your Pod again, your specific machine may be wholly occupied! In this case, we give you the option to spin up your Pod with zero GPUs so you can retain access to your data.

Remember that this does not mean there are no more GPUs of that type available, just none on the physical machine that specific Pod is locked to. Note that transfer Pods have limited computing capabilities, so transferring files using a UI may be difficult, and you may need to resort to terminal access or cloud sync options.

#### What are Network Volumes?

Network volumes allow you to share data between Pods and generally be more mobile with your important data. This feature is only available in specific secure cloud data centers, but we are actively rolling it out to more and more of our secure cloud footprint. If you use network volumes, you should rarely run into situations where you cannot use your data with a GPU without a file transfer!

[Read about it here](https://blog.runpod.io/four-reasons-to-set-up-a/).

## What if?

---

### What if I run out of funds?

All your Pods are stopped automatically when you don't have enough funds to keep your Pods running for at least ten more minutes. When your Pods are stopped, your container disk data will be lost, but your volume data will be preserved. Pods are scheduled for removal if adequate credit balance is not maintained. If you fail to do so, your Pods will be terminated, and Pod volumes will be removed.

After you add more funds to your account, you can start your Pod if you wish (assuming enough GPUs are available on the host machine).

### What if the machine that my Pod is running loses power?

If the host machine loses power, it will attempt to start your Pod again when it returns online. Your volume data will be preserved, and your container will run the same command as it ran the first time you started renting it. Your container disk and anything in memory will be lost!

### What if my Pod loses internet connectivity?

The host machine continues to run your Pod to the best of its ability, even if it is not connected to the internet. If your job requires internet connectivity, then it will not function. You will not be charged if the host loses internet connectivity, even if it continues to run your job. You may, of course, request that your Pod exit while the host is offline, and it will exit your Pod when it regains network connectivity.

### What if it says that my spending limit has been exceeded?

We implement a spending limit for newer accounts that will grow over time. This is because we have found that sometimes scammers try to interfere with the natural workings of the platform. We believe that this limit should not impact normal usage. We would be delighted to up your spending limit if you [contact us](https://www.runpod.io/contact) and share your use case.

## Legal

---

### Do you have some legal stuff I can look at?

Sure, do! Take a look at our [legal page](https://www.runpod.io/legal).

## GDPR Compliance

At Runpod, we take data protection and privacy seriously.
We have implemented robust policies, procedures, and technical measures to ensure compliance with the GDPR requirements.

### Is RunPod compliant with GDPR for data processed in Europe?

Yes, RunPod is fully compliant with the General Data Protection Regulation (GDPR) requirements for any data processed within our European data center regions.

### What measures does RunPod take to ensure GDPR compliance?

For servers hosted in GDPR-compliant regions like the European Union, we ensure:

- **Data processing procedures**: We have established clear procedures for the collection,
  storage, processing, and deletion of personal data, ensuring transparency and accountability in
  our data processing activities.
- **Data protection measures**: We have implemented appropriate technical and organizational
  measures to safeguard personal data against unauthorized access, disclosure, alteration, and
  destruction.
- **Consent mechanisms**: We obtain and record consent from individuals for the processing of
  their personal data in accordance with GDPR requirements, and we provide mechanisms for
  individuals to withdraw consent if desired.
- **Rights of data subjects**: We facilitate the rights of data subjects under the GDPR, including
  the right to access, rectify, erase, or restrict the processing of their personal data, and we handle
  data subject requests promptly and efficiently.
- **Data transfer mechanisms**: We ensure lawful and secure transfer of personal data outside the
  EU, where applicable, in compliance with GDPR requirements, utilizing appropriate mechanisms
  such as adequacy decisions, standard contractual clauses, or binding corporate rules.
- **Compliance monitoring**: We regularly monitor and review our GDPR compliance to ensure
  ongoing effectiveness and adherence to regulatory requirements, conducting data protection
  impact assessments and internal audits as needed.

For any inquiries or concerns regarding our GDPR compliance or our data protection practices, reach out to our team through email at [help@runpod.io](mailto:help@runpod.io).

---

# manage-cards.md

File: references/faq/manage-cards.md

RunPod is a US-based organization that serves clients all across the world. However, credit card processors have in general keyed into international transactions as a potential vector for fraud and tend to apply more stringent standards for blocking transactions. If your card is declined, don't panic! To minimize potential interruptions to your service, you'll want to follow these steps.

**Keep your balance topped up**

To avoid any potential issues with your balance being overrun, it's best to refresh your balance at least a few days before you're due to run out so you have a chance to address any last minute delays. Also be aware that there is an option to automatically refresh your balance when you run low under the Billing [page](https://www.runpod.io/console/user/billing):


**Call the bank that issued your card**

Once you do experience a card decline, the first step you'll want to do is to contact your issuing bank to see why a card is declined. Due to consumer/merchant privacy standards in the US, we are not provided with a reason that the card is declined, only that the transaction was not processed. Only your issuing bank can specifically tell you why a payment was declined. Many times, declines are for completely innocent reasons, such as your bank's anti-fraud protection tripping; just the same, RunPod is unable to assist with blocks put in place by your bank.

It's important that you call your bank for the initial decline before trying a different card, because the processor may block _all_ funding attempts from an account if it seems declines from multiple cards for the same account, even if these attempts would have otherwise not had any problems. These account blocks generally clear after 24 hours, but it may be difficult to load the account until then.

**Other potential reasons for card blocks**

Our payment processor may block cards for specific users based on their risk profile, so certain use patterns may trigger a block. If you use several different cards within a short period time, or have had disputed transactions in the past, this may also cause cards to decline.

To see a list of supported cards on Stripe, [click here](https://stripe.com/docs/payments/cards/supported-card-brands>).

**Contact us for support**

If all else fails, then feel free to contact [RunPod support](https://www.runpod.io/contact) if you are still having trouble loading your account. We ask that you check with your bank first, but if everything checks out on your end, we will be glad to help!

---

# gpu-types.md

File: references/gpu-types.md

The following list contains all GPU types available on RunPod.

For more information, see [GPU pricing](https://www.runpod.io/gpu-instance/pricing).

<!--
Table last generated: 2024-12-27
-->

| GPU ID                         | Display Name   | Memory (GB) |
| ------------------------------ | -------------- | ----------- |
| NVIDIA A100 80GB PCIe          | A100 PCIe      | 80          |
| NVIDIA A100-SXM4-80GB          | A100 SXM       | 80          |
| NVIDIA A30                     | A30            | 24          |
| NVIDIA A40                     | A40            | 48          |
| NVIDIA H100 NVL                | H100 NVL       | 94          |
| NVIDIA H100 PCIe               | H100 PCIe      | 80          |
| NVIDIA H100 80GB HBM3          | H100 SXM       | 80          |
| NVIDIA H200                    | H200 SXM       | 143         |
| NVIDIA L4                      | L4             | 24          |
| NVIDIA L40                     | L40            | 48          |
| NVIDIA L40S                    | L40S           | 48          |
| AMD Instinct MI300X OAM        | MI300X         | 192         |
| NVIDIA RTX 2000 Ada Generation | RTX 2000 Ada   | 16          |
| NVIDIA GeForce RTX 3070        | RTX 3070       | 8           |
| NVIDIA GeForce RTX 3080        | RTX 3080       | 10          |
| NVIDIA GeForce RTX 3080 Ti     | RTX 3080 Ti    | 12          |
| NVIDIA GeForce RTX 3090        | RTX 3090       | 24          |
| NVIDIA GeForce RTX 3090 Ti     | RTX 3090 Ti    | 24          |
| NVIDIA RTX 4000 Ada Generation | RTX 4000 Ada   | 20          |
| NVIDIA GeForce RTX 4070 Ti     | RTX 4070 Ti    | 12          |
| NVIDIA GeForce RTX 4080        | RTX 4080       | 16          |
| NVIDIA GeForce RTX 4080 SUPER  | RTX 4080 SUPER | 16          |
| NVIDIA GeForce RTX 4090        | RTX 4090       | 24          |
| NVIDIA RTX 5000 Ada Generation | RTX 5000 Ada   | 32          |
| NVIDIA RTX 6000 Ada Generation | RTX 6000 Ada   | 48          |
| NVIDIA RTX A2000               | RTX A2000      | 6           |
| NVIDIA RTX A4000               | RTX A4000      | 16          |
| NVIDIA RTX A4500               | RTX A4500      | 20          |
| NVIDIA RTX A5000               | RTX A5000      | 24          |
| NVIDIA RTX A6000               | RTX A6000      | 48          |
| Tesla V100-PCIE-16GB           | Tesla V100     | 16          |
| Tesla V100-FHHL-16GB           | V100 FHHL      | 16          |
| Tesla V100-SXM2-16GB           | V100 SXM2      | 16          |
| Tesla V100-SXM2-32GB           | V100 SXM2 32GB | 32          |

---

# leaked-api-keys.md

File: references/troubleshooting/leaked-api-keys.md

Leaked API keys can occur when users accidentally include a plain text API key in a public repository. This document provides guidance to help you remediate a compromised key.

## Disable

To disable an API key:

1. From the console, select **Settings**.
2. Under **API Keys**, select the toggle and select **Yes**.

## Revoke

To delete an API key:

1. From the console, select **Settings**.
2. Under **API Keys**, select the trash can icon and select **Revoke Key**.

---

# storage-full.md

File: references/troubleshooting/storage-full.md

Storage full can occur when users generate many files, transfer files, or perform other storage-intensive tasks. This document provides guidance to help you troubleshoot this.

## Check Disk Usage

When encountering a storage full, the first step is to check your container’s disk usage. You can use the `df -h` command to display a summary of disk usage.

```bash
df -h
```

Example output:

```bash
root@9b8e325167b2:/# df -h
Filesystem                    Size  Used Avail Use% Mounted on
overlay                        20G   16M   20G   1% /
tmpfs                          64M     0   64M   0% /dev
tmpfs                         252G     0  252G   0% /sys/fs/cgroup
shm                            24G     0   24G   0% /dev/shm
/dev/sda2                     457G   12G  423G   3% /usr/bin/nvidia-smi
tmpfs                         252G   12K  252G   1% /proc/driver/nvidia
tmpfs                         252G  4.0K  252G   1% /etc/nvidia/nvidia-application-profiles-rc.d
tmpfs                          51G  4.4M   51G   1% /run/nvidia-persistenced/socket
tmpfs                         252G     0  252G   0% /proc/asound
tmpfs                         252G     0  252G   0% /proc/acpi
tmpfs                         252G     0  252G   0% /proc/scsi
tmpfs                         252G     0  252G   0% /sys/firmware
tmpfs                         252G     0  252G   0% /sys/devices/virtual/powercap
```

## Key Areas to Check

**Container Disk Usage**: The primary storage area for your container is mounted on the `overlay` filesystem. This indicates the container’s root directory.

```bash
Filesystem                    Size  Used Avail Use% Mounted on
overlay                        20G   16M   20G   1% /
```

You can use the command `du -sh .` to check the space usage of the current directory. The default volume of container volume or network volume is mounted at `/workspace`, You can check the usage with the following example::

```bash
root@9b8e325167b2:/# cd workspace/
root@9b8e325167b2:/workspace# du -sh .
194M    .
```

**Identifying Large Files**: To identify the top 10 largest files in your `/workspace`, you can run the following command:

```bash
root@9b8e325167b2:/# find /workspace -type f -exec du -h {} + | sort -rh | head -n 10
96M     /workspace/f.txt
96M     /workspace/e.txt
1.0K    /workspace/c.txt
512     /workspace/b.txt
512     /workspace/a.txt
```

## Removing Files and Directories

Once you’ve identified large files or directories that are no longer needed, you can remove them to free up space.

:::warning
This will permanently delete the file, folder. Use with caution.
:::

```bash
# To delete a specific file, use the rm command:
rm /path/to/file

# To remove an entire directory and its contents, use the rm -r command:
rm -r /path/to/directory
```

---

# troubleshooting-502-errors.md

File: references/troubleshooting/troubleshooting-502-errors.md

502 errors can occur when users attempt to access a program running on a specific port of a deployed pod and the program isn't running or has encountered an error. This document provides guidance to help you troubleshoot this error.

### Check Your Pod's GPU

The first step to troubleshooting a 502 error is to check whether your pod has a GPU attached.

1. **Access your pod's settings**: Click on your pod's settings in the user interface to access detailed information about your pod.

2. **Verify GPU attachment**: Here, you should be able to see if your pod has a GPU attached. If it does not, you will need to attach a GPU.

If a GPU is attached, you will see it under the Pods screen (e.g. 1 x A6000). If a GPU is not attached, this number will be 0. RunPod does allow you to spin up a pod with 0 GPUs so that you can connect to it via a Terminal or CloudSync to access data. However, the options to connect to RunPod via the web interface will be nonfunctional, even if they are lit up.


### Check Your Pod's Logs

After confirming that your pod has a GPU attached, the next step is to check your pod's logs for any errors.

1. **Access your pod's logs**: You can view the logs from the pod's settings in the user interface.

2. ![](/img/docs/3500eba-image.png)\
   **Look for errors**: Browse through the logs to find any error messages that may provide clues about why you're experiencing a 502 error.

### Verify Additional Steps for Official Templates

In some cases, for our official templates, the user interface does not work right away and may require additional steps to be performed by the user.

1. **Access the template's ReadMe**: Navigate to the template's page and open the ReadMe file.

2. **Follow additional steps**: The ReadMe file should provide instructions on any additional steps you need to perform to get the UI functioning properly. Make sure to follow these instructions closely.

Remember, each template may have unique requirements or steps for setup. It is always recommended to thoroughly review the documentation associated with each template.

If you continue to experience 502 errors after following these steps, please contact our support team. We're here to help ensure that your experience on our platform is as seamless as possible.

---

# video-resources.md

File: references/video-resources.md

## RunPod Usage

| Video Link                                                                                               | Topics Covered |
| :------------------------------------------------------------------------------------------------------- | :------------- |
| [How to redeem your RunPod Coupon](https://www.youtube.com/watch?v=IYqEKwpuyWk&ab_channel=OpenCVCourses) | RunPod         |
| [RunPod Introduction and Tour](https://www.youtube.com/watch?v=6O1oM_N6pcw&ab_channel=OpenCVCourses)     | General        |

---

## Tutorials

| Video Link                                                                                                                    | Topics Covered                |
| :---------------------------------------------------------------------------------------------------------------------------- | :---------------------------- |
| [Generate Stable Diffusion Images FAST with RunPod](https://www.youtube.com/watch?v=susnjHSWFq0&t=32s&ab_channel=BillMeeks)   | StableDiffusion, Automatic111 |
| [Generate Text On Images with DeepFloyd IF](https://www.youtube.com/watch?v=Px7Vv9WYl88&t=2s&ab_channel=BillMeeks)            | DeepFloyd                     |
| [Remix Your Pics With Stable Diffusion and ControlNet](https://www.youtube.com/watch?v=BqdIdk9LU4w&t=1s&ab_channel=BillMeeks) | StableDiffusion, ControlNet   |
| [Using Automatic1111 WebUI on RunPod](https://www.youtube.com/watch?v=R6HUQOtsVic&ab_channel=OpenCVCourses)                   | WebUI, Automatic1111          |
| [RUN TextGen AI WebUI LLM On Runpod & Colab!](https://www.youtube.com/watch?v=TP2yID7Ubr4&ab_channel=Aitrepreneur)            | GoogleColab, TextGeneration   |
| [Make Your Renders 10x Faster With Runpod](https://www.youtube.com/watch?v=sJ-Diy93TAg&ab_channel=RahulAhire)                 | Rendering, Blender            |

---

## General/Generic Linux

| Video Link                                                                                                                          | Topics Covered     |
| :---------------------------------------------------------------------------------------------------------------------------------- | :----------------- |
| [How to Use the rsync Command \| Linux Essentials Tutorial](https://www.youtube.com/watch?v=2PnAohLS-Q4&ab_channel=AkamaiDeveloper) | File Transfer      |
| [SSH Key Authentication \| How to Create SSH Key Pairs](https://www.youtube.com/watch?v=33dEcCKGBO4&ab_channel=AkamaiDeveloper)     | SSH Authentication |

---

# install-runpodctl.md

File: runpodctl/install-runpodctl.md


runpodctl is an [open-source command-line interface (CLI)](https://github.com/runpod/runpodctl). You can use runpodctl to work with Pods and RunPod projects.

When you create a Pod, it comes with runpodctl installed and configured with a Pod-scoped API key. You can also run runpodctl locally.

To install runpodctl on your local machine, run the appropriate command for your operating system.

<Tabs>
```bash
```
```bash
```
```bash
```
```bash
```
```bash
```
```bash
```
```bash
```
```bash
```

---

# overview.md

File: runpodctl/overview.md


**RunPod CLI** (`runpodctl`) is a command-line interface tool designed to automate and manage GPU pods on [RunPod](https://runpod.io).

This tool facilitates efficient interaction with RunPod's cloud computing platform, enabling you to execute code, transfer data, and manage computing resources seamlessly.

Every Pod deployed to RunPod comes with this CLI tool installed.

### Purpose

The primary purpose of RunPod CLI is to provide you with a straightforward, command-line-based method to:

- Automate the management of GPU and CPU pods.
- Execute code on these pods.
- Transfer data between local systems and RunPod.
- Leverage serverless computing capabilities.

RunPod also contains a functionality that enables you to develop and deploy endpoints entirely on RunPod's infrastructure.

That means you can get a worker up and running without knowing Docker or needing to structure handler code.
This Dockerless workflow also streamlines the development process: you don't need to rebuild and push container images or edit your endpoint to use the new image each time you change your code.

To get started, see [Managing Projects](/runpodctl/projects/manage-projects).

## Installation

For more information, see [Install RunPod CLI](/runpodctl/install-runpodctl).

<Tabs>

```bash
```
```bash
```
```powershell
```
```bash
```
  ```bash
  ```
  ```bash
  ```
  ```bash
  ```
  ```bash
  ```
  ```bash
  ```
```bash
```
```bash
```
```bash
```
```bash
```

---

# get-started.md

File: runpodctl/projects/get-started.md

In this tutorial, we'll explore how to get the IP address of the machine your code is running on and deploy your code to the RunPod platform.
You will get the IP address of your local machine, the development server, and the Serverless Endpoint's server.

By the end, you'll have a solid understanding of how to set up a project environment, interact with your code, and deploy your code to a Serverless Endpoint on the RunPod platform.

While this project is scoped to getting the IP address of the machine your code is running on, you can use the RunPod platform to deploy any code you want.
For larger projects, bundling large packages a Docker image and making code changes requires multiple steps.
With a RunPod development server, you can make changes to your code and test them in a live environment without having to rebuild a Docker image or redeploy your code to the RunPod platform.

This tutorial takes advantage of making updates to your code and testing them in a live environment.

Let's get started by setting up the project environment.

## Prerequisites

Before we begin, you'll need the following:

- RunPod CLI
- Python 3.8 or later

## Set up the project environment

In this first step, you'll set up your project environment using the RunPod CLI.

Set your API key in the RunPod CLI configuration file.

```bash
runpodctl config --apiKey $(RUNPOD_API_KEY)
```

Next, use the RunPod CLI `project create` command to create a new directory and files for your project.

```bash
runpodctl project create
```

Select the **Hello World** project and follow the prompts on the screen.

## Write the code

Next, you'll write the code to get the IP address of the machine your code is running on.

Use `httpbin` to retrieve the IP address and test the code locally.

Change directories to the project directory and open the `src/handler.py` file in your text editor.

```bash
cd my_ip
```

The current code is boiler plate text.
Replace the code with the following:

```python
import runpod
import requests


def get_my_ip(job):
    response = requests.get("https://httpbin.org/ip")
    return response.json()["origin"]


runpod.serverless.start({"handler": get_my_ip})
```

This uses `httpbin` to get the IP address of the machine your code is running on.

Run this code locally to get the IP address of your machine, for example:

```bash
python3 src/handler.py --test_input '{"input": {"prompt": ""}}'
```

```text
--- Starting Serverless Worker |  Version 1.6.1 ---
INFO   | test_input set, using test_input as job input.
DEBUG  | Retrieved local job: {'input': {'prompt': ''}, 'id': 'local_test'}
INFO   | local_test | Started.
DEBUG  | local_test | Handler output: 174.21.174.xx
DEBUG  | local_test | run_job return: {'output': '174.21.174.xx'}
INFO   | Job local_test completed successfully.
INFO   | Job result: {'output': '174.21.174.xx'}
INFO   | Local testing complete, exiting.`
```

This testing environment works for smaller projects, but for larger projects, you will want to use the RunPod CLI to deploy your code to run on the RunPod platform.

In the next step, you'll see how to deploy your code to the RunPod platform.

## Run a development server

Now let's run the code you've written using RunPod's development server.
You'll start a development server using the RunPod CLI `project dev` command.

RunPod provides a development server that allows you to quickly make changes to your code and test these changes in a live environment.
You don't need to rebuild a Docker image or redeploy your code to the RunPod platform just because you made a small change or added a new dependency.

To run a development server, use the RunPod CLI `project dev` command and select a Network volume.

```bash
runpodctl project dev
```

This starts a development server on a Pod.
The logs shows the status of your Pod as well as the port number your Pod is running on.

The development server watches for changes in your code and automatically updates the Pod with changes to your code and files like `requirements.txt`.

When the Pod is running you should see the following logs:

```text
Connect to the API server at:
[lug43rcd07ug47] >  https://lug43rcd07ug47-8080.proxy.runpod.net
[lug43rcd07ug47]
[lug43rcd07ug47] Synced venv to network volume
[lug43rcd07ug47] --- Starting Serverless Worker |  Version 1.6.2 ---
```

The `[lug43rcd07ug47]` is your Worker Id.
The `https://lug43rcd07ug47-8080.proxy.runpod.net` is the URL to access your Pod with the 8080 port exposed.
You can interact with this URL like you would any other Endpoint.

## Interact with your code

In this step, you'll interact with your code by running a `curl` command to fetch the IP address from the development server.
You'll learn how to include dependencies in your project and how to use the RunPod API to run your code.

You might have noticed that the function to get an IP address uses a third-party dependency `requests`.
This means by default it's not included in Python or the RunPod environment.

To include this dependency, you need to add it to the `requirements.txt` file in the root of your project.

```text
runpod
requests
```

When you save your file, notice that the development server automatically updates the Pod with the dependencies.

During this sync, your Pod is unable to receive requests.
Wait until you see the following logs:

```text
Restarted API server with PID: 701
--- Starting Serverless Worker |  Version 1.6.2 ---
INFO   | Starting API server.
```

Now you can interact with your code.

While the Pod is still running, create a new terminal session and run the following command:

```bash
curl -X 'POST' \
  'https://${YOUR_ENDPOINT}-8080.proxy.runpod.net/runsync' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "input": {}
}'
```

This command uses the `runsync` method on the RunPod API to run your code synchronously.

The previous command returns a response:

```text
{
  "id": "test-9613c9be-3fed-401f-8cda-6b5f354417f8",
  "status": "COMPLETED",
  "output": "69.30.85.70"
}
```

The output is the IP address of the Pod your code is running on and not your local machine.
Even though you're executing code locally, you can see that it's running on a Pod.

Now, what if you wanted this function to run as a Serverless Endpoint?
Meaning, you didn't want to keep the Pod running all the time.
You only wanted it to turn on when you sent a request to it.

In the next step, you'll learn to deploy your code to the Serverless platform and get the IP address of that machine.

## Deploy your code

Now that you've tested your code in the development environment, you'll deploy it to the RunPod platform using the RunPod CLI `project deploy` command.
This will make your code available as a [Serverless Endpoint](/serverless/endpoints/overview).

Stop the development server by pressing `Ctrl + C` in the terminal.

To deploy your code to the RunPod platform, use the RunPod CLI `project deploy` command.

```bash
runpodctl project deploy
```

Select your network volume and wait for your Endpoint to deploy.

After deployment, you will see the following logs:

```text
The following URLs are available:
    - https://api.runpod.ai/v2/${YOUR_ENDPOINT}/runsync
    - https://api.runpod.ai/v2/${YOUR_ENDPOINT}/run
    - https://api.runpod.ai/v2/${YOUR_ENDPOINT}/health
```

:::note

You can follow the logs to see the status of your deployment.
You may notice that the logs show the Pod being created and then the Endpoint being created.

:::

## Interact with your Endpoint

Finally, you'll interact with your Endpoint by running a `curl` command to fetch the IP address from the deployed Serverless function.
You'll see how your code runs as expected and tested in the development environment.

When the deployment completes, you can interact with your Endpoint as you would any other Endpoint.

Replace the previous Endpoint URL and specify the new one and add your API key.

Then, run the following command:

```bash
curl -X 'POST' \
  'https://api.runpod.ai/v2/${YOUR_ENDPOINT}/runsync' \
  -H 'accept: application/json' \
  -H  'authorization: ${YOUR_API_KEY}' \
  -H 'Content-Type: application/json' \
  -d '{
  "input": {}
}'
```

The previous command returns a response:

```text
{
  "delayTime": 249,
  "executionTime": 88,
  "id": "sync-b2188a79-3f9f-4b99-b4d1-18273db3f428-u1",
  "output": "69.30.85.69",
  "status": "COMPLETED"
}
```

The output is the IP address of the Pod your code is running on.

## Conclusion

In this tutorial, you've learned how to get the IP address of the machine your code is running on and deploy your code to the RunPod platform.
You've also learned how to set up a project environment, run a development server, and interact with your code using the RunPod API.
With this knowledge, you can now use this code as a Serverless Endpoint or continue developing your project, testing, and deploying it to the RunPod platform.

---

# manage-projects.md

File: runpodctl/projects/manage-projects.md

Projects enable you to develop and deploy endpoints entirely on RunPod's infrastructure.

## Create a project

A RunPod project is a folder with everything you need to run a development session on a Pod.

1. To create a new project, run the following command.

```command
runpod project create
```

2. Select a starter project. Starter projects include preliminary settings for different kinds of project environments, such as LLM or image diffusion development.
3. Check the [base image](https://github.com/runpod/containers/tree/main/official-templates/base) for included dependencies.
4. (Optional) If you need dependencies that are not included or added by your starter project, add them to the generated `requirements.txt` file.
5. Save your changes.

You've customized your project, and now you're ready to run a development session.

## Run a development session

A development session is the active connection between your local environment and the project environment on your Pod. During a development session, local changes to your project propagate to the project environment in real time.

1. To start a development session, run the following command.

```command
runpodctl project dev
```

2. When you're done developing, press `ctrl` + `c` to end the session. Your Pod will terminate automatically when the session ends.

:::tip

You can resume developing at any time by running `runpodctl project dev` again.

:::

Now that you've developed your project, you can deploy an endpoint directly to RunPod or build a Dockerfile to create a portable image.

## Deploy a project

When you deploy a project, RunPod creates a serverless endpoint with access to saved project data on your network volume.

To deploy a project, run the following command.

```command
runpodctl project deploy
```

Your project is now deployed to a Serverless Endpoint.
You can interact with this Endpoint like you would any other Serverless Endpoint.
For more information, see [Endpoints](/serverless/endpoints/overview).

## Build a project

You have the option to build your project instead of deploying it as an endpoint. When you build a project, RunPod emits a Dockerfile.

To build a project, run the following command.

```command
runpodctl project build
```

You can use the generated Dockerfile to build an image, then deploy the image to any API server.

---

# overview.md

File: runpodctl/projects/overview.md

A RunPod project is a structured environment that allows you to develop, test, and deploy applications and endpoints on RunPod's cloud infrastructure.
It encapsulates all the necessary files, dependencies, and configurations needed to run a development session on a Pod, making it easier to manage and maintain your codebase.

## Why Use RunPod Projects?

- Streamlined Development

RunPod projects streamline the development process by providing a consistent environment for your applications. This eliminates the need to manage complex setups and configurations on local machines, ensuring that your development environment mirrors the production environment.

- Scalability and Flexibility

By leveraging RunPod's infrastructure, projects can scale effortlessly. You can run development sessions, test your code, and deploy endpoints without worrying about the underlying hardware. This scalability is particularly beneficial for resource-intensive applications such as machine learning models or data processing pipelines.

- Simplified Deployment

Deploying a project on RunPod is straightforward. With a single command, you can turn your project into a serverless endpoint, making it accessible over the internet. This reduces the time and effort required to go from development to production.

- Local vs. Cloud Environment

In traditional development workflows, you develop and test your code on your local machine before deploying it to a server or cloud environment. This can lead to discrepancies between your local environment and the production environment, causing unexpected issues.

With RunPod projects, your development and production environments are the same. You develop directly on Pods, ensuring consistency and eliminating the "it works on my machine" problem.

- Real-Time Updates

RunPod projects allow for real-time updates during development sessions. Changes made to your code locally are instantly propagated to the project environment on your Pod. This rapid feedback loop enhances productivity and allows for quicker iteration.

- Integrated Dependencies Management

RunPod projects come with integrated dependency management. You can specify your project dependencies in a `requirements.txt` file, and RunPod ensures these dependencies are installed and up-to-date in your project environment. This simplifies dependency management and reduces potential conflicts.

- Serverless Deployment

Deploying a project on RunPod transforms it into a serverless endpoint. Unlike traditional deployments that require managing servers, load balancers, and other infrastructure components, RunPod abstracts away these complexities. You focus on your code, and RunPod handles the rest.

## How to Get Started

### Creating a Project

To create a new RunPod project, use the following command:

```bash
runpodctl project create
```

You will be prompted to select a starter project, which includes pre-configured settings for different types of environments. Customize your project as needed by adding dependencies to the `requirements.txt` file.

### Running a Development Session

Start a development session with:

```bash
runpodctl project dev
```

This establishes an active connection between your local environment and the project environment on your Pod, allowing for real-time updates.

### Deploying a Project

To deploy your project as a serverless endpoint, run:

```bash
runpodctl project deploy
```

Your project is now accessible over the internet, and you can interact with it like any other serverless endpoint.

### Building a Project

If you prefer to build your project into a Docker image, use:

```bash
runpodctl project build
```

This generates a Dockerfile that you can use to build an image and deploy it to any API server.

---

# runpodctl.md

File: runpodctl/reference/runpodctl.md

You can use RunPod's CLI [runpodctl](https://github.com/runpod/runpodctl) to manage Pods.

All Pods come with `runpodctl` installed with a Pod-scoped API key, which makes managing your Pods easier through the command line.

## Quick Install

Choose one of the following methods to install the RunPod CLI.

### MacOs

**Brew**

```bash
brew install runpod/runpodctl/runpodctl
```

**ARM**

```bash
wget --quiet --show-progress https://github.com/runpod/runpodctl/releases/download/v1.14.3/runpodctl-darwin-arm64 -O runpodctl && chmod +x runpodctl && sudo mv runpodctl /usr/local/bin/runpodctl
```

**AMD**

```bash
wget --quiet --show-progress https://github.com/runpod/runpodctl/releases/download/v1.14.3/runpodctl-darwin-amd64 -O runpodctl && chmod +x runpodctl && sudo mv runpodctl /usr/local/bin/runpodctl
```

### Linux

```bash
wget --quiet --show-progress https://github.com/Run-Pod/runpodctl/releases/download/v1.14.3/runpodctl-linux-amd64 -O runpodctl && chmod +x runpodctl && sudo cp runpodctl /usr/bin/runpodctl
```

### Windows (powershell)

```bash
wget https://github.com/runpod/runpodctl/releases/download/v1.14.3/runpodctl-win-amd -O runpodctl.exe
```

### Google Collab

```bash
!wget --quiet --show-progress https://github.com/Run-Pod/runpodctl/releases/download/v1.14.3/runpodctl-linux-amd -O runpodctl
!chmod +x runpodctl
!cp runpodctl /usr/bin/runpodctl
```

### Jupyter notebook

```bash
!wget --quiet --show-progress https://github.com/Run-Pod/runpodctl/releases/download/v1.14.3/runpodctl-linux-amd -O runpodctl
!chmod +x runpodctl
!cp runpodctl /usr/bin/runpodctl
```

## Usage

```
The RunPod CLI tool to manage resources on runpod.io and develop serverless applications.

Usage:
  runpodctl [command]

Available Commands:
  completion  Generate the autocompletion script for the specified shell
  config      Manage CLI configuration
  create      create a resource
  exec        Execute commands in a pod
  get         get resource
  help        Help about any command
  project     Manage RunPod projects
  receive     receive file(s), or folder
  remove      remove a resource
  send        send file(s), or folder
  ssh         SSH keys and commands
  start       start a resource
  stop        stop a resource
  update      update runpodctl

Flags:
  -h, --help      help for runpodctl
  -v, --version   Print the version of runpodctl

Use "runpodctl [command] --help" for more information about a command.
```

---

# runpodctl_config.md

File: runpodctl/reference/runpodctl_config.md

## runpodctl config

CLI Config

### Synopsis

RunPod CLI Config Settings

```
runpodctl config [flags]
```

### Options

```
    --apiKey string   RunPod API key
    --apiUrl string   RunPod API URL (default "https://api.runpod.io/graphql")
-h, --help            help for config
```

### SEE ALSO

- [runpodctl](runpodctl.md) - CLI for runpod.io

---

# runpodctl_create.md

File: runpodctl/reference/runpodctl_create.md

## runpodctl create

create a resource

### Synopsis

create a resource in runpod.io

### Options

```
-h, --help   help for create
```

### SEE ALSO

- [runpodctl](runpodctl.md) - CLI for runpod.io
- [runpodctl create pod](runpodctl_create_pod.md) - start a pod
- [runpodctl create pods](runpodctl_create_pods.md) - create a group of pods

---

# runpodctl_create_pod.md

File: runpodctl/reference/runpodctl_create_pod.md

## runpodctl create pod

start a pod

### Synopsis

start a pod from runpod.io

```
runpodctl create pod [flags]
```

### Options

```
    --args string             container arguments
    --communityCloud          create in community cloud
    --containerDiskSize int   container disk size in GB (default 20)
    --cost float32            $/hr price ceiling, if not defined, pod will be created with lowest price available
    --env strings             container arguments
    --gpuCount int            number of GPUs for the pod (default 1)
    --gpuType string          gpu type id, e.g. 'NVIDIA GeForce RTX 3090'
-h, --help                    help for pod
    --imageName string        container image name
    --mem int                 minimum system memory needed (default 20)
    --name string             any pod name for easy reference
    --ports strings           ports to expose; max only 1 http and 1 tcp allowed; e.g. '8888/http'
    --secureCloud             create in secure cloud
    --templateId string       templateId to use with the pod
    --vcpu int                minimum vCPUs needed (default 1)
    --volumePath string       container volume path (default "/runpod")
    --volumeSize int          persistent volume disk size in GB (default 1)
    --networkVolumeId string  network volume id
```

### SEE ALSO

- [runpodctl create](runpodctl_create.md) - create a resource

---

# runpodctl_create_pods.md

File: runpodctl/reference/runpodctl_create_pods.md

## runpodctl create pods

create a group of pods

### Synopsis

create a group of pods on runpod.io

```
runpodctl create pods [flags]
```

### Options

```
    --args string             container arguments
    --communityCloud          create in community cloud
    --containerDiskSize int   container disk size in GB (default 20)
    --cost float32            $/hr price ceiling, if not defined, pod will be created with lowest price available
    --env strings             container arguments
    --gpuCount int            number of GPUs for the pod (default 1)
    --gpuType string          gpu type id, e.g. 'NVIDIA GeForce RTX 3090'
-h, --help                    help for pods
    --imageName string        container image name
    --mem int                 minimum system memory needed (default 20)
    --name string             any pod name for easy reference
    --podCount int            number of pods to create with the same name (default 1)
    --ports strings           ports to expose; max only 1 http and 1 tcp allowed; e.g. '8888/http'
    --secureCloud             create in secure cloud
    --vcpu int                minimum vCPUs needed (default 1)
    --volumePath string       container volume path (default "/runpod")
    --volumeSize int          persistent volume disk size in GB (default 1)
```

### SEE ALSO

- [runpodctl create](runpodctl_create.md) - create a resource

---

# runpodctl_get.md

File: runpodctl/reference/runpodctl_get.md

## runpodctl get

get resource

### Synopsis

get resources for pods

### Options

```
-h, --help   help for get
```

### SEE ALSO

- [runpodctl](runpodctl.md) - CLI for runpod.io
- [runpodctl get cloud](runpodctl_get_cloud.md) - get all cloud gpus
- [runpodctl get pod](runpodctl_get_pod.md) - get all pods

---

# runpodctl_get_cloud.md

File: runpodctl/reference/runpodctl_get_cloud.md

## runpodctl get cloud

get all cloud gpus

### Synopsis

get all cloud gpus available on runpod.io

```
runpodctl get cloud [gpuCount] [flags]
```

### Options

```
-c, --community   show listings from community cloud only
    --disk int    minimum disk size in GB you need
-h, --help        help for cloud
    --mem int     minimum sys memory size in GB you need
-s, --secure      show listings from secure cloud only
    --vcpu int    minimum vCPUs you need
```

### SEE ALSO

- [runpodctl get](runpodctl_get.md) - get resource

---

# runpodctl_get_pod.md

File: runpodctl/reference/runpodctl_get_pod.md

## runpodctl get pod

get all pods

### Synopsis

get all pods or specify pod id

```
runpodctl get pod [podId] [flags]
```

### Options

```
-a, --allfields   include all fields in output
-h, --help        help for pod
```

### SEE ALSO

- [runpodctl get](runpodctl_get.md) - get resource

---

# runpodctl_project.md

File: runpodctl/reference/runpodctl_project.md

## runpodctl project

Manage RunPod projects

### Synopsis

Develop and deploy projects entirely on RunPod's infrastructure

### Options

```
-h, --help   help for project
```

### SEE ALSO

- [runpodctl](runpodctl.md) - CLI for runpod.io
- [runpodctl project build](runpodctl_project_build.md) - builds Dockerfile for current project
- [runpodctl project create](runpodctl_project_create.md) - creates a new project
- [runpodctl project deploy](runpodctl_project_deploy.md) - deploys your project as an endpoint
- [runpodctl project dev](runpodctl_project_dev.md) - starts a development session for the current project

---

# runpodctl_project_build.md

File: runpodctl/reference/runpodctl_project_build.md

## runpodctl project build

builds Dockerfile for current project

### Synopsis

builds a local Dockerfile for the project in the current folder. You can use this Dockerfile to build an image and deploy it to any API server.

```
runpodctl project build [flags]
```

### Options

```
-h, --help          help for build
    --include-env   include environment variables from runpod.toml in generated Dockerfile
```

### SEE ALSO

- [runpodctl project](runpodctl_project.md) - Manage RunPod projects

---

# runpodctl_project_create.md

File: runpodctl/reference/runpodctl_project_create.md

## runpodctl project create

creates a new project

### Synopsis

creates a new RunPod project folder on your local machine

```
runpodctl project create [flags]
```

### Options

```
-h, --help          help for create
-i, --init          use the current directory as the project directory
-n, --name string   project name
```

### SEE ALSO

- [runpodctl project](runpodctl_project.md) - Manage RunPod projects

---

# runpodctl_project_deploy.md

File: runpodctl/reference/runpodctl_project_deploy.md

## runpodctl project deploy

deploys your project as an endpoint

### Synopsis

deploys a serverless endpoint for the RunPod project in the current folder

```
runpodctl project deploy [flags]
```

### Options

```
-h, --help   help for deploy
```

### SEE ALSO

- [runpodctl project](runpodctl_project.md) - Manage RunPod projects

---

# runpodctl_project_dev.md

File: runpodctl/reference/runpodctl_project_dev.md

## runpodctl project dev

starts a development session for the current project

### Synopsis

connects your local environment and the project environment on your Pod. Changes propagate to the project environment in real time.

```
runpodctl project dev [flags]
```

### Options

```
-h, --help              help for dev
    --prefix-pod-logs   prefix logs from project Pod with Pod ID (default true)
    --select-volume     select a new default network volume for current project
```

### SEE ALSO

- [runpodctl project](runpodctl_project.md) - Manage RunPod projects

---

# runpodctl_receive.md

File: runpodctl/reference/runpodctl_receive.md

## runpodctl receive

receive file(s), or folder

### Synopsis

receive file(s), or folder from pod or any computer

```
runpodctl receive [code] [flags]
```

### Options

```
-h, --help   help for receive
```

### SEE ALSO

- [runpodctl](runpodctl.md) - CLI for runpod.io

---

# runpodctl_remove.md

File: runpodctl/reference/runpodctl_remove.md

## runpodctl remove

remove a resource

### Synopsis

remove a resource in runpod.io

### Options

```
-h, --help   help for remove
```

### SEE ALSO

- [runpodctl](runpodctl.md) - CLI for runpod.io
- [runpodctl remove pod](runpodctl_remove_pod.md) - remove a pod
- [runpodctl remove pods](runpodctl_remove_pods.md) - remove all pods using name

---

# runpodctl_remove_pod.md

File: runpodctl/reference/runpodctl_remove_pod.md

## runpodctl remove pod

remove a pod

### Synopsis

remove a pod from runpod.io

```
runpodctl remove pod [podId] [flags]
```

### Options

```
-h, --help   help for pod
```

### SEE ALSO

- [runpodctl remove](runpodctl_remove.md) - remove a resource

---

# runpodctl_remove_pods.md

File: runpodctl/reference/runpodctl_remove_pods.md

## runpodctl remove pods

remove all pods using name

### Synopsis

remove all pods using name from runpod.io

```
runpodctl remove pods [name] [flags]
```

### Options

```
-h, --help           help for pods
    --podCount int   number of pods to remove with the same name (default 1)
```

### SEE ALSO

- [runpodctl remove](runpodctl_remove.md) - remove a resource

---

# runpodctl_send.md

File: runpodctl/reference/runpodctl_send.md

## runpodctl send

send file(s), or folder

### Synopsis

send file(s), or folder to pod or any computer

```
runpodctl send [filename(s) or folder] [flags]
```

### Options

```
    --code string   codephrase used to connect
-h, --help          help for send
```

### SEE ALSO

- [runpodctl](runpodctl.md) - CLI for runpod.io

---

# runpodctl_ssh.md

File: runpodctl/reference/runpodctl_ssh.md

## runpodctl ssh

SSH keys and commands

### Synopsis

SSH key management and connection to pods

### Options

```
-h, --help   help for ssh
```

### SEE ALSO

- [runpodctl](runpodctl.md) - CLI for runpod.io
- [runpodctl ssh add-key](runpodctl_ssh_add-key.md) - Adds an SSH key to the current user account
- [runpodctl ssh list-keys](runpodctl_ssh_list-keys.md) - List all SSH keys

---

# runpodctl_ssh_add-key.md

File: runpodctl/reference/runpodctl_ssh_add-key.md

## runpodctl ssh add-key

Adds an SSH key to the current user account

### Synopsis

Adds an SSH key to the current user account. If no key is provided, one will be generated.

```
runpodctl ssh add-key [flags]
```

### Options

```
-h, --help              help for add-key
    --key string        The public key to add.
    --key-file string   The file containing the public key to add.
```

### SEE ALSO

- [runpodctl ssh](runpodctl_ssh.md) - SSH keys and commands

---

# runpodctl_ssh_list-keys.md

File: runpodctl/reference/runpodctl_ssh_list-keys.md

## runpodctl ssh list-keys

List all SSH keys

### Synopsis

List all the SSH keys associated with the current user's account.

```
runpodctl ssh list-keys [flags]
```

### Options

```
-h, --help   help for list-keys
```

### SEE ALSO

- [runpodctl ssh](runpodctl_ssh.md) - SSH keys and commands

---

# runpodctl_start.md

File: runpodctl/reference/runpodctl_start.md

## runpodctl start

start a resource

### Synopsis

start a resource in runpod.io

### Options

```
-h, --help   help for start
```

### SEE ALSO

- [runpodctl](runpodctl.md) - CLI for runpod.io
- [runpodctl start pod](runpodctl_start_pod.md) - start a pod

---

# runpodctl_start_pod.md

File: runpodctl/reference/runpodctl_start_pod.md

## runpodctl start pod

start a pod

### Synopsis

start a pod from runpod.io

```
runpodctl start pod [podId] [flags]
```

### Options

```
    --bid float32   bid per gpu for spot price
-h, --help          help for pod
```

### SEE ALSO

- [runpodctl start](runpodctl_start.md) - start a resource

---

# runpodctl_stop.md

File: runpodctl/reference/runpodctl_stop.md

## runpodctl stop

stop a resource

### Synopsis

stop a resource in runpod.io

### Options

```
-h, --help   help for stop
```

### SEE ALSO

- [runpodctl](runpodctl.md) - CLI for runpod.io
- [runpodctl stop pod](runpodctl_stop_pod.md) - stop a pod

---

# runpodctl_stop_pod.md

File: runpodctl/reference/runpodctl_stop_pod.md

## runpodctl stop pod

stop a pod

### Synopsis

stop a pod from runpod.io

```
runpodctl stop pod [podId] [flags]
```

### Options

```
-h, --help   help for pod
```

### SEE ALSO

- [runpodctl stop](runpodctl_stop.md) - stop a resource

---

# runpodctl_update.md

File: runpodctl/reference/runpodctl_update.md

## runpodctl update

update runpodctl

### Synopsis

update runpodctl to the latest version

```
runpodctl update [flags]
```

### Options

```
-h, --help   help for update
```

### SEE ALSO

- [runpodctl](runpodctl.md) - CLI for runpod.io

---

# runpodctl_version.md

File: runpodctl/reference/runpodctl_version.md

## runpodctl version

runpodctl version

### Synopsis

runpodctl version

```
runpodctl version [flags]
```

### Options

```
-h, --help   help for version
```

### SEE ALSO

- [runpodctl](runpodctl.md) - CLI for runpod.io

---

# endpoints.md

File: sdks/go/endpoints.md


Interacting with RunPod's Endpoints is a core feature of the SDK, enabling the execution of tasks and the retrieval of results.
This section covers the synchronous and asynchronous execution methods, along with checking the status of operations.

## Prerequisites

Before using the RunPod Go SDK, ensure that you have:

- [Installed the RunPod Go SDK](/sdks/go/overview#install).
- Configured your API key.

## Set your Endpoint Id

Set your RunPod API key and your Endpoint Id as environment variables.

```go
package main

import (
	"log"
	"os"

	"github.com/runpod/go-sdk/pkg/sdk"
	"github.com.runpod/go-sdk/pkg/sdk/config"
	rpEndpoint "github.com/runpod/go-sdk/pkg/sdk/endpoint"
)

func main() {
	// Retrieve the API key and base URL from environment variables
	apiKey := os.Getenv("RUNPOD_API_KEY")
	baseURL := os.Getenv("RUNPOD_BASE_URL")

	// Check if environment variables are set
	if apiKey == "" {
		log.Fatalf("Environment variable RUNPOD_API_KEY is not set")
	}
	if baseURL == "" {
		log.Fatalf("Environment variable RUNPOD_BASE_URL is not set")
	}


    // Use the endpoint object
    // ...
}
```

This allows all calls to pass through your Endpoint Id with a valid API key.

The following are actions you use on the

- [RunSync](#run-sync)
- [Run](#run-async)
- [Stream](#stream)
- [Health](#health-check)
- [Status](#status)
- [Cancel](#cancel)
- [Purge Queue](#purge-queue)

Here is the revised documentation based on the Go Sample:

## Run the Endpoint {#run}

Run the Endpoint using either the asynchronous `run` or synchronous `runSync` method.

Choosing between asynchronous and synchronous execution hinges on your task's needs and application design.

### Run synchronously {#run-sync}

To execute an endpoint synchronously and wait for the result, use the `runSync` method on your endpoint.
This method blocks the execution until the endpoint run is complete or until it times out.

<Tabs>
```go
```
```json
```
```go
```
```json
```
```go
```
```json
```
```go
```
```json
```
```python
```
```go
```
```json
```
```go
```
```json
```
```go
```
```json
```
```go
```
```json
```
```go
```
```json
```
```go
```
```json
```

---

# overview.md

File: sdks/go/overview.md

Get started with setting up your RunPod projects using Go.
Whether you're building web applications, server-side implementations, or automating tasks, the RunPod Go SDK provides the tools you need.
This guide outlines the steps to get your development environment ready and integrate RunPod into your Go projects.

## Prerequisites

Before you begin, ensure that you have the following:

- Go installed on your machine (version 1.16 or later)
- A RunPod account with an API key and Endpoint Id

## Install the RunPod SDK {#install}

Before integrating RunPod into your project, you'll need to install the SDK.

To install the RunPod SDK, run the following `go get` command in your project directory.

```command
go get github.com/runpod/go-sdk
```

This command installs the `runpod-sdk` package.
Then run the following command to install the dependencies:

```command
go mod tidy
```

For more details about the package, visit the [Go package page](https://pkg.go.dev/github.com/runpod/go-sdk/pkg/sdk) or the [GitHub repository](https://github.com/runpod/go-sdk).

## Add your API key

To use the RunPod SDK in your project, you first need to import it and configure it with your API key and endpoint ID. Ensure these values are securely stored, preferably as environment variables.

Below is a basic example of how to initialize and use the RunPod SDK in your Go project.

```go
func main() {
    endpoint, err := rpEndpoint.New(
        &config.Config{ApiKey: sdk.String(os.Getenv("RUNPOD_API_KEY"))},
        &rpEndpoint.Option{EndpointId: sdk.String(os.Getenv("RUNPOD_BASE_URL"))},
    )
    if err != nil {
        panic(err)
    }

    // Use the endpoint object
    // ...
}
```

This snippet demonstrates how to import the SDK, initialize it with your API key, and reference a specific endpoint using its ID.

### Secure your API key

When working with the RunPod SDK, it's essential to secure your API key.
Storing the API key in environment variables is recommended, as shown in the initialization example. This method keeps your key out of your source code and reduces the risk of accidental exposure.

:::note

Use environment variables or secure secrets management solutions to handle sensitive information like API keys.

:::

For more information, see the following:

- [RunPod SDK Go Package](https://pkg.go.dev/github.com/runpod/go-sdk/pkg/sdk)
- [RunPod GitHub Repository](https://github.com/runpod/go-sdk)
- [Endpoints](/sdks/go/endpoints)

---

# configurations.md

File: sdks/graphql/configurations.md

For details on queries, mutations, fields, and inputs, see the [RunPod GraphQL Spec](https://graphql-spec.runpod.io/).

When configuring your environment, certain arguments are essential to ensure the correct setup and operation. Below is a detailed overview of each required argument:

### `containerDiskInGb`

- **Description**: Specifies the size of the disk allocated for the container in gigabytes. This space is used for the operating system, installed applications, and any data generated or used by the container.
- **Type**: Integer
- **Example**: `10` for a 10 GB disk size.

### `dockerArgs`

- **Description**: If specified, overrides the [container start command](https://docs.docker.com/engine/reference/builder/#cmd). If this argument is not provided, it will rely on the start command provided in the docker image.
- **Type**: String
- **Example**: `sleep infinity` to run the container in the background.

<!--
Contains additional arguments that are passed directly to Docker when starting the container. This can include mount points, network settings, or any other Docker command-line arguments.
-->

### `env`

- **Description**: A set of environment variables to be set within the container. These can configure application settings, external service credentials, or any other configuration data required by the software running in the container.
- **Type**: Dictionary or Object
- **Example**: `{"DATABASE_URL": "postgres://user:password@localhost/dbname"}`.

### `imageName`

- **Description**: The name of the Docker image to use for the container. This should include the repository name and tag, if applicable.
- **Type**: String
- **Example**: `"nginx:latest"` for the latest version of the Nginx image.

### `name`

- **Description**: The name assigned to the container instance. This name is used for identification and must be unique within the context it's being used.
- **Type**: String
- **Example**: `"my-app-container"`.

### `volumeInGb`

- **Description**: Defines the size of an additional persistent volume in gigabytes. This volume is used for storing data that needs to persist between container restarts or redeployments.
- **Type**: Integer
- **Example**: `5` for a 5GB persistent volume.

Ensure that these arguments are correctly specified in your configuration to avoid errors during deployment.

Optional arguments may also be available, providing additional customization and flexibility for your setup.

---

# manage-endpoints.md

File: sdks/graphql/manage-endpoints.md


`gpuIds`, `name`, and `templateId` are required arguments; all other arguments are optional, and default values will be used if unspecified.

## Create a new Serverless Endpoint

<Tabs>
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```

---

# manage-pod-templates.md

File: sdks/graphql/manage-pod-templates.md


Required arguments:

- `containerDiskInGb`
- `dockerArgs`
- `env`
- `imageName`
- `name`
- `volumeInGb`

All other arguments are optional.

If your container image is private, you can also specify Docker login credentials with a `containerRegistryAuthId` argument, which takes the ID (_not_ the name) of the container registry credentials you saved in your RunPod user settings as a string.

:::note

Template names must be unique as well; if you try to create a new template with the same name as an existing one, your call will fail.
:::

## Create a Pod Template

### Create GPU Template

<Tabs>
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```

---

# manage-pods.md

File: sdks/graphql/manage-pods.md


## Authentication

RunPod uses API Keys for all API requests.
Go to [Settings](https://www.runpod.io/console/user/settings) to manage your API keys.

## GraphQL API Spec

If you need detailed queries, mutations, fields, and inputs, look at the [GraphQL Spec](https://graphql-spec.runpod.io/).

## Create Pods

A Pod consists of the following resources:

- 0 or more GPUs - A pod can be started with 0 GPUs for the purposes of accessing data, though GPU-accelerated functions and web services will fail to work.
- vCPU
- System RAM
- Container Disk
  - It's temporary and removed when the pod is stopped or terminated.
  - You only pay for the container disk when the pod is running.
- Instance Volume
  - Data persists even when you reset or stop a Pod. Volume is removed when the Pod is terminated.
  - You pay for volume storage even when the Pod is stopped.
- Global Networking

### Create On-Demand Pod

<Tabs>
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```
```curl
```
```graphql
```
```json
```

---

# endpoints.md

File: sdks/javascript/endpoints.md


Interacting with RunPod's endpoints is a core feature of the SDK, enabling the execution of tasks and the retrieval of results.
This section covers the synchronous and asynchronous execution methods, along with checking the status of operations.

## Prerequisites

Before using the RunPod JavaScript, ensure that you have:

- Installed the RunPod JavaScript SDK.
- Configured your API key.

## Set your Endpoint Id

Set your RunPod API key and your Endpoint Id as environment variables.

```javascript
const { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;
import runpodSdk from "runpod-sdk";

const runpod = runpodSdk(RUNPOD_API_KEY);
const endpoint = runpod.endpoint(ENDPOINT_ID);
```

This allows all calls to pass through your Endpoint Id with a valid API key.

In most situations, you'll set a variable name `endpoint` on the `Endpoint` class.
This allows you to use the following methods or instances variables from the `Endpoint` class:

- [health](#health-check)
- [purge_queue](#purge-queue)
- [runSync](#run-synchronously)
- [run](#run-asynchronously)

## Run the Endpoint

Run the Endpoint with the either the asynchronous `run` or synchronous `runSync` method.

Choosing between asynchronous and synchronous execution hinges on your task's needs and application design.

### Run synchronously

To execute an endpoint synchronously and wait for the result, use the `runSync` method on your endpoint.
This method blocks the execution until the endpoint run is complete or until it times out.

<Tabs>
```javascript
```
```json
```
```javascript
```
```json
```
```javascript
```
```json
```
```javascript
```
```text
```
```javascript
```
```json
```
```python
```
```javascript
```
```json
```
```javascript
```
```json
```
```javascript
```
```json
```
```javascript
```
```json
```
```javascript
```
```json
```
```javascript
```
```json
```

---

# overview.md

File: sdks/javascript/overview.md

Get started with setting up your RunPod projects using JavaScript. Whether you're building web applications, server-side implementations, or automating tasks, the RunPod JavaScript SDK provides the tools you need.
This guide outlines the steps to get your development environment ready and integrate RunPod into your JavaScript projects.

## Install the RunPod SDK

Before integrating RunPod into your project, you'll need to install the SDK.
Using Node.js and npm (Node Package Manager) simplifies this process.
Ensure you have Node.js and npm installed on your system before proceeding.

To install the RunPod SDK, run the following npm command in your project directory.

```command
npm install --save runpod-sdk
# or
yarn add runpod-sdk
```

This command installs the `runpod-sdk` package and adds it to your project's `package.json` dependencies.
For more details about the package, visit the [npm package page](https://www.npmjs.com/package/runpod-sdk) or the [GitHub repository](https://github.com/runpod/js-sdk).

## Add your API key

To use the RunPod SDK in your project, you first need to import it and configure it with your API key and endpoint ID. Ensure these values are securely stored, preferably as environment variables.

Below is a basic example of how to initialize and use the RunPod SDK in your JavaScript project.

```javascript
const { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;
import runpodSdk from "runpod-sdk";

const runpod = runpodSdk(RUNPOD_API_KEY);
const endpoint = runpod.endpoint(ENDPOINT_ID);
```

This snippet demonstrates how to import the SDK, initialize it with your API key, and reference a specific endpoint using its ID.
Remember, the RunPod SDK uses the ES Module (ESM) system and supports asynchronous operations, making it compatible with modern JavaScript development practices.

### Secure your API key

When working with the RunPod SDK, it's essential to secure your API key.
Storing the API key in environment variables is recommended, as shown in the initialization example. This method keeps your key out of your source code and reduces the risk of accidental exposure.

:::note

Use environment variables or secure secrets management solutions to handle sensitive information like API keys.

:::

For more information, see the following:

- [RunPod SDK npm Package](https://www.npmjs.com/package/runpod-sdk)
- [RunPod GitHub Repository](https://github.com/runpod/js-sdk)
- [Endpoints](/sdks/javascript/endpoints)

---

# overview.md

File: sdks/overview.md

RunPod SDKs provide developers with tools to use the RunPod API for creating serverless functions and managing infrastructure.
They enable custom logic integration, simplify deployments, and allow for programmatic infrastructure management.

## Interacting with Serverless Endpoints

Once deployed, serverless functions is exposed as an Endpoints, you can allow external applications to interact with them through HTTP requests.

#### Interact with Serverless Endpoints:

Your Serverless Endpoints works similarly to an HTTP request.
You will need to provide an Endpoint Id and a reference to your API key to complete requests.

## Infrastructure management

The RunPod SDK facilitates the programmatic creation, configuration, and management of various infrastructure components, including Pods, Templates, and Endpoints.

### Managing Pods

Pods are the fundamental building blocks in RunPod, representing isolated environments for running applications.

#### Manage Pods:

1. **Create a Pod**: Use the SDK to instantiate a new Pod with the desired configuration.
2. **Configure the Pod**: Adjust settings such as GPU, memory allocation, and network access according to your needs.
3. **Deploy Applications**: Deploy your applications or services within the Pod.
4. **Monitor and scale**: Utilize the SDK to monitor Pod performance and scale resources as required.

### Manage Templates and Endpoints

Templates define the base environment for Pods, while Endpoints enable external access to services running within Pods.

#### Use Templates and Endpoints:

1. **Create a Template**: Define a Template that specifies the base configuration for Pods.
2. **Instantiate Pods from Templates**: Use the Template to create Pods with a consistent environment.
3. **Expose Services via Endpoints**: Configure Endpoints to allow external access to applications running in Pods.

---

# _loggers.md

File: sdks/python/_loggers.md

Logging is essential for insight into your application's performance and health.
It facilitates quick identification and resolution of issues, ensuring smooth operation.

Because of this, RunPod provides a structured logging interface, simplifying application monitoring and debugging, for your Handler code.

To setup logs, instantiate the `RunPodLogger()` module.

```python
import runpod

log = runpod.RunPodLogger()
```

Then set the log level.
In the following example, there are two logs levels being set.

```python
import runpod
import os

log = runpod.RunPodLogger()


def handler(job):
    try:
        job_input = job["input"]
        log.info("Processing job input")

        name = job_input.get("name", "World")
        log.info("Processing completed successfully")

        return f"Hello, {name}!"
    except Exception as e:
        # Log the exception with an error level log
        log.error(f"An error occurred: {str(e)}")
        return "An error occurred during processing."


runpod.serverless.start({"handler": handler})
```

## Log levels

RunPod provides a logging interface with types you're already familiar with.

The following provides a list of log levels you can set inside your application.

- `debug`: For in-depth troubleshooting. Use during development to track execution flow.
- `info`: (default) Indicates normal operation. Confirms the application is running as expected.
- `warn`: Alerts to potential issues. Signals unexpected but non-critical events.
- `error`: Highlights failures. Marks inability to perform a function, requiring immediate attention.

---

# apis.md

File: sdks/python/apis.md


This document outlines the core functionalities provided by the RunPod API, including how to interact with Endpoints, manage Templates, and list available GPUs.
These operations let you dynamically manage computational resources within the RunPod environment.

## Get Endpoints

To retrieve a comprehensive list of all available endpoint configurations within RunPod, you can use the `get_endpoints()` function.
This function returns a list of endpoint configurations, allowing you to understand what's available for use in your projects.

```python
import runpod
import os

runpod.api_key = os.getenv("RUNPOD_API_KEY")

# Fetching all available endpoints
endpoints = runpod.get_endpoints()

# Displaying the list of endpoints
print(endpoints)
```

## Create Template

Templates in RunPod serve as predefined configurations for setting up environments efficiently.
The `create_template()` function facilitates the creation of new templates by specifying a name and a Docker image.

<Tabs>
```python
```
```json
```
```python
```
```json
```
```python
```
```json
```
```python
```
```json
```

---

# endpoints.md

File: sdks/python/endpoints.md


This documentation provides detailed instructions on how to use the RunPod Python SDK to interact with various endpoints.
You can perform synchronous and asynchronous operations, stream data, and check the health status of endpoints.

## Prerequisites

Before using the RunPod Python, ensure that you have:

- Installed the RunPod Python SDK.
- Configured your API key.

## Set your Endpoint Id

Pass your Endpoint Id on the `Endpoint` class.

```python
import runpod
import os

runpod.api_key = os.getenv("RUNPOD_API_KEY")

endpoint = runpod.Endpoint("YOUR_ENDPOINT_ID")
```

This allows all calls to pass through your Endpoint Id with a valid API key.

In most situations, you'll set a variable name `endpoint` on the `Endpoint` class.
This allows you to use the following methods or instances variables from the `Endpoint` class:

- [health](#health-check)
- [purge_queue](#purge-queue)
- [run_sync](#run-synchronously)
- [run](#run-asynchronously)

## Run the Endpoint

Run the Endpoint with the either the asynchronous `run` or synchronous `run_sync` method.

Choosing between asynchronous and synchronous execution hinges on your task's needs and application design.

- **Asynchronous methods**: Choose the asynchronous method for handling tasks efficiently, especially when immediate feedback isn't crucial.
  They allow your application to stay responsive by running time-consuming operations in the background, ideal for:
  - **Non-blocking calls**: Keep your application active while waiting on long processes.
  - **Long-running operations**: Avoid timeouts on tasks over 30 seconds, letting your app's workflow continue smoothly.
  - **Job tracking**: Get a Job Id to monitor task status, useful for complex or delayed-result operations.

- **Synchronous methods**: Choose the synchronous method for these when your application requires immediate results from operations.
  They're best for:
  - **Immediate results**: Necessary for operations where quick outcomes are essential to continue with your app's logic.
  - **Short operations**: Ideal for tasks under 30 seconds to prevent application delays.
  - **Simplicity and control**: Provides a straightforward execution process, with timeout settings for better operational control.

### Run synchronously

To execute an endpoint synchronously and wait for the result, use the `run_sync` method.
This method blocks the execution until the endpoint run is complete or until it times out.

```python
import runpod
import os

runpod.api_key = os.getenv("RUNPOD_API_KEY")

endpoint = runpod.Endpoint("YOUR_ENDPOINT_ID")

try:
    run_request = endpoint.run_sync(
        {
            "prompt": "Hello, world!",
        },
        timeout=60,  # Timeout in seconds.
    )

    print(run_request)
except TimeoutError:
    print("Job timed out.")
```

### Run asynchronously

Asynchronous execution allows for non-blocking operations, enabling your code to perform other tasks while waiting for an operation to complete.
RunPod supports both standard asynchronous execution and advanced asynchronous programming with Python's [asyncio](https://docs.python.org/3/library/asyncio.html) framework.

Depending on your application's needs, you can choose the approach that best suits your scenario.

For non-blocking operations, use the `run` method.
This method allows you to start an endpoint run and then check its status or wait for its completion at a later time.

#### Asynchronous execution

This executes a standard Python environment without requiring an asynchronous event loop.

<Tabs>
```python
```
```text
```
```python
```
```text
```
```python
```
```json
```
```python
```
```python
```
```python
```
```text
```
```python
```
```json
```
```python
```
```text
```
```python
```

---

# overview.md

File: sdks/python/overview.md


Get started with setting up your RunPod projects using Python.
Depending on the specific needs of your project, there are various ways to interact with the RunPod platform.
This guide provides an approach to get you up and running.

## Install the RunPod SDK

Create a Python virtual environment to install the RunPod SDK library.
Virtual environments allow you to manage dependencies for different projects separately, avoiding conflicts between project requirements.

To get started, install setup a virtual environment then install the RunPod SDK library.

<Tabs>
    ```command
    ```
    ```command
    ```
    ```command
    ```
```command
```
    ```command
    ```
    ```command
    ```
    ```command
    ```
    ```python
    ```
    ```text
    ```
```python
```

---

# get-started.md

File: serverless/endpoints/get-started.md

Now that your Endpoint is deployed, send a test.
This is a great way to test your Endpoint before sending a request from your application.

## Send a Request

1. From the Endpoint's page, select **Requests**.
2. Choose **Run**.
3. You should see a successful response with the following:

```json
{
  "id": "6de99fd1-4474-4565-9243-694ffeb65218-u1",
  "status": "IN_QUEUE"
}
```

After a few minutes, the stream will show the full response.

You can now begin sending requests to your Endpoint from your terminal and an application.

## Send a request using cURL

Once your Endpoint is deployed, you can send a request.
This example sends a response to the Endpoint using cURL; however, you can use any HTTP client.

```curl
curl --request POST \
     --url https://api.runpod.ai/v2/${endpoint_id}/runsync
     --header "accept: application/json" \
     --header "authorization: ${YOUR_API_KEY}" \
     --header "content-type: application/json" \
     --data '
{
  "input": {
    "prompt": "A coffee cup.",
    "height": 512,
    "width": 512,
    "num_outputs": 1,
    "num_inference_steps": 50,
    "guidance_scale": 7.5,
    "scheduler": "KLMS"
  }
}
'
```

Where `endpoint_id` is the name of your Endpoint and `YOUR_API_KEY` is your API Key.

:::note

Depending on any modifications you made to your Handler Function, you may need to modify the request.

:::

## Next steps

Now that you have successfully launched an endpoint using a template, you can:

- [Invoke jobs](/serverless/endpoints/job-operations)

If the models provided aren't enough, you can write your own customize Function Handler:

- [Customize the Handler Function](/serverless/workers/handlers/overview)

---

# job-operations.md

File: serverless/endpoints/job-operations.md


This page provides instructions on job operations using the Runpod Endpoint.
You can invoke a job to run Endpoints the way you would interact with an API, get a status of a job, purge your job queue, and more with operations.

The following guide demonstrates how to use cURL to interact with an Endpoint.
You can also use the following SDK to interact with Endpoints programmatically:

- [Python SDK](/sdks/python/endpoints)

For information on sending requests, see [Send a request](/serverless/endpoints/send-requests).

## Asynchronous Endpoints

Asynchronous endpoints are designed for long-running tasks. When you submit a job through these endpoints, you receive a Job ID in response.
You can use this Job ID to check the status of your job at a later time, allowing your application to continue processing without waiting for the job to complete immediately.
This approach is particularly useful for tasks that require significant processing time or when you want to manage multiple jobs concurrently.

<Tabs>
```bash
```
```json
```
```bash
```
```json
```
```bash
```
```json
```
```bash
```
```json
```
```bash
```
```json
```
```bash
```
```json
```
```bash
```
```json
```

---

# manage-endpoints.md

File: serverless/endpoints/manage-endpoints.md

Learn to manage Severless Endpoints.

## Create an Endpoint

You can create an Endpoint in the Web interface.

1. Navigate to [Serverless Endpoints](https://www.runpod.io/console/serverless).
2. Select **+ New Endpoint** and enter the following:
   1. Endpoint Name.
   2. Select your GPUs.
   3. Configure your workers.
   4. Add a container image.
   5. Select **Deploy**.

## Delete an Endpoint

You can delete an Endpoint in the Web interface.
Before an Endpoint can be deleted, all workers must be removed.

1. Navigate to [Serverless Endpoints](https://www.runpod.io/console/serverless).
2. Select the Endpoint you'd like to remove.
3. Select **Edit Endpoint** and set **Max Workers** to `0`.
4. Choose **Update** and then **Delete Endpoint**.

## Edit an Endpoint

You can edit a running Endpoint in the Web interface after you've deployed it.

1. Navigate to [Serverless Endpoints](https://www.runpod.io/console/serverless).
2. Select the Endpoint you'd like to edit.
3. Select **Edit Endpoint** and make your changes.
4. Choose **Update**.

## Set GPU prioritization an Endpoint

When creating or modifying a Worker Endpoint, specify your GPU preferences in descending order of priority.
This allows you to configure the desired GPU models for your Worker Endpoints.

RunPod attempts to allocate your first choice if it's available.
If your preferred GPU isn't available, the system automatically defaults to the next available GPU in your priority list.

1. Navigate to [Serverless Endpoints](https://www.runpod.io/console/serverless).
2. Select the Endpoint you'd like to update.
3. Select the priority of the GPUs you'd like to use.
4. Choose **Update**.

:::note

You can force a configuration update by setting **Max Workers** to 0, selecting **Update**, then updating your max workers back to your needed value.

:::

## Add a Network Volume

Network volumes are a way to share data between Workers: they are mounted to the same path on each Worker.
For example, if a Worker contains a large-language model, you can use a network volume to share the model across all Workers.

1. Navigate to [Serverless Endpoints](https://www.runpod.io/console/serverless).
2. Select the Endpoint you'd like to edit.
3. Select **Edit Endpoint** and make your changes.
4. Under **Advanced** choose **Select Network Volume**.
5. Select the storage device and then choose **Update** to continue.

---

# overview.md

File: serverless/endpoints/overview.md

RunPod Endpoints serve as the gateway to deploying and managing your Serverless Workers.
These endpoints allow for flexible interaction with a variety of models, supporting both asynchronous and synchronous operations tailored to your computational needs.
Whether you're processing large data sets, requiring immediate results, or scheduling tasks to run in the background, RunPod's API Endpoints provide the versatility and scalability essential for modern computing tasks.

### Key features

- **Asynchronous and synchronous jobs:** Choose the execution mode that best fits your workflow, whether it's a task that runs in the background or one that delivers immediate results.
- **Serverless Workers:** Deploy your computational tasks without worrying about server management, enjoying the benefits of a fully managed infrastructure.
- **Scalability and flexibility:** Easily scale your operations up or down based on demand, with the flexibility to handle various computational loads.

### Key Concepts

Check out these two links for fundamental endpoint concepts, including key definitions and basic settings.

- [Glossary](../../glossary.md)
- [Settings](../references/endpoint-configurations.md)

### Getting started

Before you begin, ensure you have obtained your [RunPod API key](/get-started/api-keys).
This key is essential for authentication, billing, and accessing the API.

You can find your API key in the [user settings section](https://www.runpod.io/console/user/settings) of your RunPod account.

:::note

**Privacy and security:** RunPod prioritizes your data's privacy and security.
Inputs and outputs are retained for a maximum of 30 minutes for asynchronous requests and 1 minute for synchronous requests to protect your information.

:::

### Exploring RunPod Endpoints

Dive deeper into what you can achieve with RunPod Endpoints through the following resources:

- [Use the vLLM Worker](/serverless/workers/vllm/overview): Learn how to deploy a vLLM Worker as a Serverless Endpoint, with detailed guides on configuration and sending requests.
- [Invoke Jobs](/serverless/endpoints/job-operations): Learn how to submit jobs to your serverless workers, with detailed guides on both asynchronous and synchronous operations.
- [Send Requests](/serverless/endpoints/send-requests): Discover how to communicate with your endpoints, including tips on structuring requests for optimal performance.
- [Manage Endpoints](/serverless/endpoints/manage-endpoints): Find out how to manage your endpoints effectively, from deployment to scaling and monitoring.
- [Endpoint Operations](/serverless/references/operations): Access a comprehensive list of operations supported by RunPod Endpoints, including detailed documentation and examples.

---

# send-requests.md

File: serverless/endpoints/send-requests.md

Before sending a job request, ensure you have deployed your custom endpoint.

Let's start by constructing our request body to send to the endpoint.

## JSON Request Body

You can make requests to your endpoint with JSON. Your request must include a JSON object containing an `input` key. For example, if your handler requires an input prompt, you might send in something like this:

```json
{
  "input": {
    "prompt": "The lazy brown fox jumps over the"
  }
}
```

## Optional Inputs

Along with an input key, you can include other top-level inputs to access different functions. If a key is passed in at the top level and not included in the body of your request, it will be discarded and unavailable to your handler.

The following optional inputs are available to all endpoints regardless of the worker.

- Webhooks
- Execution policies
- S3-compatible storage

### Webhooks

To see notifications for completed jobs, pass a URL in the top level of the request:

```json
{
  "input": {},
  "webhook": "https://URL.TO.YOUR.WEBHOOK"
}
```

Your webhook endpoint should respond with a `200` status to acknowledge the successful call. If the call is not successful, the request waits 10 seconds and sends the call again up to two more times.

A `POST` request goes to your URL when the job is complete. This request contains the same information as fetching the results from the `/status/{job_id}` endpoint.

### Execution Policies

By default, if a job remains `IN_PROGRESS` for longer than 10 minutes without completion, it's automatically terminated.

This default behavior keeps a hanging request from draining your account credits.

To customize the management of job lifecycles and resource consumption, the following policies can be configured:

- **Execution Timeout**: Specifies the maximum duration that a job can run before it's automatically terminated. This limit helps prevent jobs from running indefinitely and consuming resources. You can overwrite the value for a request by specifying `executionTimeout` in the job input.

:::note

Changing the **Execution Timeout** value through the Web UI sets the value for all requests to an Endpoint.
You can still overwrite the value for individual requests with `executionTimeout` in the job input.

:::

- **Low Priority**: When true, the job does not trigger scaling up resources to execute. Instead, it executes when there are no pending higher priority jobs in the queue. Use this option for tasks that are not time-sensitive.
- **TTL (Time-to-Live)**: Defines the maximum time a job can remain in the queue before it's automatically terminated. This parameter ensures that jobs don't stay in the queue indefinitely.

```json
{
  "input": {},
  "policy": {
    "executionTimeout": int, // Time in milliseconds. Must be greater than 5 seconds.
    "lowPriority": bool, // Sets the job's priority to low. Default behavior escalates to high under certain conditions.
    "ttl": int // Time in milliseconds. Must be greater than or equal to 10 seconds. Default is 24 hours. Maximum is one week.
  }
}
```

By configuring the execution timeout, priority, and TTL policies, you have more control over job execution and efficient system resource management.

### S3-Compatible Storage

Pass in the credentials for S3-compatible object storage as follows:

```json
{
  "input": {},
  "s3Config": {
    "accessId": "key_id_or_username",
    "accessSecret": "key_secret_or_password",
    "bucketName": "storage_location_name",
    "endpointUrl": "storage_location_address"
  }
}
```

The configuration only passes to the worker. It is not returned as part of the job request output.

:::note

The serverless worker must contain logic that allows it to use this input. If you build a custom endpoint and request s3Config in the job input, your worker is ultimately responsible for using the information passed in to upload the output.

:::

---

# get-started.md

File: serverless/get-started.md

## Build a Serverless Application on RunPod

Follow these steps to set up a development environment, create a handler file, test it locally, and build a Docker image for deployment:

1. Create a Python virtual environment and install RunPod SDK

```bash
# 1. Create a Python virtual environment
python3 -m venv venv

# 2. Activate the virtual environment
# On macOS/Linux:

source venv/bin/activate

# On Windows:
venv\Scripts\activate

# 3. Install the RunPod SDK
pip install runpod
```

2. Create the handler file (rp_handler.py):

```python
import runpod
import time

def handler(event):
    input = event['input']
    instruction = input.get('instruction')
    seconds = input.get('seconds', 0)

    # Placeholder for a task; replace with image or text generation logic as needed
    time.sleep(seconds)
    result = instruction.replace(instruction.split()[0], 'created', 1)

    return result

if __name__ == '__main__':
    runpod.serverless.start({'handler': handler})
```

3. Create a test_input.json file in the same folder:

```python
{
    "input": {
        "instruction": "create a image",
        "seconds": 15
    }
}
```

4. Test the handler code locally:

```python
python3 rp_handler.py

# You should see an output like this:
--- Starting Serverless Worker |  Version 1.7.0 ---
INFO   | Using test_input.json as job input.
DEBUG  | Retrieved local job: {'input': {'instruction': 'create a image', 'seconds': 15}, 'id': 'local_test'}
INFO   | local_test | Started.
DEBUG  | local_test | Handler output: created a image
DEBUG  | local_test | run_job return: {'output': 'created a image'}
INFO   | Job local_test completed successfully.
INFO   | Job result: {'output': 'created a image'}
INFO   | Local testing complete, exiting.
```

5. Create a Dockerfile:

```docker
FROM python:3.10-slim

WORKDIR /
RUN pip install --no-cache-dir runpod
COPY rp_handler.py /

# Start the container
CMD ["python3", "-u", "rp_handler.py"]
```

6. Build and push your Docker image

```command
docker build --platform linux/amd64 --tag <username>/<repo>:<tag> .
```

7. Push to your container registry:

```command
docker push <username>/<repo>:<tag>
```

:::note

When building your docker image, you might need to specify the platform you are building for.
This is important when you are building on a machine with a different architecture than the one you are deploying to.

When building for RunPod providers use `--platform=linux/amd64`.

:::

Alternatively, you can clone our [worker-basic](https://github.com/runpod-workers/worker-basic) repository to quickly build a Docker image and push it to your container registry for a faster start.

Now that you've pushed your container registry, you're ready to deploy your Serverless Endpoint to RunPod.

## Deploy a Serverless Endpoint

This step will walk you through deploying a Serverless Endpoint to RunPod. You can refer to this walkthrough to deploy your own custom Docker image.

<iframe 
    src="https://app.tango.us/app/embed/7df17d43-9467-4d09-9b0f-19eba8a17249" 
    sandbox="allow-scripts allow-top-navigation-by-user-activation allow-popups allow-same-origin" 
    security="restricted" 
    title="Deploy your first serverless endpoint" 
    width="100%" 
    height="600px" 
    referrerpolicy="strict-origin-when-cross-origin" 
    frameborder="0" 
    webkitallowfullscreen="webkitallowfullscreen" 
    mozallowfullscreen="mozallowfullscreen" 
    allowfullscreen
></iframe>

---

# github-integration.md

File: serverless/github-integration.md

RunPod manages the container registry and docker build process, enabling seamless integration with your developer workflow.

1. Pulls your code and Dockerfile
2. Builds the container image using layer caching for speed
3. Stores it in our secure container registry
4. Deploys it to your endpoint

This integration enables you to focus on development while RunPod handles the infrastructure complexity.

:::note

You must use [RunPod](/serverless/workers/development/overview) Python library to develop your Serverless worker.

:::

## Authorize RunPod

You will need to authorize RunPod to have access to your GitHub repository.

You can connect and authorize your GitHub account either through the [settings page](http://runpod.io/console/user/settings) or when deploying through the GitHub integration for the first time.
Only one GitHub account per RunPod account can be connected at a time.

Authorizing the integration allows the following options:

- **All repositories:** This applies to all current _and_ future repositories owned by the resource owner. Also includes public repositories (read-only).
- **Only select repositories**: Select at least one repository. Also includes public repositories (read-only).

You can manage the connection through the settings page of RunPod or within your GitHub account.

## Set up

To get started with the GitHub integration use the following steps:

1. Go to the [Serverless section](http://runpod.io/console/serverless).
2. Select **+ New Endpoint** and choose **GitHub Repo**.
3. Select **Next**.
4. Select the repository you want to connect to and choose **Next**.
5. Configure your deployment options and choose **Next**:
   1. **Branch**: Select the branch to watch for updates to.
   2. **Dockerfile**: Specify the path to the Dockerfile.
6. Configure your compute options

Your GitHub repository is now configured with RunPod.

Every `git push` to your specified branch results in an updated Endpoint.

:::note

Your first build will take some time; however, every subsequent build will rely on RunPod's intelligent layer caching to build your container images faster.

:::

## Multiple Environments

GitHub Integration enables streamlined development workflows for your Serverless endpoints. By cloning endpoints and connecting them to different branches, you can maintain separate environments for testing and production.

For instance:

- Production endpoint tracking the `main` branch
- Staging endpoint tracking the `dev` branch

Each environment maintains independent GPU and worker configurations. To set this up, select **Clone Endpoint** and modify the repository branch setting.
This ensures safe testing while maintaining full control over your deployment environments.

## Status

Builds can have the following statuses:

| Status name | Description                                  |
| ----------- | -------------------------------------------- |
| Building    | Your container is building.                  |
| Failed      | Something went wrong. Check your build logs. |
| Pending     | RunPod is scheduling the build.              |
| Uploading   | Your container is uploading to the registry. |
| Completed   | The container build and upload is complete.  |

## Known limitations

- Private registry base images
  At the moment, RunPod does not support privately hosted images as base images for docker build. A good workaround is to pack as much of the content in the privately hosted image into the image you are building.

- GPU builds
  Some builds require GPUs. A good example is ones that rely on the GPU build version of bitsandbytes.

- Images only served on the RunPod platform
  Images that are built using runpod's image builder service cannot be used anywhere else.

## Disconnect GitHub

To disconnect your GitHub account, go to Settings → Connections → Edit Connection, select your GitHub account, click Configure, scroll down to the Danger Zone, and uninstall “RunPod Inc.”.

---

# overview.md

File: serverless/overview.md

RunPod offers Serverless GPU and CPU computing for AI inference, training, and general compute, allowing users to pay by the second for their compute usage.
This flexible platform is designed to scale dynamically, meeting the computational needs of AI workloads from the smallest to the largest scales.

You can use the following methods:

- Handler Functions: Bring your own functions and run in the cloud.
- Quick Deploy: Quick deploys are pre-built custom endpoints of the most popular AI models.

## Why RunPod Serverless?

You should choose RunPod Serverless instances for the following reasons:

- **AI Inference:** Handle millions of inference requests daily and can be scaled to handle billions, making it an ideal solution for machine learning inference tasks. This allows users to scale their machine learning inference while keeping costs low.
- **Autoscale:** Dynamically scale workers from 0 to 100 on the Secure Cloud platform, which is highly available and distributed globally. This provides users with the computational resources exactly when needed.
- **AI Training:** Machine learning training tasks that can take up to 12 hours. GPUs can be spun up per request and scaled down once the task is done, providing a flexible solution for AI training needs.
- **Container Support:** Bring any Docker container to RunPod. Both public and private image repositories are supported, allowing users to configure their environment exactly how they want.
- **3s Cold-Start:** To help reduce cold-start times, RunPod proactively pre-warms workers. The total start time will vary based on the runtime, but for stable diffusion, the total start time is 3 seconds cold-start plus 5 seconds runtime.
- **Metrics and Debugging:** Transparency is vital in debugging. RunPod provides access to GPU, CPU, Memory, and other metrics to help users understand their computational workloads. Full debugging capabilities for workers through logs and SSH are also available, with a web terminal for even easier access.
- **Webhooks:** Users can leverage webhooks to get data output as soon as a request is done. Data is pushed directly to the user's Webhook API, providing instant access to results.

RunPod Serverless are not just for AI Inference and Training.
They're also great for a variety of other use cases.
Feel free to use them for tasks like rendering, molecular dynamics, or any other computational task that suits your needs.

<!--
### Endpoints

A Serverless Endpoint provides the REST API endpoint that serves your application.
You can create multiple endpoints for your application, each with its own configuration.

### Serverless handlers

Serverless handlers are the core of the Serverless platform.
They are the code that is executed when a request is made to a Serverless endpoint.
Handlers are written in Python and can be used to run any code that can be run in a Docker container.
-->

---

# quick-deploys.md

File: serverless/quick-deploys.md

Quick Deploys lets you deploy custom Endpoints of popular models with minimal configuration.

You can find [Quick Deploys](https://www.runpod.io/console/serverless) and their descriptions in the Web interface.

## How to do I get started with Quick Deploys?

You can get started by following the steps below:

1. Go to the [Serverless section](https://www.runpod.io/console/serverless) in the Web interface.
2. Select your model.
3. Provide a name for your Endpoint.
4. Select your GPU instance.
   1. (optional) You can further customize your deployment.
5. Select **Deploy**.

Your Endpoint Id is now created and you can use it in your application.

## Customizing your Functions

To customize AI Endpoints, visit the [RunPod GitHub repositories](https://github.com/runpod-workers).
Here, you can fork the programming and compute model templates.

Begin with the [worker-template](https://github.com/runpod-workers/worker-template) and modify it as needed.
These RunPod workers incorporate CI/CD features to streamline your project setup.

For detailed guidance on customizing your interaction Endpoints, refer to [Handler Functions](/serverless/workers/handlers/overview).

---

# endpoint-configurations.md

File: serverless/references/endpoint-configurations.md

The following are configurable settings within an Endpoint.

## Endpoint Name

Create a name you'd like to use for the Endpoint configuration.
The resulting Endpoint is assigned a random ID to be used for making calls.

The name is only visible to you.

## GPU Selection

Select one or more GPUs that you want your Endpoint to run on. RunPod matches you with GPUs in the order that you select them, so the first GPU type that you select is prioritized, then the second, and so on. Selecting multiple GPU types can help you get a worker more quickly, especially if your first selection is an in-demand GPU.

## Active (Min) Workers

Setting the active workers to 1 or more ensures you have “always on” workers, ready to respond to job requests without cold start delays.

Default: 0

:::note

Active workers incur charges as soon as you enable them (set to >0), but they come with a discount of up to 30% off the regular price.

:::

## Max Workers

Max workers set a limit on the number of workers your endpoint can run simultaneously. If the max workers are set too low, you might experience [throttled workers](/glossary#throttled-worker). To prevent this, consider increasing the max workers to 5 or more if you see frequent throttling.

Default: 3

<details>
<summary>

How to configure Max Workers

</summary>
You can also configure a max worker count. This is the top limit of what RunPod will attempt to auto-scale for you. Use this to cap your concurrent request count and also limit your cost ceiling.

:::note

We currently base your caching coefficient by this number, so an endpoint with higher max worker count will also receive a higher priority when caching workers.

This is partially why we limit new accounts to a relatively low max concurrency at the account level.
If you want to get this number raised, you generally will need to have a higher history of spending, or commit to a relatively high spend per month.

You should generally aim to set your max worker count to be 20% higher than you expect your max concurrency to be.

:::

</details>

## GPUs / Worker

The number of GPUs you would like assigned to your worker.

:::note

Currently only available for 48 GB GPUs.

:::

## Idle Timeout

The amount of time a worker remains running after completing its current request. During this period, the worker stays active, continuously checking the queue for new jobs, and continues to incur charges. If no new requests arrive within this time, the worker will go to sleep.

Default: 5 seconds

## FlashBoot

FlashBoot is RunPod’s magic solution for reducing the average cold-start times on your endpoint. It works probabilistically. When your endpoint has consistent traffic, your workers have a higher chance of benefiting from FlashBoot for faster spin-ups. However, if your endpoint isn’t receiving frequent requests, FlashBoot has fewer opportunities to optimize performance. There’s no additional cost associated with FlashBoot.

## Advanced

Additional controls to help you control where your endpoint is deployed and how it responds to incoming requests.

### Data Centers

Control which data centers can deploy and cache your workers. Allowing multiple data centers can help you get a worker more quickly.

Default: all data centers

### Select Network Volume

Attach a network storage volume to your deployed workers.

Network volumes will be mounted to `/runpod-volume/`.

:::note

While this is a high performance network drive, do keep in mind that it will have higher latency than a local drive.

This will limit the availability of cards, as your endpoint workers will be locked to the datacenter that houses your network volume.

:::

### Scale Type

- **Queue Delay** scaling strategy adjusts worker numbers based on request wait times. With zero workers initially, the first request adds one worker. Subsequent requests add workers only after waiting in the queue for the defined number of delay seconds.
- **Request Count** scaling strategy adjusts worker numbers according to total requests in the queue and in progress. It automatically adds workers as the number of requests increases, ensuring tasks are handled efficiently.

```text
_Total Workers Formula: Math.ceil((requestsInQueue + requestsInProgress) / <set request count)_
```

### GPU Types

Within the select GPU size category you can further select which GPU models you would like your endpoint workers to run on.
Default: `4090` | `A4000` | `A4500`

<details>
<summary>
What's the difference between GPU models.
</summary>
A100s are about 2-3x faster than A5000s and also allow double the VRAM with very high bandwidth throughout. 3090s and A5000s are 1.5-2x faster than A4000s. Sometimes, it may make more sense to use 24 GB even if you don't need it compared to 16 GB due to faster response times. Depending on the nature of the task, it's also possible that execution speeds may be bottlenecked and not significantly improved simply by using a higher-end card. Do your own calculations and experimentation to determine out what's most cost-effective for your workload and task type.

Want access to different flavors? [Let us know](https://www.runpod.io/contact) and we can look at expanding our offerings!

</details>

## CUDA version selection

You have the ability to select the allowed CUDA versions for your workloads.
The CUDA version selection determines the compatible GPU types that will be used to execute your serverless tasks.

Specifically, the CUDA version selection works as follows:

- You can choose one or more CUDA versions that your workload is compatible with or requires.
- RunPod will then match your workload to available GPU instances that have the selected CUDA versions installed.
- This ensures that your serverless tasks run on GPU hardware that meets the CUDA version requirements.

For example, if you select CUDA 11.6, your serverless tasks will be scheduled to run on GPU instances that have CUDA 11.6 or a compatible version installed. This allows you to target specific CUDA versions based on your workload's dependencies or performance requirements.

---

# job-states.md

File: serverless/references/job-states.md

When working with Handler Functions in RunPod, it's essential to understand the various states a job can go through from initiation to completion.
Each state provides insight into the job's current status and helps in managing the job flow effectively.

## Job state

Here are the states a job can be in:

- `IN_QUEUE`: This state indicates that the job is currently in the endpoint queue. It's waiting for an available worker to pick it up for processing.
- `IN_PROGRESS`: Once a worker picks up the job, its state changes to `IN_PROGRESS`. This means the job is actively being processed and is no longer in the queue.
- `COMPLETED`: After the job successfully finishes processing and returns a result, it moves to the `COMPLETED` state. This indicates the successful execution of the job.
- `FAILED`: If a job encounters an error during its execution and returns with an error, it is marked as `FAILED`. This state signifies that the job did not complete successfully and encountered issues.
- `CANCELLED`: Jobs can be manually cancelled using the `/cancel/job_id` endpoint. If a job is cancelled before it completes or fails, it will be in the `CANCELLED` state.
- `TIMED_OUT`: This state occurs in two scenarios: when a job expires before a worker picks it up, or if the worker fails to report back a result for the job before it reaches its timeout threshold.

---

# operations.md

File: serverless/references/operations.md

RunPod's Endpoints facilitate submitting jobs and retrieving outputs. Access these endpoints at: `https://api.runpod.ai/v2/{endpoint_id}/{operation}`

### /run

- **Method**: `POST`
- **Description**: Asynchronous endpoint for submitting jobs
- **Returns**: Unique Job ID
- **Payload Limit**: 10 MB
- **Rate Limit**: 1000 requests per 10 seconds, 200 concurrent
- **Job Availability**: 30 minutes after completion

### /runsync

- **Method**: `POST`
- **Description**: Synchronous endpoint for shorter running jobs
- **Returns**: Immediate results
- **Payload Limit**: 20 MB
- **Rate Limit**: 2000 requests per 10 seconds, 400 concurrent
- **Job Availability**: 60 seconds after completion

### Queue Limits

- Requests will receive a `429 (Too Many Requests)` status if:
  - Queue size exceeds 50 jobs AND
  - Queue size exceeds endpoint.WorkersMax * 500

### Status and Stream Operations

All status and stream operations share a rate limit of 2000 requests per 10 seconds, 400 concurrent:

- `/status/{job_id}` - Check job status and retrieve outputs
  - Methods: `GET` | `POST`
- `/status-sync/{job_id}` - Synchronous status check
  - Methods: `GET` | `POST`
- `/stream/{job_id}` - Stream results from generator-type handlers
  - Methods: `GET` | `POST`

### Additional Operations

- `/cancel/{job_id}`
  - **Method**: `POST`
  - **Rate Limit**: 100 requests per 10 seconds, 20 concurrent

- `/purge-queue`
  - **Method**: `POST`
  - **Description**: Clears all queued jobs (does not affect running jobs)
  - **Rate Limit**: 2 requests per 10 seconds

- `/health`
  - **Method**: `GET`
  - **Description**: Provides worker statistics and endpoint health

- `/requests`
  - **Method**: `GET`
  - **Rate Limit**: 10 requests per 10 seconds, 2 concurrent

To see how to run these Endpoint Operations, see [Invoke a Job](/serverless/endpoints/job-operations).

---

# deploy.md

File: serverless/workers/deploy/deploy.md

Once you have a Handler Function, the next step is to package it into a Docker image that can be deployed as a scalable Serverless Worker.
This is accomplished by defining a Docker file to import everything required to run your handler. Example Docker files are in the [runpod-workers](https://github.com/orgs/runpod-workers/repositories) repository on GitHub.

:::note

For deploying large language models (LLMs), you can use the [Configurable Endpoints](/serverless/workers/vllm/configurable-endpoints) feature instead of working directly with Docker.

Configurable Endpoints simplify the deployment process by allowing you to select a pre-configured template and customize it according to your needs.

:::

_Unfamiliar with Docker? Check out Docker's [overview page](https://docs.docker.com/get-started/overview/) or see our guide on [Containers](/category/containers)._

## Docker file

Let's say we have a directory that looks like the following:

```
project_directory
├── Dockerfile
├── src
│   └── handler.py
└── builder
    └── requirements.txt
```

Your Dockerfile would look something like this:

```text Docker
from python:3.11.1-buster

WORKDIR /

COPY builder/requirements.txt .
RUN pip install -r requirements.txt

ADD handler.py .

CMD [ "python", "-u", "/handler.py" ]
```

To build and push the image, review the steps in [Get started](/serverless/workers/overview).

> 🚧 If your handler requires external files such as model weights, be sure to cache them into your docker image. You are striving for a completely self-contained worker that doesn't need to download or fetch external files to run.

## Continuous integrations

Integrate your Handler Functions through continuous integration.

The [Test Runner](https://github.com/runpod/test-runner) GitHub Action is used to test and integrate your Handler Functions into your applications.

:::note

Running any Action that sends requests to RunPod occurs a cost.

:::

You can add the following to your workflow file:

```yaml
- uses: actions/checkout@v3
- name: Run Tests
  uses: runpod/runpod-test-runner@v1
  with:
    image-tag: [tag of image to test]
    runpod-api-key: [a valid Runpod API key]
    test-filename: [path for a json file containing a list of tests, defaults to .github/tests.json]
    request-timeout: [number of seconds to wait on each request before timing out, defaults to 300]
```

If `test-filename` is omitted, the Test Runner Action attempts to look for a test file at `.github/tests.json`.

You can find a working example in the [Worker Template repository](https://github.com/runpod-workers/worker-template/tree/main/.github).

## Using Docker tags

We also highly recommend the use of tags for Docker images and not relying on the default `:latest` tag label, this will make version tracking and releasing updates significantly easier.

### Docker Image Versioning

To ensure consistent and reliable versioning of Docker images, we highly recommend using SHA tags instead of relying on the default `:latest` tag.

Using SHA tags offers several benefits:

- **Version Control:** SHA tags provide a unique identifier for each image version, making it easier to track changes and updates.
- **Reproducibility:** By using SHA tags, you can ensure that the same image version is used across different environments, reducing the risk of inconsistencies.
- **Security:** SHA tags help prevent accidental overwrites and ensure that you are using the intended image version.

### Using SHA Tags

To pull a Docker image using its SHA tag, use the following command:

```bash
docker pull <image_name>@<sha256:hash>
```

For example:

```bash
docker pull myapp@sha256:4d3d4b3c5a5c2b3a5a5c3b2a5a4d2b3a2b3c5a3b2a5d2b3a3b4c3d3b5c3d4a3
```

### Best Practices

- Avoid using the `:latest` tag, as it can lead to unpredictable behavior and make it difficult to track which version of the image is being used.
- Use semantic versioning (e.g., `v1.0.0`, `v1.1.0`) along with SHA tags to provide clear and meaningful version identifiers.
- Document the SHA tags used for each deployment to ensure easy rollback and version management.

## Other considerations

While we do not impose a limit on the Docker image size your container registry might have, be sure to review any limitations they may have. Ideally, you want to keep your final Docker image as small as possible and only container the absolute minimum to run your handler.

---

# cleanup.md

File: serverless/workers/development/cleanup.md


When developing for RunPod serverless, it's crucial to manage resources efficiently.
The RunPod SDK provides a `clean()` function to help you remove temporary files and folders after processing.
This guide will show you how to use this cleanup utility effectively.

## The clean() Function

The `clean()` function is part of RunPod's serverless utilities.
It helps maintain a clean environment by removing specified folders and files after a job is completed.

To use it, import the function from the RunPod serverless utilities:

```python
from runpod.serverless.utils.rp_cleanup import clean
```

## Default Behavior

By default, `clean()` removes the following:

- `input_objects` folder
- `output_objects` folder
- `job_files` folder
- `output.zip` file

## Using clean() in Your Handler

Here's an example of how to incorporate the `clean()` function in your AI model handler:

<Tabs>
```python
```
```python
```
```bash
```
```json
```
```bash
```

---

# concurrency.md

File: serverless/workers/development/concurrency.md

In this tutorial, we'll dive deep into the `--rp_api_concurrency` argument of the RunPod Python SDK.
This powerful feature allows you to simulate multiple concurrent requests to your serverless function, mimicking real-world scenarios more closely.

## What is rp_api_concurrency?

The `--rp_api_concurrency` argument controls the number of concurrent workers that the local FastAPI server uses when simulating the RunPod serverless environment. Each worker can handle a separate request simultaneously, allowing you to test how your function performs under parallel execution.

### Basic Usage

To set the number of concurrent workers, use the `--rp_api_concurrency` flag followed by the desired number of workers:

```bash
python your_function.py --rp_serve_api --rp_api_concurrency 2
```

This command starts your local server with 2 concurrent workers.

### Example: Testing a Counter Function

Let's create a simple function that increments a counter and test it with different concurrency settings.

1. Create a file named `counter_function.py`:

```python
import runpod

counter = 0


def handler(event):
    global counter
    counter += 1
    return {"counter": counter}


runpod.serverless.start({"handler": handler})
```

2. Run the function with a single worker:

```bash
python counter_function.py --rp_serve_api --rp_api_concurrency 1
```

3. In another terminal, use curl to send multiple requests.
   Create a new file called `counter.sh` and add the following to the file.

```bash
for i in {1..10}; do
    curl -X POST http://localhost:8000/runsync -H "Content-Type: application/json" -d '{"input": {}}' &
done
```

To execute this file run `bash counter.sh`.

4. Observe the results. With a single worker, the requests are processed sequentially, and you'll see the counter increment from 1 to 10.

5. Now, let's run the function with multiple workers:

```bash
python counter_function.py --rp_serve_api --rp_api_concurrency 4
```

6. When you try to run this command, you'll encounter the following error:

```
WARNING:  You must pass the application as an import string to enable 'reload' or 'workers'.
```

This error occurs because the RunPod SDK integrates with FastAPI to create the local server, and FastAPI has certain expectations about how the application is structured and named.

7. To resolve this issue, we need to understand a bit more about the FastAPI integration:

   - The RunPod SDK uses FastAPI to create an ASGI application that simulates the serverless environment.
   - FastAPI's underlying server, Uvicorn, expects the main application to be in a file named `main.py` by default.
   - When you use the `--rp_api_concurrency` flag to specify multiple workers, Uvicorn tries to spawn separate processes, each running your application.

8. To make this work, we need to rename our file to `main.py`. This allows Uvicorn to correctly import and run multiple instances of your application. Here's what you need to do:

   a. Rename your `counter_function.py` to `main.py`:

   ```bash
   mv counter_function.py main.py
   ```

   b. Now, run the command again:

   ```bash
   python main.py --rp_serve_api --rp_api_concurrency 4
   ```

   This time, the command should work without errors, starting your local server with 4 concurrent workers.

9. With the server running, you can now send multiple requests using the curl command from step 3:

```bash
for i in {1..10}; do
    curl -X POST http://localhost:8000/run -H "Content-Type: application/json" -d '{"input": {}}' &
done
```

10. Observe the results. With multiple workers, you might see inconsistent results due to race conditions.
    The counter might not reach 10, and you may see duplicate values.

## Handling Concurrency in your code

To make your function concurrency-safe, you need to use appropriate synchronization mechanisms.
Here's an improved version of the counter function:

```python
import runpod
from threading import Lock

counter = 0
counter_lock = Lock()


def handler(event):
    global counter
    with counter_lock:
        counter += 1
        return {"counter": counter}


runpod.serverless.start({"handler": handler})
```

Now, even with multiple workers, the counter will increment correctly.

## Best Practices for Using rp_api_concurrency

1. **Start Low**: Begin testing with a low number of workers and gradually increase.
2. **Match Production**: Set concurrency to match your expected production configuration.
3. **Test Varied Loads**: Try different concurrency levels to understand your function's behavior under various conditions.
4. **Monitor Resources**: Keep an eye on CPU and memory usage as you increase concurrency.
5. **Use Logging**: Implement detailed logging to track the flow of concurrent executions.

It's important to note that `--rp_api_concurrency` provides concurrent execution, not necessarily parallel execution.
The degree of parallelism depends on your system's capabilities and the nature of your function.

## Conclusion

The `--rp_api_concurrency` argument is a powerful tool for testing your RunPod serverless functions under more realistic conditions. By simulating concurrent requests, you can identify and resolve issues related to race conditions, resource contention, and scalability before deploying to production.

Remember, while local testing with concurrency is valuable, it's not a complete substitute for load testing in a production-like environment.
Use this feature as part of a comprehensive testing strategy to ensure your serverless functions are robust and scalable.

---

# debugger.md

File: serverless/workers/development/debugger.md

In the previous lesson, we covered the basics of running your RunPod serverless functions locally.
Now, let's explore some advanced options that give you more control over your local testing environment.

The RunPod Python SDK provides several command-line arguments that allow you to customize your local testing setup.

Let's go through each of these options:

### Controlling Log Levels

```bash
--rp_log_level ERROR | WARN | INFO | DEBUG
```

This argument allows you to set the verbosity of the console output. Options are:

- `ERROR`: Only show error messages
- `WARN`: Show warnings and errors
- `INFO`: Show general information, warnings, and errors
- `DEBUG`: Show all messages, including detailed debug information

Example:

```bash
python hello_world.py --rp_server_api --rp_log_level DEBUG
```

The `--rp_log_level` flag enables the RunPod debugger, which can help you troubleshoot issues in your code.

Example:

```bash
python hello_world.py --rp_server_api --rp_debugger
```

### Customizing the API Server

The following arguments allow you to configure the FastAPI server that simulates the RunPod serverless environment:

```bash
--rp_serve_api
--rp_api_port <port_number>
--rp_api_concurrency <number_of_workers>
--rp_api_host <hostname>
```

- `--rp_serve_api`: Starts the API server
- `--rp_api_port`: Sets the port number (default is 8000)
- `--rp_api_concurrency`: Sets the number of concurrent workers (default is 1)
- `--rp_api_host`: Sets the hostname (default is "localhost")

Example:

```bash
python hello_world.py --rp_serve_api --rp_api_port 8080 --rp_api_concurrency 4 --rp_api_host 0.0.0.0
```

This command starts the API server on port 8080 with 4 concurrent workers and makes it accessible from other devices on the network.

### Providing test input

As we saw in the previous lesson, you can provide test input either through a JSON file or directly via the command line:

```bash
--test_input '<JSON_string>'
```

Example:

```bash
python hello_world.py --rp_server_api --test_input '{"input": {"name": "RunPod"}}'
```

You can combine these arguments to create a highly customized local testing environment. Here's an example that uses multiple options:

```bash
python hello_world.py --rp_server_api --rp_log_level DEBUG --rp_debugger --rp_api_port 8080 --rp_api_concurrency 2 --test_input '{"input": {"name": "Advanced Tester"}}'
```

This command:

1. Starts the local server
2. Sets the log level to DEBUG for maximum information
3. Enables the debugger
4. Uses port 8080 for the API server
5. Sets up 2 concurrent workers
6. Provides a test input directly in the command

## Conclusion

These advanced options for local testing with the RunPod Python SDK give you fine-grained control over your development environment. By mastering these tools, you can ensure your serverless functions are robust and ready for deployment to the RunPod cloud.

In the next lesson, we'll explore how to structure more complex handlers to tackle advanced use cases in your serverless applications.

---

# environment-variables.md

File: serverless/workers/development/environment-variables.md

Incorporating environment variables into your Handler Functions is a key aspect of managing external resources like S3 buckets.

This section focuses on how to use environment variables to facilitate the uploading of images to an S3 bucket using RunPod Handler Functions.

You will go through the process of writing Python code for the uploading and setting the necessary environment variables in the Web interface.

## Prerequisites

- Ensure the RunPod Python library is installed: `pip install runpod`.
- Have an image file named `image.png` in the Docker container's working directory.

## Python Code for S3 Uploads

Let's break down the steps to upload an image to an S3 bucket using Python:

1. **Handler Function for S3 Upload**:
   Here's an example of a handler function that uploads `image.png` to an S3 bucket and returns the image URL:

   ```python
   from runpod.serverless.utils import rp_upload
   import runpod


   def handler(job):
       image_url = rp_upload.upload_image(job["id"], "./image.png")
       return [image_url]


   runpod.serverless.start({"handler": handler})
   ```

2. **Packaging Your Code**:
   Follow the guidelines in [Worker Image Creation](/serverless/workers/deploy) for packaging and deployment.

### Setting Environment Variables for S3

Using environment variables securely passes the necessary credentials and configurations to your serverless function:

1. **Accessing Environment Variables Setting**:
   In the template creation/editing interface of your pod, navigate to the bottom section where you can set environment variables.

2. **Configuring S3 Variables**:
   Set the following key variables for your S3 bucket:
   - `BUCKET_ENDPOINT_URL`
   - `BUCKET_ACCESS_KEY_ID`
   - `BUCKET_SECRET_ACCESS_KEY`

Ensure that your `BUCKET_ENDPOINT_URL` includes the bucket name.
For example: `https://your-bucket-name.nyc3.digitaloceanspaces.com` | `https://your-bucket-name.nyc3.digitaloceanspaces.com`

## Testing your API

Finally, test the serverless function to confirm that it successfully uploads images to your S3 bucket:

1. **Making a Request**:
   Make a POST request to your API endpoint with the necessary headers and input data. Remember, the input must be a JSON item:

   ```python
   import requests

   endpoint = "https://api.runpod.ai/v2/xxxxxxxxx/run"
   headers = {"Content-Type": "application/json", "Authorization": "Bearer XXXXXXXXXXXXX"}
   input_data = {"input": {"inp": "this is an example input"}}

   response = requests.post(endpoint, json=input_data, headers=headers)
   ```

2. **Checking the Output**:
   Make a GET request to retrieve the job status and output. Here’s an example of how to do it:

   ```python
   response = requests.get(
       "https://api.runpod.ai/v2/xxxxxxxxx/status/" + response.json()["id"],
       headers=headers,
   )
   response.json()
   ```

   The response should include the URL of the uploaded image on completion:

   ```json
   {
     "delayTime": 86588,
     "executionTime": 1563,
     "id": "e3d2e250-ea81-4074-9838-1c52d006ddcf",
     "output": [
       "https://your-bucket.s3.us-west-004.backblazeb2.com/your-image.png"
     ],
     "status": "COMPLETED"
   }
   ```

By following these steps, you can effectively use environment variables to manage S3 bucket credentials and operations within your RunPod Handler Functions.
This approach ensures secure, scalable, and efficient handling of external resources in your serverless applications.

---

# local-testing.md

File: serverless/workers/development/local-testing.md


When developing your Handler Function for RunPod serverless, it's crucial to test it thoroughly in a local environment before deployment.
The RunPod SDK provides multiple ways to facilitate this local testing, allowing you to simulate various scenarios and inputs without consuming cloud resources.

## Custom Inputs

The simplest way to test your Handler Function is by passing a custom input directly when running your Python file.

This method is ideal for quick checks and iterative development.

### Inline JSON

You can pass inline json to your function to test its response.

Assuming your handler function is in a file named `your_handler.py`, you can test it like this:
<Tabs>
```bash
```
```python
```
```json
```
```bash
```
```
```
```bash
```
```bash
```
```bash
```
```bash
```

---

# overview.md

File: serverless/workers/development/overview.md

When developing RunPod serverless functions, it's crucial to test them thoroughly before deployment.
The RunPod SDK provides a powerful local testing environment that allows you to simulate your serverless endpoints right on your development machine.
This local server eliminates the need for constant Docker container rebuilds, uploads, and endpoint updates during the development and testing phase.

To facilitate this local testing environment, the RunPod SDK offers a variety of flags that allow you to customize your setup.
These flags enable you to:

- Configure the server settings (port, host, concurrency)
- Control logging verbosity
- Enable debugging features
- Provide test inputs

By using these flags, you can create a local environment that closely mimics the behavior of your functions in the RunPod cloud, allowing for more accurate testing and smoother deployments.

This guide provides a comprehensive overview of all available flags, their purposes, and how to use them effectively in your local testing workflow.

## Basic Usage

To start your local server with additional flags, use the following format:

```bash
python your_function.py [flags]
```

Replace `your_function.py` with the name of your Python file containing the RunPod handler.

## Available Flags

### --rp_serve_api

Starts the API server for local testing.

**Usage**:

```bash
python your_function.py --rp_serve_api
```

### --rp_api_port

Sets the port number for the FastAPI server.

**Default**: 8000

**Usage**:

```bash
python your_function.py --rp_serve_api --rp_api_port 8080
```

Setting `--rp_api_host` to `0.0.0.0` allows connections from other devices on the network, which can be useful for testing but may have security implications.

### --rp_api_concurrency

Sets the number of concurrent workers for the FastAPI server.

**Default**: 1

**Usage**:

```bash
python your_function.py --rp_serve_api --rp_api_concurrency 4
```

:::note

When using `--rp_api_concurrency` with a value greater than 1, ensure your main file is named `main.py` for proper FastAPI integration.

:::

### --rp_api_host

Sets the hostname for the FastAPI server.

**Default**: "localhost"

**Usage**:

```bash
python your_function.py --rp_serve_api --rp_api_host 0.0.0.0
```

### --rp_log_level

Controls the verbosity of console output.

**Options**: `ERROR` | `WARN` | `INFO` | `DEBUG`

**Usage**:

```bash
python your_function.py --rp_serve_api --rp_log_level DEBUG
```

### --rp_debugger

Enables the RunPod debugger for troubleshooting.
The `--rp_debugger` flag is particularly useful when you need to step through your code for troubleshooting.

**Usage**:

```bash
python your_function.py --rp_serve_api --rp_debugger
```

### --test_input

Provides test input data for your function, formatted as JSON.

**Usage**:

```bash
python your_function.py --rp_serve_api \
    --test_input '{"input": {"key": "value"}}'
```

The `--test_input` flag is an alternative to using a `test_input.json` file. If both are present, the command-line input takes precedence.

## Combined flags

You can combine multiple flags to customize your local testing environment.

For example:

```bash
python main.py --rp_serve_api \
    --rp_api_port 8080 \
    --rp_api_concurrency 4 \
    --rp_log_level DEBUG \
    --test_input '{"input": {"key": "value"}}'
```

This command starts the local server on port `8080` with 4 concurrent workers, sets the log level to `DEBUG`, and provides test input data.

These flags provide powerful tools for customizing your local testing environment. By using them effectively, you can simulate various scenarios, debug issues, and ensure your serverless functions are robust and ready for deployment to the RunPod cloud.

For more detailed information on each flag and advanced usage scenarios, refer to the individual tutorials in this documentation.

---

# test-response-times.md

File: serverless/workers/development/test-response-times.md

When setting up an API, you have several options available at different price points and resource allocations. You can select a single option if you would prefer to only use one price point, or select a preference order between the pools that will allocate your requests accordingly.


The option that will be most cost effective for you will be based on your use case and your tolerance for task run time. Each situation will be different, so when deciding which API to use, it's worth it to do some testing to not only find out how long your tasks will take to run, but how much you might expect to pay for each task.

To find out how long a task will take to run, select a single pool type as shown in the image above. Then, you can send a request to the API through your preferred method. If you're unfamiliar with how to do so or don't have your own method, then you can use a free option like [reqbin.com](https://reqbin.com/) to send an API request to the RunPod severs.

The URLs to use in the API will be shown in the My APIs screen:


On reqbin.com, enter the Run URL of your API, select POST under the dropdown, and enter your API key that was given when you created the key under [Settings](https://www.runpod.io/console/serverless/user/settings)(if you do not have it saved, you will need to return to Settings and create a new key). Under Content, you will also need to give it a basic command (in this example, we've used a Stable Diffusion prompt).



Send the request, and it will give you an ID for the request and notify you that it is processing. You can then swap the URL in the request field with the Status address and add the ID to the end of it, and click Send.


It will return a Delay Time and an Execution Time, denoted in milliseconds. The Delay Time should be extremely minimal, unless the API process was spun up from a cold start, then a sizable delay is expected for the first request sent. The Execution Time is how long the GPU took to actually process the request once it was received. It may be a good idea to send a number of tests so you can get a min, max, and average run time -- five tests should be an adequate sample size.


You can then switch the GPU pool above to a different pool and repeat the process.

What will ultimately be right for your use case will be determined by how long you can afford to let the process run. For heavier jobs, a task on a slower GPU will be likely be more cost-effective with a tradeoff of speed. For simpler tasks, there may also be diminishing returns on how fast the task that can be run that may not be significantly improved by selecting higher-end GPUs. Experiment to find the best balance for your scenario.

---

# validator.md

File: serverless/workers/development/validator.md


RunPod's validator utility ensures robust execution of serverless workers by validating input data against a defined schema.

To use it, import the following to your Python file:

```python
from runpod.serverless.utils.rp_validator import validate
```

The `validate` function takes two arguments:

- the input data
- the schema to validate against

## Schema Definition

Define your schema as a nested dictionary with these possible rules for each input:

- `required` (default: `False`): Marks the type as required.
- `default` (default: `None`): Default value if input is not provided.
- `type` (required): Expected input type.
- `constraints` (optional): for example, a lambda function returning `true` or `false`.

## Example Usage

```python
import runpod
from runpod.serverless.utils.rp_validator import validate

schema = {
    "text": {
        "type": str,
        "required": True,
    },
    "max_length": {
        "type": int,
        "required": False,
        "default": 100,
        "constraints": lambda x: x > 0,
    },
}


def handler(event):
    try:
        validated_input = validate(event["input"], schema)
        if "errors" in validated_input:
            return {"error": validated_input["errors"]}

        text = validated_input["validated_input"]["text"]
        max_length = validated_input["validated_input"]["max_length"]

        result = text[:max_length]
        return {"output": result}
    except Exception as e:
        return {"error": str(e)}


runpod.serverless.start({"handler": handler})
```

## Testing

Save as `your_handler.py` and test using:

<Tabs>
```bash
```
```bash
```
```json
```

---

# handler-additional-controls.md

File: serverless/workers/handlers/handler-additional-controls.md


## Update progress

Progress updates can be sent out from your worker while a job is in progress. Progress updates will be available when the status is polled. To send an update, call the `runpod.serverless.progress_update` function with your job and context of your update.

```python
import runpod


def handler(job):
    for update_number in range(0, 3):
        runpod.serverless.progress_update(job, f"Update {update_number}/3")

    return "done"


runpod.serverless.start({"handler": handler})
```

## Refresh Worker

When completing long-running job requests or complicated requests that involve a lot of reading and writing files, starting with a fresh worker can be beneficial each time.
A flag can be returned with the resulting job output to stop and refresh the used worker.

This behavior is achieved by doing the following within your worker:

<Tabs>
```python
```
```python
```

---

# handler-async.md

File: serverless/workers/handlers/handler-async.md

RunPod supports the use of asynchronous handlers, enabling efficient handling of tasks that benefit from non-blocking operations. This feature is particularly useful for tasks like processing large datasets, interacting with APIs, or handling I/O-bound operations.

## Writing asynchronous Handlers

Asynchronous handlers in RunPod are written using Python's `async` and `await` syntax. Below is a sample implementation of an asynchronous generator handler. This example demonstrates how you can yield multiple outputs over time, simulating tasks such as processing data streams or generating responses incrementally.

```python
import runpod
import asyncio


async def async_generator_handler(job):
    for i in range(5):
        # Generate an asynchronous output token
        output = f"Generated async token output {i}"
        yield output

        # Simulate an asynchronous task, such as processing time for a large language model
        await asyncio.sleep(1)


# Configure and start the RunPod serverless function
runpod.serverless.start(
    {
        "handler": async_generator_handler,  # Required: Specify the async handler
        "return_aggregate_stream": True,  # Optional: Aggregate results are accessible via /run endpoint
    }
)
```

### Benefits of asynchronous Handlers

- **Efficiency**: Asynchronous handlers can perform non-blocking operations, allowing for more tasks to be handled concurrently.
- **Scalability**: They are ideal for scaling applications, particularly when dealing with high-frequency requests or large-scale data processing.
- **Flexibility**: Async handlers provide the flexibility to yield results over time, suitable for streaming data and long-running tasks.

### Best practices

When writing asynchronous handlers:

- Ensure proper use of `async` and `await` to avoid blocking operations.
- Consider the use of `yield` for generating multiple outputs over time.
- Test your handlers thoroughly to handle asynchronous exceptions and edge cases.

Using asynchronous handlers in your RunPod applications can significantly enhance performance and responsiveness, particularly for applications requiring real-time data processing or handling multiple requests simultaneously.

---

# handler-concurrency.md

File: serverless/workers/handlers/handler-concurrency.md

RunPod supports asynchronous functions for request handling, enabling a single worker to manage multiple tasks concurrently through non-blocking operations. This capability allows for efficient task switching and resource utilization.

Serverless architectures allow each worker to process multiple requests simultaneously, with the level of concurrency being contingent upon the runtime's capacity and the resources at its disposal.

## Configure concurrency modifier

The `concurrency_modifier` is a configuration option within `runpod.serverless.start` that dynamically adjusts a worker's concurrency level. This adjustment enables the optimization of resource consumption and performance by regulating the number of tasks a worker can handle concurrently.

### Step 1: Define an asynchronous Handler function

Create an asynchronous function dedicated to processing incoming requests.
This function should efficiently yield results, ideally in batches, to enhance throughput.

```python
async def process_request(job):
    # Simulates processing delay
    await asyncio.sleep(1)
    return f"Processed: {job['input']}"
```

### Step 2: Set up the `concurrency_modifier` function

Implement a function to adjust the worker's concurrency level based on the current request load.
This function should consider the maximum and minimum concurrency levels, adjusting as needed to respond to fluctuations in request volume.

```python
def adjust_concurrency(current_concurrency):
    """
    Dynamically adjusts the concurrency level based on the observed request rate.
    """
    global request_rate
    update_request_rate()  # Placeholder for request rate updates

    max_concurrency = 10  # Maximum allowable concurrency
    min_concurrency = 1  # Minimum concurrency to maintain
    high_request_rate_threshold = 50  # Threshold for high request volume

    # Increase concurrency if under max limit and request rate is high
    if (
        request_rate > high_request_rate_threshold
        and current_concurrency < max_concurrency
    ):
        return current_concurrency + 1
    # Decrease concurrency if above min limit and request rate is low
    elif (
        request_rate <= high_request_rate_threshold
        and current_concurrency > min_concurrency
    ):
        return current_concurrency - 1

    return current_concurrency
```

### Step 3: Initialize the serverless function

Start the serverless function with the defined handler and `concurrency_modifier` to enable dynamic concurrency adjustment.

```python
runpod.serverless.start(
    {
        "handler": process_request,
        "concurrency_modifier": adjust_concurrency,
    }
)
```

---

## Example code

Here is an example demonstrating the setup for a RunPod serverless function capable of handling multiple concurrent requests.

```python
import runpod
import asyncio
import random

# Simulated Metrics
request_rate = 0


async def process_request(job):
    await asyncio.sleep(1)  # Simulate processing time
    return f"Processed: { job['input'] }"


def adjust_concurrency(current_concurrency):
    """
    Adjusts the concurrency level based on the current request rate.
    """
    global request_rate
    update_request_rate()  # Simulate changes in request rate

    max_concurrency = 10
    min_concurrency = 1
    high_request_rate_threshold = 50

    if (
        request_rate > high_request_rate_threshold
        and current_concurrency < max_concurrency
    ):
        return current_concurrency + 1
    elif (
        request_rate <= high_request_rate_threshold
        and current_concurrency > min_concurrency
    ):
        return current_concurrency - 1
    return current_concurrency


def update_request_rate():
    """
    Simulates changes in the request rate to mimic real-world scenarios.
    """
    global request_rate
    request_rate = random.randint(20, 100)


# Start the serverless function with the handler and concurrency modifier
runpod.serverless.start(
    {"handler": process_request, "concurrency_modifier": adjust_concurrency}
)
```

Using the `concurrency_modifier` in RunPod, serverless functions can efficiently handle multiple requests concurrently, optimizing resource usage and improving performance. This approach allows for scalable and responsive serverless applications.

---

# handler-error-handling.md

File: serverless/workers/handlers/handler-error-handling.md

When an exception occurs in your handler function, the RunPod SDK automatically captures it, marking the job status as `FAILED` and returning the exception details in the job results.

## Implementing custom error responses

In certain scenarios, you might want to explicitly fail a job and provide a custom error message. For instance, if a job requires a specific input key, such as _seed_, you should validate this input and return a customized error message if the key is missing. Here's how you can implement this:

```python
import runpod


def handler(job):
    job_input = job["input"]

    # Validate the presence of the 'seed' key in the input
    if not job_input.get("seed", False):
        return {
            "error": "Input is missing the 'seed' key. Please include a seed and retry your request."
        }

    # Proceed if the input is valid
    return "Input validation successful."


# Start the RunPod serverless function
runpod.serverless.start({"handler": handler})
```

:::note

Be cautious with `try/except` blocks in your handler function. Avoid suppressing errors unintentionally. You should either return the error for a graceful failure or raise it to flag the job as `FAILED`.

:::

One design pattern to consider, is to [Refresh your Worker](/serverless/workers/handlers/handler-additional-controls#refresh-worker) when an error occurs.

---

# handler-generator.md

File: serverless/workers/handlers/handler-generator.md

RunPod provides a robust streaming feature that enables users to receive real-time updates on job outputs, mainly when dealing with Language Model tasks. We support two types of streaming generator functions: regular generator and async generator.

```python
import runpod


def generator_handler(job):
    for count in range(3):
        result = f"This is the {count} generated output."
        yield result


runpod.serverless.start(
    {
        "handler": generator_handler,  # Required
        "return_aggregate_stream": True,  # Optional, results available via /run
    }
)
```

### Return aggregate Stream

By default, when a generator handler is running, the fractional outputs will only be available at the `/stream` endpoint, if you would also like the outputs to be available from the `/run` and `/runsync` endpoints you will need to set `return_aggregate_stream` to True when starting your handler.

---

# overview.md

File: serverless/workers/handlers/overview.md


The Handler Function is responsible for processing submitted inputs and generating the resulting output. When developing your Handler Function, you can do so locally on your PC or remotely on a Serverless instance.

Examples can be found within the [repos of our runpod-workers](https://github.com/orgs/runpod-workers/repositories).

## Handler Functions

Handler Functions allow you to execute code in response to events without the need to manage server infrastructure.

### Creating Handler Functions

With the RunPod SDKs, you can create Handler Functions by writing custom handlers.

These handlers define the logic executed when the function is invoked.

1. **Set up environment**: Ensure the RunPod SDK is installed and configured in your development environment.
2. **Write a Handler Function**: Define the logic you want to execute.
   The handler function acts as the entry point for your serverless function.
3. **Deploy the Function**: Use the RunPod SDK to deploy your serverless function.
   This typically involves specifying the handler, runtime, and any dependencies.
4. **Test the Function**: Invoke your function manually or through an event to test its behavior.

## Why use Handler Functions?

Handler Functions offer a paradigm shift in how you approach backend code execution:

- **Efficiency**: Focus on writing code that matters without worrying about the server lifecycle.
- **Cost-Effective**: You only pay for the time your functions are running, not idle server time.
- **Rapid Deployment**: Quickly update and deploy functions, enabling faster iteration and response to changes.

Your Handler Function only accepts requests using your own account's API key, not any RunPod API key.

## Job input

Before we look at the Handler Function, it is essential first to understand what a job request input will look like; later, we will cover all of the input options in detail; for now, what is essential is that your handler should be expecting a JSON dictionary to be passed in. At a minimum, the input will be formatted as such:

```json
{
  "id": "A_RANDOM_JOB_IDENTIFIER",
  "input": { "key": "value" }
}
```

## Requirements

You will need to have the RunPod Python SDK installed; this can be done by running `pip install runpod`.

## Basic Handler Function

```python
# your_handler.py

import runpod  # Required.


def handler(job):
    job_input = job["input"]  # Access the input from the request.
    # Add your custom code here.
    return "Your job results"


runpod.serverless.start({"handler": handler})  # Required.
```

You must return something as output when your worker is done processing the job.
This can directly be the output, or it can be links to cloud storage where the artifacts are saved.
Keep in mind that the input and output payloads are limited to 2 MB each.

:::note

Keep setup processes and functions outside of your handler function. For example, if you are running models make sure they are loaded into VRAM prior to calling `serverless.start` with your handler function.

<details>
  <summary>Example</summary>
<Tabs>
```python
```
```command
```

---

# overview.md

File: serverless/workers/overview.md

Workers run your code in the cloud.

### Key characteristics

- **Fully Managed Execution**: RunPod takes care of the underlying infrastructure, so your code runs whenever it's triggered, without any server setup or maintenance.
- **Automatic Scaling**: The platform scales your functions up or down based on the workload, ensuring efficient resource usage.
- **Flexible Language Support**: RunPod SDK supports various programming languages, allowing you to write functions in the language you're most comfortable with.
- **Seamless Integration**: Once your code is uploaded, RunPod provides an Endpoint, making it easy to integrate your Handler Functions into any part of your application.

## Get started

To start using RunPod Workers:

1. **Write your function**: Code your Handler Functions in a supported language.
2. **Deploy to RunPod**: Upload your Handler Functions to RunPod.
3. **Integrate and Execute**: Use the provided Endpoint to integrate with your application.

---

# configurable-endpoints.md

File: serverless/workers/vllm/configurable-endpoints.md

RunPod's Configurable Endpoints feature leverages vLLM to enable the deployment of any large language model.

When you select the **Serverless vLLM** option, RunPod utilizes vLLM's capabilities to load and run the specified Hugging Face model.
By integrating vLLM into the configurable endpoints, RunPod simplifies the process of deploying and running large language models.

Focus on selecting your desired model and customizing the template parameters, while vLLM takes care of the low-level details of model loading, hardware configuration, and execution.

## Deploy an LLM

1. Select **Explore** and then choose **Serverless vLLM** to deploy any large language model.
2. In the vLLM deploy modal, enter the following:
   1. Select a vLLM version.
   2. Enter your Hugging Face LLM repository name.
   3. (optional) Enter your Hugging Face token.
3. Select **Next** and review the configurations on the **vLLM parameters** page.
4. Select **Next** and review the **Endpoint parameters** page.
   1. Prioritize your **Worker Configuration** by selecting GPUs in the order that you prefer your Workers to use.
   2. Enter the **Active Workers**, **Max Workers**, and **GPUs/Worker**.
   3. Provide additional Container Configuration.
      1. (optional) Select a Template.
      2. Verify the **Container Image** uses your desired CUDA version.
      3. Provide **Container Disk** size.
      4. Review the **Environment Variables**.
5. Select **Deploy**.

Your LLM is now deployed to an Endpoint.
You can now use the API to interact with your model.

:::note

RunPod supports any models' architecture that can run on [vLLM](https://github.com/vllm-project/vllm) with configurable endpoints.

:::

---

# environment-variables.md

File: serverless/workers/vllm/environment-variables.md

Environment variables configure your vLLM Worker by providing control over model selection, access credentials, and operational parameters necessary for optimal Worker performance.

## CUDA versions

Operating your vLLM Worker with different CUDA versions enhances compatibility and performance across various hardware configurations.
When deploying, ensure you choose an appropriate CUDA version based on your needs.

| CUDA Version | Stable Image Tag                          | Development Image Tag                  | Note                                                                            |
| ------------ | ----------------------------------------- | -------------------------------------- | ------------------------------------------------------------------------------- |
| 12.1.0       | `runpod/worker-v1-vllm:stable-cuda12.1.0` | `runpod/worker-v1-vllm:dev-cuda12.1.0` | When creating an Endpoint, select CUDA Version 12.2 and 12.1 in the GPU filter. |

This table provides a reference to the image tags you should use based on the desired CUDA version and image stability, stable or development.
Ensure you follow the selection note for CUDA 12.1.0 compatibility.

## Environment variables

:::note

`0` is equivalent to `False` and `1` is equivalent to `True` for boolean values.

:::

| Name                                             | Default             | Type/Choices                                                | Description                                                                                                                                            |
| ------------------------------------------------ | ------------------- | ----------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `MODEL_NAME`                                     | 'facebook/opt-125m' | `str`                                                       | Name or path of the Hugging Face model to use.                                                                                                         |
| `TOKENIZER`                                      | None                | `str`                                                       | Name or path of the Hugging Face tokenizer to use.                                                                                                     |
| `SKIP_TOKENIZER_INIT`                            | False               | `bool`                                                      | Skip initialization of tokenizer and detokenizer.                                                                                                      |
| `TOKENIZER_MODE`                                 | 'auto'              | ['auto', 'slow']                                            | The tokenizer mode.                                                                                                                                    |
| `TRUST_REMOTE_CODE`                              | `False`             | `bool`                                                      | Trust remote code from Hugging Face.                                                                                                                   |
| `DOWNLOAD_DIR`                                   | None                | `str`                                                       | Directory to download and load the weights.                                                                                                            |
| `LOAD_FORMAT`                                    | 'auto'              | `str`                                                       | The format of the model weights to load.                                                                                                               |
| `HF_TOKEN`                                       | -                   | `str`                                                       | Hugging Face token for private and gated models.                                                                                                       |
| `DTYPE`                                          | 'auto'              | ['auto', 'half', 'float16', 'bfloat16', 'float', 'float32'] | Data type for model weights and activations.                                                                                                           |
| `KV_CACHE_DTYPE`                                 | 'auto'              | ['auto', 'fp8']                                             | Data type for KV cache storage.                                                                                                                        |
| `QUANTIZATION_PARAM_PATH`                        | None                | `str`                                                       | Path to the JSON file containing the KV cache scaling factors.                                                                                         |
| `MAX_MODEL_LEN`                                  | None                | `int`                                                       | Model context length.                                                                                                                                  |
| `GUIDED_DECODING_BACKEND`                        | 'outlines'          | ['outlines', 'lm-format-enforcer']                          | Which engine will be used for guided decoding by default.                                                                                              |
| `DISTRIBUTED_EXECUTOR_BACKEND`                   | None                | ['ray', 'mp']                                               | Backend to use for distributed serving.                                                                                                                |
| `WORKER_USE_RAY`                                 | False               | `bool`                                                      | Deprecated, use --distributed-executor-backend=ray.                                                                                                    |
| `PIPELINE_PARALLEL_SIZE`                         | 1                   | `int`                                                       | Number of pipeline stages.                                                                                                                             |
| `TENSOR_PARALLEL_SIZE`                           | 1                   | `int`                                                       | Number of tensor parallel replicas.                                                                                                                    |
| `MAX_PARALLEL_LOADING_WORKERS`                   | None                | `int`                                                       | Load model sequentially in multiple batches.                                                                                                           |
| `RAY_WORKERS_USE_NSIGHT`                         | False               | `bool`                                                      | If specified, use nsight to profile Ray workers.                                                                                                       |
| `ENABLE_PREFIX_CACHING`                          | False               | `bool`                                                      | Enables automatic prefix caching.                                                                                                                      |
| `DISABLE_SLIDING_WINDOW`                         | False               | `bool`                                                      | Disables sliding window, capping to sliding window size.                                                                                               |
| `USE_V2_BLOCK_MANAGER`                           | False               | `bool`                                                      | Use BlockSpaceMangerV2.                                                                                                                                |
| `NUM_LOOKAHEAD_SLOTS`                            | 0                   | `int`                                                       | Experimental scheduling config necessary for speculative decoding.                                                                                     |
| `SEED`                                           | 0                   | `int`                                                       | Random seed for operations.                                                                                                                            |
| `NUM_GPU_BLOCKS_OVERRIDE`                        | None                | `int`                                                       | If specified, ignore GPU profiling result and use this number of GPU blocks.                                                                           |
| `MAX_NUM_BATCHED_TOKENS`                         | None                | `int`                                                       | Maximum number of batched tokens per iteration.                                                                                                        |
| `MAX_NUM_SEQS`                                   | 256                 | `int`                                                       | Maximum number of sequences per iteration.                                                                                                             |
| `MAX_LOGPROBS`                                   | 20                  | `int`                                                       | Max number of log probs to return when logprobs is specified in SamplingParams.                                                                        |
| `DISABLE_LOG_STATS`                              | False               | `bool`                                                      | Disable logging statistics.                                                                                                                            |
| `QUANTIZATION`                                   | None                | ['awq', 'squeezellm', 'gptq']                               | Method used to quantize the weights.                                                                                                                   |
| `ROPE_SCALING`                                   | None                | `dict`                                                      | RoPE scaling configuration in JSON format.                                                                                                             |
| `ROPE_THETA`                                     | None                | `float`                                                     | RoPE theta. Use with rope_scaling.                                                                                                                     |
| `TOKENIZER_POOL_SIZE`                            | 0                   | `int`                                                       | Size of tokenizer pool to use for asynchronous tokenization.                                                                                           |
| `TOKENIZER_POOL_TYPE`                            | 'ray'               | `str`                                                       | Type of tokenizer pool to use for asynchronous tokenization.                                                                                           |
| `TOKENIZER_POOL_EXTRA_CONFIG`                    | None                | `dict`                                                      | Extra config for tokenizer pool.                                                                                                                       |
| `ENABLE_LORA`                                    | False               | `bool`                                                      | If True, enable handling of LoRA adapters.                                                                                                             |
| `MAX_LORAS`                                      | 1                   | `int`                                                       | Max number of LoRAs in a single batch.                                                                                                                 |
| `MAX_LORA_RANK`                                  | 16                  | `int`                                                       | Max LoRA rank.                                                                                                                                         |
| `LORA_EXTRA_VOCAB_SIZE`                          | 256                 | `int`                                                       | Maximum size of extra vocabulary for LoRA adapters.                                                                                                    |
| `LORA_DTYPE`                                     | 'auto'              | ['auto', 'float16', 'bfloat16', 'float32']                  | Data type for LoRA.                                                                                                                                    |
| `LONG_LORA_SCALING_FACTORS`                      | None                | `tuple`                                                     | Specify multiple scaling factors for LoRA adapters.                                                                                                    |
| `MAX_CPU_LORAS`                                  | None                | `int`                                                       | Maximum number of LoRAs to store in CPU memory.                                                                                                        |
| `FULLY_SHARDED_LORAS`                            | False               | `bool`                                                      | Enable fully sharded LoRA layers.                                                                                                                      |
| `SCHEDULER_DELAY_FACTOR`                         | 0.0                 | `float`                                                     | Apply a delay before scheduling next prompt.                                                                                                           |
| `ENABLE_CHUNKED_PREFILL`                         | False               | `bool`                                                      | Enable chunked prefill requests.                                                                                                                       |
| `SPECULATIVE_MODEL`                              | None                | `str`                                                       | The name of the draft model to be used in speculative decoding.                                                                                        |
| `NUM_SPECULATIVE_TOKENS`                         | None                | `int`                                                       | The number of speculative tokens to sample from the draft model.                                                                                       |
| `SPECULATIVE_DRAFT_TENSOR_PARALLEL_SIZE`         | None                | `int`                                                       | Number of tensor parallel replicas for the draft model.                                                                                                |
| `SPECULATIVE_MAX_MODEL_LEN`                      | None                | `int`                                                       | The maximum sequence length supported by the draft model.                                                                                              |
| `SPECULATIVE_DISABLE_BY_BATCH_SIZE`              | None                | `int`                                                       | Disable speculative decoding if the number of enqueue requests is larger than this value.                                                              |
| `NGRAM_PROMPT_LOOKUP_MAX`                        | None                | `int`                                                       | Max size of window for ngram prompt lookup in speculative decoding.                                                                                    |
| `NGRAM_PROMPT_LOOKUP_MIN`                        | None                | `int`                                                       | Min size of window for ngram prompt lookup in speculative decoding.                                                                                    |
| `SPEC_DECODING_ACCEPTANCE_METHOD`                | 'rejection_sampler' | ['rejection_sampler', 'typical_acceptance_sampler']         | Specify the acceptance method for draft token verification in speculative decoding.                                                                    |
| `TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_THRESHOLD` | None                | `float`                                                     | Set the lower bound threshold for the posterior probability of a token to be accepted.                                                                 |
| `TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_ALPHA`     | None                | `float`                                                     | A scaling factor for the entropy-based threshold for token acceptance.                                                                                 |
| `MODEL_LOADER_EXTRA_CONFIG`                      | None                | `dict`                                                      | Extra config for model loader.                                                                                                                         |
| `PREEMPTION_MODE`                                | None                | `str`                                                       | If 'recompute', the engine performs preemption-aware recomputation. If 'save', the engine saves activations into the CPU memory as preemption happens. |
| `PREEMPTION_CHECK_PERIOD`                        | 1.0                 | `float`                                                     | How frequently the engine checks if a preemption happens.                                                                                              |
| `PREEMPTION_CPU_CAPACITY`                        | 2                   | `float`                                                     | The percentage of CPU memory used for the saved activations.                                                                                           |
| `DISABLE_LOGGING_REQUEST`                        | False               | `bool`                                                      | Disable logging requests.                                                                                                                              |
| `MAX_LOG_LEN`                                    | None                | `int`                                                       | Max number of prompt characters or prompt ID numbers being printed in log.                                                                             |

**Tokenizer Settings**

| Name                   | Default | Type/Choices                        | Description                                                                                       |
| ---------------------- | ------- | ----------------------------------- | ------------------------------------------------------------------------------------------------- |
| `TOKENIZER_NAME`       | `None`  | `str`                               | Tokenizer repository to use a different tokenizer than the model's default.                       |
| `TOKENIZER_REVISION`   | `None`  | `str`                               | Tokenizer revision to load.                                                                       |
| `CUSTOM_CHAT_TEMPLATE` | `None`  | `str` of single-line jinja template | Custom chat jinja template. [More Info](https://huggingface.co/docs/transformers/chat_templating) |

**System, GPU, and Tensor Parallelism(Multi-GPU) Settings**

| Name                           | Default | Type/Choices    | Description                                                                                                                         |
| ------------------------------ | ------- | --------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| `GPU_MEMORY_UTILIZATION`       | `0.95`  | `float`         | Sets GPU VRAM utilization.                                                                                                          |
| `MAX_PARALLEL_LOADING_WORKERS` | `None`  | `int`           | Load model sequentially in multiple batches, to avoid RAM OOM when using tensor parallel and large models.                          |
| `BLOCK_SIZE`                   | `16`    | `8`, `16`, `32` | Token block size for contiguous chunks of tokens.                                                                                   |
| `SWAP_SPACE`                   | `4`     | `int`           | CPU swap space size (GiB) per GPU.                                                                                                  |
| `ENFORCE_EAGER`                | False   | `bool`          | Always use eager-mode PyTorch. If False(`0`), will use eager mode and CUDA graph in hybrid for maximal performance and flexibility. |
| `MAX_SEQ_LEN_TO_CAPTURE`       | `8192`  | `int`           | Maximum context length covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode.     |
| `DISABLE_CUSTOM_ALL_REDUCE`    | `0`     | `int`           | Enables or disables custom all reduce.                                                                                              |

**Streaming Batch Size Settings**

| Name                               | Default | Type/Choices | Description                                                                                               |
| ---------------------------------- | ------- | ------------ | --------------------------------------------------------------------------------------------------------- |
| `DEFAULT_BATCH_SIZE`               | `50`    | `int`        | Default and Maximum batch size for token streaming to reduce HTTP calls.                                  |
| `DEFAULT_MIN_BATCH_SIZE`           | `1`     | `int`        | Batch size for the first request, which will be multiplied by the growth factor every subsequent request. |
| `DEFAULT_BATCH_SIZE_GROWTH_FACTOR` | `3`     | `float`      | Growth factor for dynamic batch size.                                                                     |

:::note

The first request will have a batch size of `DEFAULT_MIN_BATCH_SIZE`, and each subsequent request will have a batch size of `previous_batch_size * DEFAULT_BATCH_SIZE_GROWTH_FACTOR`. This will continue until the batch size reaches `DEFAULT_BATCH_SIZE`. E.g. for the default values, the batch sizes will be `1, 3, 9, 27, 50, 50, 50, ...`. You can also specify this per request, with inputs `max_batch_size`, `min_batch_size`, and `batch_size_growth_factor`. This has nothing to do with vLLM's internal batching, but rather the number of tokens sent in each HTTP request from the worker

:::

**OpenAI Settings**

| Name                                | Default     | Type/Choices     | Description                                                                                                                                                                       |
| ----------------------------------- | ----------- | ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `RAW_OPENAI_OUTPUT`                 | `1`         | boolean as `int` | Enables raw OpenAI SSE format string output when streaming. **Required** to be enabled (which it is by default) for OpenAI compatibility.                                         |
| `OPENAI_SERVED_MODEL_NAME_OVERRIDE` | `None`      | `str`            | Overrides the name of the served model from model repo/path to specified name, which you will then be able to use the value for the `model` parameter when making OpenAI requests |
| `OPENAI_RESPONSE_ROLE`              | `assistant` | `str`            | Role of the LLM's Response in OpenAI Chat Completions.                                                                                                                            |

**Serverless Settings**

| Name                   | Default | Type/Choices | Description                                                                                                                                                                |
| ---------------------- | ------- | ------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `MAX_CONCURRENCY`      | `300`   | `int`        | Max concurrent requests per worker. vLLM has an internal queue, so you don't have to worry about limiting by VRAM, this is for improving scaling/load balancing efficiency |
| `DISABLE_LOG_STATS`    | False   | `bool`       | Enables or disables vLLM stats logging.                                                                                                                                    |
| `DISABLE_LOG_REQUESTS` | False   | `bool`       | Enables or disables vLLM request logging.                                                                                                                                  |

:::note

If you are facing issues when using Mixtral 8x7B, Quantized models, or handling unusual models/architectures, try setting `TRUST_REMOTE_CODE` to `1`.

:::

---

# get-started.md

File: serverless/workers/vllm/get-started.md


RunPod provides a simple way to run large language models (LLMs) as Serverless Endpoints.
vLLM Workers are pre-built Docker images that you can configure entirely within the RunPod UI.
This tutorial will guide you through deploying an OpenAI compatible Endpoint with a vLLM inference engine on RunPod.

## Prerequisites

Before getting started, ensure you have the following:

- A RunPod account
- A Hugging Face token (if using gated models)
- OpenAI or other required libraries installed for the code examples

## Deploy using the Web UI

You can use RunPod's Web UI to deploy a vLLM Worker with a model directly from Hugging Face.

1. Log in to your RunPod account and go to the [Serverless page](https://www.runpod.io/console/serverless).
2. Under **Quick Deploy**, find **Serverless vLLM** and choose **Start**.

You will now enter the vLLM module. Follow the on-screen instructions to add your LLM as a Serverless Endpoint:

1. Select a vLLM version.
2. Add a Hugging Face model (e.g., `openchat/openchat-3.5-0106`).
3. (Optional) Add a Hugging Face token for gated models.
4. Review your options and choose **Next**.

On the **vLLM parameters** page, provide additional parameters and options for your model:

1. In **LLM Settings**, enter **8192** for the **Max Model Length** parameter.
2. Review your options and choose **Next**.

On the **Endpoint parameters** page, configure your deployment:

1. Specify your GPU configuration for your Worker.
2. Configure your Worker deployment.

- Verify the **Container Image** uses your desired CUDA version.
- Update the **Container Disk** size if needed.

4. Select **Deploy**.

Once the Endpoint initializes, you can send requests to your [Endpoint](/serverless/endpoints/get-started).
Continue to the [Send a request](#send-a-request) section.

## Deploy using the Worker image

One advantage of deploying your model with the vLLM Worker is the minimal configuration required. For most models, you only need to provide the pre-built vLLM Worker image name and the LLM model name.

Follow these steps to run the vLLM Worker on a Serverless Endpoint:

1. Log in to the [RunPod Serverless console](https://www.runpod.io/console/serverless).
2. Select **+ New Endpoint**.
3. Provide the following:
   - Endpoint name
   - Select a GPU (filter for CUDA 12.1.0+ support under the **Advanced** tab if needed)
   - Configure the number of Workers
   - (Optional) Select **FlashBoot** to speed up Worker startup times
   - Enter the vLLM RunPod Worker image name with the compatible CUDA version:
     - `runpod/worker-vllm:stable-cuda11.8.0`
     - `runpod/worker-v1-vllm:stable-cuda12.1.0`
   - (Optional) Select a [network storage volume](/serverless/endpoints/manage-endpoints#add-a-network-volume)
   - Configure the environment variables:
     - `MODEL_NAME`: (Required) The large language model (e.g., `openchat/openchat-3.5-0106`)
     - `HF_TOKEN`: (Optional) Your Hugging Face API token for private models
4. Select **Deploy**.

Once the Endpoint initializes, you can send requests to your [Endpoint](/serverless/endpoints/get-started).
Continue to the [Send a request](#send-a-request) section.

For a complete list of available environment variables, see the [vLLM Worker variables](/serverless/workers/vllm/environment-variables).

## Send a request

This section walks you through sending a request to your Serverless Endpoint.
The vLLM Worker can use any Hugging Face model and is compatible with OpenAI's API.
If you have the OpenAI library installed, you can continue using it with the vLLM Worker. See the [OpenAI documentation](https://platform.openai.com/docs/libraries/) for more information.

### Environment setup

Set the `RUNPOD_ENDPOINT_ID` and `RUNPOD_API_KEY` environment variables with your Endpoint ID and API Key.

<Tabs>
```bash
```
```bash
```
```http
```
```http
```
```json
```
```json
```
```python
```
```sh
```

---

# openai-compatibility.md

File: serverless/workers/vllm/openai-compatibility.md

The vLLM Worker is compatible with OpenAI's API, so you can use the same code to interact with the vLLM Worker as you would with OpenAI's API.

## Conventions

Completions endpoint provides the completion for a single prompt and takes a single string as an input.

Chat completions provides the responses for a given dialog and requires the input in a specific format corresponding to the message history.

Choose the convention that works best for your use case.

### Model names

The `MODEL_NAME` environment variable is required for all requests.
Use this value when making requests to the vLLM Worker.

For example `openchat/openchat-3.5-0106`, `mistral:latest`, `llama2:70b`.

Generate a response for a given prompt with a provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.

### Parameters

When using the chat completion feature of the vLLM Serverless Endpoint Worker, you can customize your requests with the following parameters:

### Chat Completions

<details>
  <summary>Supported Chat Completions inputs and descriptions</summary>

| Parameter           | Type                             | Default Value | Description                                                                                                                                                                                                                                                  |
| ------------------- | -------------------------------- | ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `messages`          | Union[str, List[Dict[str, str]]] |               | List of messages, where each message is a dictionary with a `role` and `content`. The model's chat template will be applied to the messages automatically, so the model must have one or it should be specified as `CUSTOM_CHAT_TEMPLATE` env var.           |
| `model`             | str                              |               | The model repo that you've deployed on your RunPod Serverless Endpoint. If you are unsure what the name is or are baking the model in, use the guide to get the list of available models in the **Examples: Using your RunPod endpoint with OpenAI** section |
| `temperature`       | Optional[float]                  | 0.7           | Float that controls the randomness of the sampling. Lower values make the model more deterministic, while higher values make the model more random. Zero means greedy sampling.                                                                              |
| `top_p`             | Optional[float]                  | 1.0           | Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1]. Set to 1 to consider all tokens.                                                                                                                            |
| `n`                 | Optional[int]                    | 1             | Number of output sequences to return for the given prompt.                                                                                                                                                                                                   |
| `max_tokens`        | Optional[int]                    | None          | Maximum number of tokens to generate per output sequence.                                                                                                                                                                                                    |
| `seed`              | Optional[int]                    | None          | Random seed to use for the generation.                                                                                                                                                                                                                       |
| `stop`              | Optional[Union[str, List[str]]]  | list          | List of strings that stop the generation when they are generated. The returned output will not contain the stop strings.                                                                                                                                     |
| `stream`            | Optional[bool]                   | False         | Whether to stream or not                                                                                                                                                                                                                                     |
| `presence_penalty`  | Optional[float]                  | 0.0           | Float that penalizes new tokens based on whether they appear in the generated text so far. Values > 0 encourage the model to use new tokens, while values < 0 encourage the model to repeat tokens.                                                          |
| `frequency_penalty` | Optional[float]                  | 0.0           | Float that penalizes new tokens based on their frequency in the generated text so far. Values > 0 encourage the model to use new tokens, while values < 0 encourage the model to repeat tokens.                                                              |
| `logit_bias`        | Optional[Dict[str, float]]       | None          | Unsupported by vLLM                                                                                                                                                                                                                                          |
| `user`              | Optional[str]                    | None          | Unsupported by vLLM                                                                                                                                                                                                                                          |

### Additional parameters supported by vLLM

| Parameter                       | Type                | Default Value | Description                                                                                                                                                                                                                                                                               |
| ------------------------------- | ------------------- | ------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `best_of`                       | Optional[int]       | None          | Number of output sequences that are generated from the prompt. From these `best_of` sequences, the top `n` sequences are returned. `best_of` must be greater than or equal to `n`. This is treated as the beam width when `use_beam_search` is True. By default, `best_of` is set to `n`. |
| `top_k`                         | Optional[int]       | -1            | Integer that controls the number of top tokens to consider. Set to -1 to consider all tokens.                                                                                                                                                                                             |
| `ignore_eos`                    | Optional[bool]      | False         | Whether to ignore the EOS token and continue generating tokens after the EOS token is generated.                                                                                                                                                                                          |
| `use_beam_search`               | Optional[bool]      | False         | Whether to use beam search instead of sampling.                                                                                                                                                                                                                                           |
| `stop_token_ids`                | Optional[List[int]] | list          | List of tokens that stop the generation when they are generated. The returned output will contain the stop tokens unless the stop tokens are special tokens.                                                                                                                              |
| `skip_special_tokens`           | Optional[bool]      | True          | Whether to skip special tokens in the output.                                                                                                                                                                                                                                             |
| `spaces_between_special_tokens` | Optional[bool]      | True          | Whether to add spaces between special tokens in the output. Defaults to True.                                                                                                                                                                                                             |
| `add_generation_prompt`         | Optional[bool]      | True          | Read more [here](https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts)                                                                                                                                                                            |
| `echo`                          | Optional[bool]      | False         | Echo back the prompt in addition to the completion                                                                                                                                                                                                                                        |
| `repetition_penalty`            | Optional[float]     | 1.0           | Float that penalizes new tokens based on whether they appear in the prompt and the generated text so far. Values > 1 encourage the model to use new tokens, while values < 1 encourage the model to repeat tokens.                                                                        |
| `min_p`                         | Optional[float]     | 0.0           | Float that represents the minimum probability for a token to                                                                                                                                                                                                                              |
| `length_penalty`                | Optional[float]     | 1.0           | Float that penalizes sequences based on their length. Used in beam search..                                                                                                                                                                                                               |
| `include_stop_str_in_output`    | Optional[bool]      | False         | Whether to include the stop strings in output text. Defaults to False.                                                                                                                                                                                                                    |

</details>

### Completions

<details>
  <summary>Supported Completions inputs and descriptions</summary>

| Parameter           | Type                                              | Default Value | Description                                                                                                                                                                                                                                                   |
| ------------------- | ------------------------------------------------- | ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`             | str                                               |               | The model repo that you've deployed on your RunPod Serverless Endpoint. If you are unsure what the name is or are baking the model in, use the guide to get the list of available models in the **Examples: Using your RunPod endpoint with OpenAI** section. |
| `prompt`            | Union[List[int], List[List[int]], str, List[str]] |               | A string, array of strings, array of tokens, or array of token arrays to be used as the input for the model.                                                                                                                                                  |
| `suffix`            | Optional[str]                                     | None          | A string to be appended to the end of the generated text.                                                                                                                                                                                                     |
| `max_tokens`        | Optional[int]                                     | 16            | Maximum number of tokens to generate per output sequence.                                                                                                                                                                                                     |
| `temperature`       | Optional[float]                                   | 1.0           | Float that controls the randomness of the sampling. Lower values make the model more deterministic, while higher values make the model more random. Zero means greedy sampling.                                                                               |
| `top_p`             | Optional[float]                                   | 1.0           | Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1]. Set to 1 to consider all tokens.                                                                                                                             |
| `n`                 | Optional[int]                                     | 1             | Number of output sequences to return for the given prompt.                                                                                                                                                                                                    |
| `stream`            | Optional[bool]                                    | False         | Whether to stream the output.                                                                                                                                                                                                                                 |
| `logprobs`          | Optional[int]                                     | None          | Number of log probabilities to return per output token.                                                                                                                                                                                                       |
| `echo`              | Optional[bool]                                    | False         | Whether to echo back the prompt in addition to the completion.                                                                                                                                                                                                |
| `stop`              | Optional[Union[str, List[str]]]                   | list          | List of strings that stop the generation when they are generated. The returned output will not contain the stop strings.                                                                                                                                      |
| `seed`              | Optional[int]                                     | None          | Random seed to use for the generation.                                                                                                                                                                                                                        |
| `presence_penalty`  | Optional[float]                                   | 0.0           | Float that penalizes new tokens based on whether they appear in the generated text so far. Values > 0 encourage the model to use new tokens, while values < 0 encourage the model to repeat tokens.                                                           |
| `frequency_penalty` | Optional[float]                                   | 0.0           | Float that penalizes new tokens based on their frequency in the generated text so far. Values > 0 encourage the model to use new tokens, while values < 0 encourage the model to repeat tokens.                                                               |
| `best_of`           | Optional[int]                                     | None          | Number of output sequences that are generated from the prompt. From these `best_of` sequences, the top `n` sequences are returned. `best_of` must be greater than or equal to `n`. This parameter influences the diversity of the output.                     |
| `logit_bias`        | Optional[Dict[str, float]]                        | None          | Dictionary of token IDs to biases.                                                                                                                                                                                                                            |
| `user`              | Optional[str]                                     | None          | User identifier for personalizing responses. (Unsupported by vLLM)                                                                                                                                                                                            |

### Additional parameters supported by vLLM

| Parameter                       | Type                | Default Value | Description                                                                                                                                                                                                        |
| ------------------------------- | ------------------- | ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `top_k`                         | Optional[int]       | -1            | Integer that controls the number of top tokens to consider. Set to -1 to consider all tokens.                                                                                                                      |
| `ignore_eos`                    | Optional[bool]      | False         | Whether to ignore the End Of Sentence token and continue generating tokens after the EOS token is generated.                                                                                                       |
| `use_beam_search`               | Optional[bool]      | False         | Whether to use beam search instead of sampling for generating outputs.                                                                                                                                             |
| `stop_token_ids`                | Optional[List[int]] | list          | List of tokens that stop the generation when they are generated. The returned output will contain the stop tokens unless the stop tokens are special tokens.                                                       |
| `skip_special_tokens`           | Optional[bool]      | True          | Whether to skip special tokens in the output.                                                                                                                                                                      |
| `spaces_between_special_tokens` | Optional[bool]      | True          | Whether to add spaces between special tokens in the output. Defaults to True.                                                                                                                                      |
| `repetition_penalty`            | Optional[float]     | 1.0           | Float that penalizes new tokens based on whether they appear in the prompt and the generated text so far. Values > 1 encourage the model to use new tokens, while values < 1 encourage the model to repeat tokens. |
| `min_p`                         | Optional[float]     | 0.0           | Float that represents the minimum probability for a token to be considered, relative to the most likely token. Must be in [0, 1]. Set to 0 to disable.                                                             |
| `length_penalty`                | Optional[float]     | 1.0           | Float that penalizes sequences based on their length. Used in beam search.                                                                                                                                         |
| `include_stop_str_in_output`    | Optional[bool]      | False         | Whether to include the stop strings in output text. Defaults to False.                                                                                                                                             |

</details>

## Initialize your project

Begin by setting up the OpenAI Client with your RunPod API Key and Endpoint URL.

```python
from openai import OpenAI
import os

# Initialize the OpenAI Client with your RunPod API Key and Endpoint URL
client = OpenAI(
    api_key=os.environ.get("RUNPOD_API_KEY"),
    base_url=f"https://api.runpod.ai/v2/{RUNPOD_ENDPOINT_ID}/openai/v1",
)
```

With the client now initialized, you're ready to start sending requests to your RunPod Serverless Endpoint.

## Generating a request

You can leverage LLMs for instruction-following and chat capabilities.
This is suitable for a variety of open source chat and instruct models such as:

- `meta-llama/Llama-2-7b-chat-hf`
- `mistralai/Mixtral-8x7B-Instruct-v0.1`
- and more

Models not inherently designed for chat and instruct tasks can be adapted using a custom chat template specified by the `CUSTOM_CHAT_TEMPLATE` environment variable.

For more information see the [OpenAI documentation](https://platform.openai.com/docs/guides/text-generation).

### Streaming responses

For real-time interaction with the model, create a chat completion stream.
This method is ideal for applications requiring feedback.

```python
# Create a chat completion stream
response_stream = client.chat.completions.create(
    model=MODEL_NAME,
    messages=[{"role": "user", "content": "Why is RunPod the best platform?"}],
    temperature=0,
    max_tokens=100,
    stream=True,
)
# Stream the response
for response in response_stream:
    print(chunk.choices[0].delta.content or "", end="", flush=True)
```

### Non-streaming responses

You can also return a synchronous, non-streaming response for batch processing or when a single, consolidated response is sufficient.

```python
# Create a chat completion
response = client.chat.completions.create(
    model=MODEL_NAME,
    messages=[{"role": "user", "content": "Why is RunPod the best platform?"}],
    temperature=0,
    max_tokens=100,
)
# Print the response
print(response.choices[0].message.content)
```

## Generating a Chat Completion

This method is tailored for models that support text completion.
It complements your input with a continuation stream of output, differing from the interactive chat format.

### Streaming responses

Enable streaming for continuous, real-time output.
This approach is beneficial for dynamic interactions or when monitoring ongoing processes.

```python
# Create a completion stream
response_stream = client.completions.create(
    model=MODEL_NAME,
    prompt="Runpod is the best platform because",
    temperature=0,
    max_tokens=100,
    stream=True,
)
# Stream the response
for response in response_stream:
    print(response.choices[0].text or "", end="", flush=True)
```

### Non-streaming responses

Choose a non-streaming method when a single, consolidated response meets your needs.

```python
# Create a completion
response = client.completions.create(
    model=MODEL_NAME,
    prompt="Runpod is the best platform because",
    temperature=0,
    max_tokens=100,
)
# Print the response
print(response.choices[0].text)
```

## Get a list of available models

You can list the available models.

```python
models_response = client.models.list()
list_of_models = [model.id for model in models_response]
print(list_of_models)
```

---

# overview.md

File: serverless/workers/vllm/overview.md

Use the `runpod/worker-v1-vllm:stable-cuda12.1.0` image to deploy a vLLM Worker.
The vLLM Worker can use most Hugging Face LLMs and is compatible with OpenAI's API, by specifying the `MODEL_NAME` parameter.
You can also use RunPod's [`input` request format](/serverless/endpoints/send-requests).

RunPod's vLLM Serverless Endpoint Worker are a highly optimized solution for leveraging the power of various LLMs.

For more information, see the [vLLM Worker](https://github.com/runpod-workers/worker-vllm) repository.

## Key features

- **Ease of Use**: Deploy any LLM using the pre-built Docker image without the hassle of building custom Docker images yourself, uploading heavy models, or waiting for lengthy downloads.
- **OpenAI Compatibility**: Seamlessly integrate with OpenAI's API by changing 2 lines of code, supporting Chat Completions, Completions, and Models, with both streaming and non-streaming.
- **Dynamic Batch Size**: Experience the rapid time-to-first-token high of no batching combined with the high throughput of larger batch sizes. (Related to batching tokens when streaming output)
- **Extensive Model Support**: Deploy almost any LLM from Hugging Face, including your own.
- **Customization**: Have full control over the configuration of every aspect of your deployment, from the model settings, to tokenizer options, to system configurations, and much more, all done through environment variables.
- **Speed**: Experience the speed of the vLLM Engine.
- **Serverless Scalability and Cost-Effectiveness**: Scale your deployment to handle any number of requests and only pay for active usage.

## Compatible models

You can deploy most [models from Hugging Face](https://huggingface.co/models?other=LLM).
For a full list of supported models architectures, see [Compatible model architectures](https://github.com/runpod-workers/worker-vllm/blob/main/README.md#compatible-model-architectures).

## Getting started

At a high level, you can set up the vLLM Worker by:

- Selecting your deployment options
- Configure any necessary environment variables
- Deploy your model

For detailed guidance on setting up, configuring, and deploying your vLLM Serverless Endpoint Worker, including compatibility details, environment variable settings, and usage examples, see [Get started](/serverless/workers/vllm/get-started).

### Deployment options

- **[Configurable Endpoints](/serverless/workers/vllm/get-started#deploy-using-the-web-ui)**: (recommended) Use RunPod's Web UI to quickly deploy the OpenAI compatible LLM with the vLLM Worker.

- **[Pre-Built docker image](/serverless/workers/vllm/get-started#deploy-using-the-worker-image)**: Leverage pre-configured Docker image for hassle-free deployment. Ideal for users seeking a quick and straightforward setup process

- **Custom docker image**: For advanced users, customize and build your Docker image with the model baked in, offering greater control over the deployment process.

For more information see:

- [vLLM Worker GitHub Repository](https://github.com/runpod-workers/worker-vllm)
- [vLLM Worker Docker Hub](https://hub.docker.com/r/runpod/worker-vllm/tags)

For more information on creating a custom docker image, see [Build Docker Image with Model Inside](https://github.com/runpod-workers/worker-vllm/blob/main/README.md#option-2-build-docker-image-with-model-inside).

## Next steps

- [Get started](/serverless/workers/vllm/get-started): Learn how to deploy a vLLM Worker as a Serverless Endpoint, with detailed guides on configuration and sending requests.
- [Configurable Endpoints](/serverless/workers/vllm/configurable-endpoints): Select your Hugging Face model and vLLM takes care of the low-level details of model loading, hardware configuration, and execution.
- [Environment variables](/serverless/workers/vllm/environment-variables): Explore the environment variables available for the vLLM Worker, including detailed documentation and examples.
- [Run Gemma 7b](/tutorials/serverless/gpu/run-gemma-7b): Walk through deploying Google's Gemma model using RunPod's vLLM Worker, guiding you to set up a Serverless Endpoint with a gated large language model (LLM).

---

# create-dockerfiles.md

File: tutorials/introduction/containers/create-dockerfiles.md

In the previous step, you ran a command that prints the container's uptime.
Now you'll create a Dockerfile to customize the contents of your own Docker image.

### Create a Dockerfile

Create a new file called `Dockerfile` and add the following items.

```dockerfile
FROM busybox
COPY entrypoint.sh /
RUN chmod +x /entrypoint.sh
ENTRYPOINT ["/entrypoint.sh"]
```

This Dockerfile starts from the `busybox` image like we used before. It then adds a custom `entrypoint.sh` script, makes it executable, and configures it as the entrypoint.

## The entrypoint script

Now let's create `entrypoint.sh` with the following contents:

```bash
#!/bin/sh
echo "The time is: $(date)"
```

:::note

While we named this script `entrypoint.sh` you will see a variety of naming conventions; such as:

- `start.sh`
- `CMD.sh`
- `entry_path.sh`

These files are normally placed in a folder called `script` but it is dependent on the maintainers of that repository.

:::

This is a simple script that will print the current time when the container starts.

### Why an entrypoint script:

- It lets you customize what command gets run when a container starts from your image.
- For example, our script runs date to print the time.
- Without it, containers would exit immediately after starting.
- Entrypoints make images executable and easier to reuse.

## Build the image

With those files created, we can now build a Docker image using our Dockerfile:

```bash
docker image build -t my-time-image .
```

This will build the image named `my-time-image` from the Dockerfile in the current directory.

### Why build a custom image:

- Lets you package up custom dependencies and configurations.
- For example you can install extra software needed for your app.
- Makes deploying applications more reliable and portable.
- Instead of installing things manually on every server, just use your image.
- Custom images can be shared and reused easily across environments.
- Building images puts your application into a standardized unit that "runs anywhere".
- You can version images over time as you update configurations.

## Run the image

Finally, let's run a container from our new image:

```bash
docker run my-time-image
```

We should see the same output as before printing the current time!

Entrypoints and Dockerfiles let you define reusable, executable containers that run the software and commands you need. This makes deploying and sharing applications much easier without per-server configuration.

By putting commands like this into a Dockerfile, you can easily build reusable and shareable images.

---

# docker-commands.md

File: tutorials/introduction/containers/docker-commands.md

RunPod enables bring-your-own-container (BYOC) development. If you choose this workflow, you will be using Docker commands to build, run, and manage your containers.

:::note

For a Dockerless workflow, see [RunPod projects](/docs/runpodctl/projects/overview.md).

:::

The following is a reference sheet to some of the most commonly used Docker commands.

## Login

Log in to a registry (like Docker Hub) from the CLI.
This saves credentials locally.

```command
docker login
docker login -u myusername
```

## Images

`docker push` - Uploads a container image to a registry like Docker Hub.
`docker pull` - Downloads container images from a registry like Docker Hub.
`docker images` - Lists container images that have been downloaded locally.
`docker rmi` - Deletes/removes a Docker container image from the machine.

```
docker push myuser/myimage:v1   # Push custom image
docker pull someimage           # Pull shared image
docker images                   # List downloaded images
docker rmi <image>              # Remove/delete image
```

## Containers

`docker run` - Launches a new container from a Docker image.
`docker ps` - Prints out a list of containers currently running.
`docker logs` - Shows stdout/stderr logs for a specific container.
`docker stop/rm` - Stops or totally removes a running container.

```command
docker run        # Start new container from image
docker ps         # List running containers
docker logs       # Print logs from container
docker stop       # Stop running container
docker rm         # Remove/delete container
```

## Dockerfile

`docker build` - Builds a Docker image by reading build instructions from a Dockerfile.

```command
docker build                         # Build image from Dockerfile
docker build --platform=linux/amd64  # Build for specific architecture
```

:::note

For the purposes of using Docker with RunPod, you should ensure your build command uses the `--platform=linux/amd64` flag to build for the correct architecture.

:::

## Volumes

:::note

When working with a Docker and RunPod, see how to [attach a Network Volume](/pods/storage/create-network-volumes).

:::

`docker volume create` - Creates a persisted and managed volume that can outlive containers.
`docker run -v` - Mounts a volume into a specific container to allow persisting data past container lifecycle.

```command
docker volume create         # Create volume
docker run -v <vol>:/data    # Mount volume into container
```

## Network

`docker network create` - Creates a custom virtual network for containers to communicate over.
`docker run --network=<name>` - Connects a running container to a Docker user-defined network.

```command
docker network create           # Create user-defined network
docker run --network=<name>     # Connect container
```

## Execute

`docker exec` - Execute a command in an already running container.
Useful for debugging/inspecting containers:

```command
docker exec
docker exec mycontainer ls -l /etc     # List files in container
```

---

# overview.md

File: tutorials/introduction/containers/overview.md

## What are containers?

> A container is an isolated environment for your code. This means that a container has no knowledge of your operating system, or your files. It runs on the environment provided to you by Docker Desktop. Containers have everything that your code needs in order to run, down to a base operating system.

[From Docker's website](https://docs.docker.com/guides/walkthroughs/what-is-a-container/#:~:text=A%20container%20is%20an%20isolated,to%20a%20base%20operating%20system)

Developers package their applications, frameworks, and libraries into a Docker container. Then, those containers can run outside their development environment.

### Why use containers?

> Build, ship, and run anywhere.

Containers are self-contained and run anywhere Docker runs. This means you can run a container on-premises or in the cloud, as well as in hybrid environments.
Containers include both the application and any dependencies, such as libraries and frameworks, configuration data, and certificates needed to run your application.

In cloud computing, you get the best cold start times with containers.

## What are images?

Docker images are fixed templates for creating containers. They ensure that applications operate consistently and reliably across different environments, which is vital for modern software development.

To create Docker images, you use a process known as "Docker build." This process uses a Dockerfile, a text document containing a sequence of commands, as instructions guiding Docker on how to build the image.

### Why use images?

Using Docker images helps in various stages of software development, including testing, development, and deployment. Images ensure a seamless workflow across diverse computing environments.

### Why not use images?

You must rebuild and push the container image, then edit your endpoint to use the new image each time you iterate on your code. Since development requires changing your code every time you need to troubleshoot a problem or add a feature, this workflow can be inconvenient.

For a streamlined development workflow, check out [RunPod projects](/docs/runpodctl/projects/overview.md). When you're done with development, you can create a Dockerfile from your project to reduce initialization overhead in production.

### What is Docker Hub?

After their creation, Docker images are stored in a registry, such as Docker Hub.
From these registries, you can download images and use them to generate containers, which make it easy to widely distribute and deploy applications.

Now that you've got an understanding of Docker, containers, images, and whether containerization is right for you, let's move on to installing Docker.

## Installing Docker

For this walkthrough, install Docker Desktop.
Docker Desktop bundles a variety of tools including:

- Docker GUI
- Docker CLI
- Docker extensions
- Docker Compose

The majority of this walkthrough uses the Docker CLI, but feel free to use the GUI if you prefer.

For the best installation experience, see Docker's [official documentation](https://docs.docker.com/get-docker/).

### Running your first command

Now that you've installed Docker, open a terminal window and run the following command:

```command
docker version
```

You should see something similar to the following output.

```text
docker version
Client: Docker Engine - Community
 Version:           24.0.7
 API version:       1.43
 Go version:        go1.21.3
 Git commit:        afdd53b4e3
 Built:             Thu Oct 26 07:06:42 2023
 OS/Arch:           darwin/arm64
 Context:           desktop-linux

Server: Docker Desktop 4.26.1 (131620)
 Engine:
  Version:          24.0.7
  API version:      1.43 (minimum version 1.12)
  Go version:       go1.20.10
  Git commit:       311b9ff
  Built:            Thu Oct 26 09:08:15 2023
  OS/Arch:          linux/arm64
  Experimental:     false
 containerd:
  Version:          1.6.25
  GitCommit:       abcd
 runc:
  Version:          1.1.10
  GitCommit:        v1.1.10-0-g18a0cb0
 docker-init:
  Version:          0.19.0
```

If at any point you need help with a command, you can use the `--help` flag to see documentation on the command you're running.

```command
docker --help
```

Let's run `busybox` from the command line to print out today's date.

```command
docker run busybox sh -c 'echo "The time is: $(date)"'
# The time is: Thu Jan 11 06:35:39 UTC 2024
```

- `busybox` is a lightweight Docker image with the bare minimum Linux utilities installed, including `echo`
- The `echo` command prints the container's uptime.

You've successfully installed Docker and run your first commands.

---

# persist-data.md

File: tutorials/introduction/containers/persist-data.md

In the [previous step](/tutorials/introduction/containers/create-dockerfiles), you created a Dockerfile and executed a command.
Now, you'll learn how to persist data outside of containers.

:::note

This walkthrough teaches you how to persist data outside of container.
RunPod has the same concept used for attaching a Network Volume to your Pod.

Consult the documentation on [attaching a Network Volume to your Pod](/pods/storage/create-network-volumes).

:::

## Why persist data outside a container?

The key goal is to have data persist across multiple container runs and removals.

By default, containers are ephemeral - everything inside them disappears when they exit.

So running something like:

```command
docker run busybox date > file.txt
```

Would only write the date to `file.txt` temporarily inside that container. As soon as the container shuts down, that file and data is destroyed.
This isn't great when you're training data and want your information to persist past your LLM training.

Because of this, we need to persist data outside the container.
Let's take a look at a workflow you can use to persist data outside a container.

---

## Create a named volume

First, we'll create a named volume to represent the external storage:

```command
docker volume create date-volume
```

### Update Dockerfile

Next, we'll modify our Dockerfile to write the date output to a file rather than printing directly to stdout:

```dockerfile
FROM busybox
WORKDIR /data
RUN touch current_date.txt
COPY entrypoint.sh /
RUN chmod +x /entrypoint.sh
ENTRYPOINT ["/entrypoint.sh"]
```

This sets the working directory to `/data`, touches a file called `current_date.txt`, and copies our script.

### Update entrypoint script

The `entrypoint.sh` script is updated:

```text
#!/bin/sh
date > /data/current_date.txt
```

This will write the date to the `/data/current_date.txt` file instead of printing it.

## Mount the volume

Now when the container runs, this will write the date to the `/data/current_date.txt` file instead of printing it.

Finally, we can mount the named volume to this data directory:

```command
docker run -v date-volume:/data my-image
```

This runs a container from my-image and mounts the `date-volume` Docker volume to the /data directory in the container.
Anything written to `/data` inside the container will now be written to the `date-volume` on the host instead of the container's ephemeral file system.
This allows the data to persist.
Once the container exits, the date output file is safely stored on the host volume.

After the container exits, we can exec into another container sharing the volume to see the persisted data file:

```command
docker run --rm -v date-volume:/data busybox cat /data/current_date.txt
```

This runs a new busybox container and also mounts the `date-volume`.

- Using the same -`v date-volume:/data mount` point maps the external volume dir to `/data` again.
- This allows the new container to access the persistent date file that the first container wrote.
- The `cat /data/current_date.txt` command prints out the file with the date output from the first container.
- The `--rm`flag removes the container after running so we don't accumulate stopped containers.

:::note

Remember, this is a general tutorial on Docker.
These concepts will help give you a better understanding of working with RunPod.

:::

---

# overview.md

File: tutorials/introduction/overview.md

This set of tutorials is meant to provide a deeper understanding of the tools that surround the RunPod platform.
These tutorials help you understand how to use the RunPod platform to build and deploy your applications.

While the documentation around the introduction section gives a holistic view and enough information to get started with RunPod, for more detailed information on the various of these tools or technologies, reach out to the source material.

- If you are looking for an understanding of Containers and Docker, see [Container overview](/tutorials/introduction/containers/overview).
- If you are looking to run your first Pod with RunPod, see [Run your first Fast Stable Diffusion with Jupyter Notebook](/tutorials/pods/run-your-first).
- For Serverless implementation, see [Run your first serverless endpoint with Stable Diffusion](/tutorials/serverless/gpu/run-your-first).

---

# images.md

File: tutorials/migrations/banana/images.md

Migrating your AI models and applications from one cloud service to another can often present a challenge, especially when the two platforms operate differently. This tutorial aims to streamline the process of moving from Banana Dev to RunPod, focusing on transferring Docker-based applications and AI models. Whether you're shifting due to preferences in service offerings, pricing, or performance, this guide will help you through the transition smoothly.

## Introduction

Banana Dev provides an environment for deploying machine learning models easily, while RunPod offers robust and scalable serverless solutions. Transitioning between these platforms involves adapting your application to the new environment's requirements and deploying it effectively.

Below, we'll walk through how to adapt a Python application from Banana Dev to RunPod, including necessary changes to your Dockerfile and deployment configurations.

## Step 1: Understand Your Application

First, take a comprehensive look at your current Banana Dev application. Our example application uses the `potassium` framework for serving a machine learning model:

```python
from io import BytesIO
from potassium import Potassium, Request, Response
from diffusers import DiffusionPipeline, DDPMScheduler
import torch
import base64

# create a new Potassium app
app = Potassium("my_app")


# @app.init runs at startup, and loads models into the app's context
@app.init
def init():
    repo_id = "Meina/MeinaUnreal_V3"

    ddpm = DDPMScheduler.from_pretrained(repo_id, subfolder="scheduler")

    model = DiffusionPipeline.from_pretrained(
        repo_id, use_safetensors=True, torch_dtype=torch.float16, scheduler=ddpm
    ).to("cuda")

    context = {
        "model": model,
    }

    return context


# @app.handler runs for every call
@app.handler()
def handler(context: dict, request: Request) -> Response:
    model = context.get("model")

    prompt = request.json.get("prompt")
    negative_prompt = "(worst quality, low quality:1.4), monochrome, zombie, (interlocked fingers), cleavage, nudity, naked, nude"

    image = model(
        prompt=prompt,
        negative_prompt=negative_prompt,
        guidance_scale=7,
        num_inference_steps=request.json.get("steps", 30),
        generator=(
            torch.Generator(device="cuda").manual_seed(request.json.get("seed"))
            if request.json.get("seed")
            else None
        ),
        width=512,
        height=512,
    ).images[0]

    buffered = BytesIO()
    image.save(buffered, format="JPEG", quality=80)
    img_str = base64.b64encode(buffered.getvalue())

    return Response(json={"output": str(img_str, "utf-8")}, status=200)


if __name__ == "__main__":
    app.serve()
```

This application initializes a BERT model for fill-mask tasks and serves it over HTTP.

---

## Step 2: Adapt Your Code for RunPod

In RunPod, applications can be adapted to run in a serverless manner, which involves modifying your application logic to fit into the RunPod's handler function format. Below is an example modification that adapts our initial Banana Dev application to work with RunPod, using the `diffusers` library for AI model inference:

```python
import runpod
from diffusers import AutoPipelineForText2Image
import base64
import io
import time

# If your handler runs inference on a model, load the model here.
# You will want models to be loaded into memory before starting serverless.

try:
    pipe = AutoPipelineForText2Image.from_pretrained("meina/meinaunreal_v3")
    pipe.to("cuda")
except RuntimeError:
    quit()


def handler(job):
    """Handler function that will be used to process jobs."""
    job_input = job["input"]
    prompt = job_input["prompt"]

    time_start = time.time()
    image = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]
    print(f"Time taken: {time.time() - time_start}")

    buffer = io.BytesIO()
    image.save(buffer, format="PNG")
    image_bytes = buffer.getvalue()

    return base64.b64encode(image_bytes).decode("utf-8")


runpod.serverless.start({"handler": handler})
```

---

This modification involves initializing your model outside of the handler function to ensure it's loaded into memory before processing jobs, a crucial step for efficient serverless execution.

## Step 3: Update Your Dockerfile

The Dockerfile must also be adapted for RunPod's environment. Here's a comparison between a typical Banana Dev Dockerfile and the adapted version for RunPod:

### Banana Dev Dockerfile

```dockerfile
FROM pytorch/pytorch:1.11.0-cuda11.3-cudnn8-runtime
...
CMD python3 -u app.py
```

---

### RunPod Dockerfile

```dockerfile
FROM runpod/base:0.4.0-cuda11.8.0

CMD python3.11 -u /handler.py
```

---

The key differences include the base image and the execution command, reflecting RunPod's requirements and Python version specifics.

## Step 4: Deploy to RunPod

Once your code and Dockerfile are ready, the next steps involve building your Docker image and deploying it on RunPod. This process typically involves:

- Building your Docker image with the adapted Dockerfile.
- Pushing the image to a container registry (e.g., DockerHub).
- Creating a serverless function on RunPod and configuring it to use your Docker image.

## Testing and Verification

After deployment, thoroughly test your application to ensure it operates as expected within the RunPod environment. This may involve sending requests to your serverless endpoint and verifying the output.

## Conclusion

Migrating from Banana Dev to RunPod involves several key steps: adapting your application code, updating the Dockerfile, and deploying the adapted application on RunPod. By following this guide, you can make the transition smoother and take advantage of RunPod's serverless capabilities for your AI applications.

Remember to review RunPod's documentation for specific details on serverless deployment and configuration options to optimize your application's performance and cost-efficiency.

---

# overview.md

File: tutorials/migrations/banana/overview.md


To get started with RunPod:

- [Create a RunPod account](/get-started/manage-accounts)
- [Add funds](/get-started/billing-information)
- [Use the RunPod SDK](#setting-up-your-project) to build and connect with your Serverless Endpoints

<details>
<summary>

**Quick migration with Docker**

</summary>

Transitioning from Banana to RunPod doesn't have to be a lengthy process.
For users seeking a swift migration path while maintaining Banana's dependencies for the interim, the Docker approach provides an efficient solution.
This method allows you to leverage Docker to encapsulate your environment, simplifying the migration process and enabling a smoother transition to RunPod.

**Why consider the Dockerfile approach?**

Utilizing a Dockerfile for migration offers a bridge between Banana and RunPod, allowing for immediate deployment of existing projects without the need to immediately discard Banana's dependencies. This approach is particularly beneficial for those looking to test or move their applications to RunPod with minimal initial adjustments.

**Dockerfile**

The provided Dockerfile outlines a straightforward process for setting up your application on RunPod.

Add this Dockerfile to your project.

```dockerfile
FROM runpod/banana:peel as bread
FROM repo/image:tag

RUN pip install runpod

COPY --from=bread /handler.py .
COPY --from=bread /start.sh .

RUN chmod +x start.sh
CMD ["./start.sh"]
```

**Building and deploying**

After creating your Dockerfile, build your Docker image and deploy it to RunPod.
This process involves using Docker commands to build the image and then deploying it to RunPod.

**Advantages and considerations**

This Dockerfile approach expedites the migration process, allowing you to leverage RunPod's powerful features with minimal initial changes to your project.
It's an excellent way to quickly transition and test your applications on RunPod.

However, while this method facilitates a quick start on RunPod, it's advisable to plan for a future migration away from Banana's dependencies, as there is overhead to building Banana's dependencies and deploying them to RunPod.

Gradually adapting your project to utilize RunPod's native features and services will optimize your application's performance and scalability.

**Moving forward**

Once you've migrated your application using the Docker approach, consider exploring RunPod's full capabilities.
Transitioning away from Banana's dependencies and fully integrating with RunPod's services will allow you to take full advantage of what RunPod has to offer.

This quick migration guide is just the beginning.
Continue with the rest of our tutorial to learn how to leverage RunPod's features to their fullest and ensure your project is fully adapted to its new environment.

</details>

The rest of this guide will help you set up a RunPod project.

## Setting up your project

Just like with Banana, RunPod provides a Python SDK to run your projects.

To get started, install setup a virtual environment then install the SDK library.

<Tabs>
    ```command
    ```
    ```command
    ```
```command
```
```command
```
```text
```
```text
```

---

# text.md

File: tutorials/migrations/banana/text.md

Migrating your AI models and applications from one cloud service to another can often present a challenge, especially when the two platforms operate differently. This tutorial aims to streamline the process of moving from Banana Dev to RunPod, focusing on transferring Docker-based applications and AI models. Whether you're shifting due to preferences in service offerings, pricing, or performance, this guide will help you through the transition smoothly.

## Vllm Worker

RunPod provides an optimized image for running AI models. This image is called the Vllm Worker. It is based on the [Vllm](https://github.com/vllm/vllm) framework, which is a lightweight, high-performance, and portable AI framework. The Vllm Worker image is designed to run on RunPod's serverless infrastructure and is optimized for performance and scalability.

### Vllm Worker Image

To get started, login to RunPod and select Serverless.
Choose your GPU.
Add `runpod/worker-vllm:0.2.2` to the Container Image.
Set the Container Disk to size large enough for your model.
Under Environment Variables, add `MODEL_NAME` and set it to your model name, for example `bert-base-uncased`.
Select **Deploy**.

Once your serverless pod has initialized, you can start executing commands against the Endpoint.

```command
curl --request POST \
     --url https://api.runpod.ai/v2/{YOUR_ENDPOINT}/runsync \
     --header 'accept: application/json' \
     --header 'authorization: ${YOUR_API_KEY}' \
     --header 'content-type: application/json' \
     --data @- <<EOF
{
  "input": {
    "prompt": "What is the meaning of life?",
    "do_sample": false,
    "max_length": 100,
    "temperature": 0.9
  }
}
EOF
```

---

# overview.md

File: tutorials/migrations/cog/overview.md

To get started with RunPod:

- [Create a RunPod account](/get-started/manage-accounts)
- [Add funds](/get-started/billing-information)
- [Use the RunPod SDK](/serverless/overview) to build and connect with your Serverless Endpoints

In this tutorial, you'll go through the process of migrating a model deployed via replicate.com or utilizing the Cog framework to a RunPod serverless worker.

This guide assumes you are operating within a Linux terminal environment and have Docker installed on your system.

:::note

This method might occur a delay when working with RunPod Serverless Endpoints.
This delay is due to the FastAPI server that is used to run the Cog model.

To eliminate this delay, consider using [RunPod Handler](/serverless/workers/overview) functions in a future iteration.

:::

By following this streamlined process, you'll be able to simplify the migration and deployment of your Cog image.

### Prerequisites

- Docker installed on your system
- Familiarity with the Cog framework
- Existing model on Replicate.com
- RunPod account

## Clone and navigate the cog-worker repository

Before we begin, let's set up the necessary environment.
You will need to clone the `cog-worker` repository, which contains essential scripts and configuration files required for the migration process.
To do this, run the following commands in your terminal:

```bash
git clone https://github.com/runpod-workers/cog-worker.git
cd cog-worker/
```

The cog-worker repository contains essential scripts and configuration files required for the migration.

Now that the repository is cloned and you've navigated to the correct directory, you're ready to proceed with the next step.

## Identify model information

In this step, you will need to gather the necessary information about your Cog model that is currently hosted on Replicate.com.
You will require your username, model name, and version.

Identify the username, model name, and version you wish to use from Replicate.
For example, if you are using [this model](https://replicate.com/lucataco/hotshot-xl/versions):

- your username is `lucataco`
- your model name is `hotshot-xl`
- your model version is `78b3a6257e16e4b241245d65c8b2b81ea2e1ff7ed4c55306b511509ddbfd327a`

Once you have collected the required information, you can move on to the next step, where you will build and push your Docker image.

## Build and push docker image

Now that you have identified the necessary information about your model, you can proceed to build and push your Docker image. This is a crucial step, as it prepares your model for deployment on the RunPod platform.

Build the Docker image by providing the necessary arguments for your model.
Once your Docker image is built, push it to a container repository such as DockerHub:

```bash
# replace user, model_name, and model_version with the appropriate values
docker build -platform=linux/amd64 --tag <username>/<repo>:<tag> --build-arg COG_REPO=user --build-arg COG_MODEL=model_name --build-arg COG_VERSION=model_version .
docker push <username>/<repo>:<tag>
```

The `--tag` option allows you to specify a name and tag for your image, while the `--build-arg` options provide the necessary information for building the image.

With your Docker image built and pushed, you're one step closer to deploying your Cog model on RunPod.

## Create and Deploy a Serverless Endpoint

Now that your Docker image is ready, it's time to create and deploy a serverless endpoint on RunPod.
This step will enable you to send requests to your new endpoint and use your Cog model in a serverless environment.

To create and deploy a serverless endpoint on RunPod:

1. Log in to the [RunPod Serverless console](https://www.runpod.io/console/serverless).
2. Select **+ New Endpoint**.
3. Provide the following:
   1. Endpoint name.
   2. Select a GPU.
   3. Configure the number of Workers.
   4. (optional) Select **FlashBoot**.
   5. (optional) Select a template.
   6. Enter the name of your Docker image.
      - For example `<username>/<repo>:<tag>`.
   7. Specify enough memory for your Docker image.
4. Select **Deploy**.

Now, let's send a request to your [Endpoint](/serverless/endpoints/get-started).

Once your endpoint is set up and deployed, you'll be able to start receiving requests and utilize your Cog model in a serverless context.

## Conclusion

Congratulations, you have successfully migrated your Cog model from Replicate to RunPod and set up a serverless endpoint.
As you continue to develop your models and applications, consider exploring additional features and capabilities offered by RunPod to further enhance your projects.

Here are some resources to help you continue your journey:

- [Learn more about RunPod serverless workers](/serverless/overview)
- [Explore additional RunPod tutorials and examples](/tutorials/introduction/overview)

---

# overview.md

File: tutorials/migrations/openai/overview.md

To get started with RunPod:

- [Create a RunPod account](/get-started/manage-accounts)
- [Add funds](/get-started/billing-information)
- [Use the RunPod SDK](/serverless/overview) to build and connect with your Serverless Endpoints

This tutorial guides you through the steps necessary to modify your OpenAI Codebase for use with a deployed vLLM Worker on RunPod. You will learn to adjust your code to be compatible with OpenAI's API, specifically for utilizing Chat Completions, Completions, and Models routes. By the end of this guide, you will have successfully updated your codebase, enabling you to leverage the capabilities of OpenAI's API on RunPod.


To update your codebase, you need to replace the following:

- Your OpenAI API Key with your RunPod API Key
- Your OpenAI Serverless Endpoint URL with your RunPod Serverless Endpoint URL
- Your OpenAI model with your custom LLM model deployed on RunPod

<Tabs>
```python
````
```javascript
````

---

# overview.md

File: tutorials/overview.md

Learn how to build and deploy applications on the RunPod platform with this set of tutorials. Covering tools, technologies, and deployment methods, including Containers, Docker, and Serverless implementation.

## Serverless

Explore how to run and deploy AI applications using RunPod's Serverless platform.

### GPUs

- [Generate images with SDXL Turbo](/tutorials/serverless/gpu/generate-sdxl-turbo): Learn how to build a web application using RunPod's Serverless Workers and SDXL Turbo from Stability AI, a fast text-to-image model, and send requests to an Endpoint to generate images from text-based inputs.
- [Run Google's Gemma model](/tutorials/serverless/gpu/run-gemma-7b): Deploy Google's Gemma model on RunPod's vLLM Worker, create a Serverless Endpoint, and interact with the model using OpenAI APIs and Python.
- [Run your first serverless endpoint with Stable Diffusion](/tutorials/serverless/gpu/run-your-first): Use RunPod's Stable Diffusion v1 inference endpoint to generate images, set up your serverless worker, start a job, check job status, and retrieve results.

### CPUs

- [Run an Ollama Server on a RunPod CPU](/tutorials/serverless/cpu/run-ollama-inference): Set up and run an Ollama server on RunPod CPU for inference with this step-by-step tutorial.

## Pods

Discover how to leverage RunPod Pods to run and manage your AI applications.

### GPUs

- [Fine tune an LLM with Axolotl on RunPod](/tutorials/pods/fine-tune-llm-axolotl): Learn how to fine-tune large language models with Axolotl on RunPod, a streamlined workflow for configuring and training AI models with GPU resources, and explore examples for LLaMA2, Gemma, LLaMA3, and Jamba.
- [Run Fooocus in Jupyter Notebook](/tutorials/pods/run-fooocus): Learn how to run Fooocus, an open-source image generating model, in a Jupyter Notebook and launch the Gradio-based interface in under 5 minutes, with minimal requirements of 4GB Nvidia GPU memory and 8GB system memory.
- [How To Connect to a Pod Instance through VSCode](/tutorials/pods/connect-to-vscode): Learn how to connect to a RunPod Pod instance through VSCode for seamless development and management.
- [Build Docker Images on Runpod with Bazel](/tutorials/pods/build-docker-images): Learn how to build Docker images on RunPod using Bazel, a powerful build tool for creating consistent and efficient builds.
- [Set up Ollama on your GPU Pod](/tutorials/pods/run-ollama): Set up Ollama, a powerful language model, on a GPU Pod using RunPod, and interact with it through HTTP API requests, harnessing the power of GPU acceleration for your AI projects.
- [Run your first Fast Stable Diffusion with Jupyter Notebook](/tutorials/pods/run-your-first): Deploy a Jupyter Notebook to RunPod and generate your first image with Stable Diffusion in just 20 minutes, requiring Hugging Face user access token, RunPod infrastructure, and basic familiarity with the platform.

### CPUs

- [Run Docker in Docker on RunPod CPU Instances](/tutorials/pods/run-docker-in-docker): Learn how to run Docker in Docker on RunPod CPU instances for enhanced development and testing capabilities.

## Containers

Understand the use of Docker images and containers within the RunPod ecosystem.

- [Persist data outside of containers](/tutorials/introduction/containers/persist-data): Learn how to persist data outside of containers by creating named volumes, mounting volumes to data directories, and accessing persisted data from multiple container runs and removals in Docker.
- [Containers overview](/tutorials/introduction/containers/overview): Discover the world of containerization with Docker, a platform for isolated environments that package applications, frameworks, and libraries into self-contained containers for consistent and reliable deployment across diverse computing environments.
- [Dockerfile](/tutorials/introduction/containers/create-dockerfiles): Learn how to create a Dockerfile to customize a Docker image and use an entrypoint script to run a command when the container starts, making it a reusable and executable unit for deploying and sharing applications.
- [Docker commands](/tutorials/introduction/containers/docker-commands): RunPod enables BYOC development with Docker, providing a reference sheet for commonly used Docker commands, including login, images, containers, Dockerfile, volumes, network, and execute.

## Integrations

Explore how to integrate RunPod with other tools and platforms like OpenAI, SkyPilot, and Charm's Mods.

### OpenAI

- [Overview](/tutorials/migrations/openai/overview): Use the OpenAI SDK to integrate with your Serverless Endpoints.

### SkyPilot

- [Running RunPod on SkyPilot](/integrations/skypilot): Learn how to deploy Pods from RunPod using SkyPilot.

### Mods

- [Running RunPod on Mods](/integrations/mods): Learn to integrate into Charm's Mods tool chain and use RunPod as the Serverless Endpoint.

## Migration

Learn how to migrate from other tools and technologies to RunPod.

### Cog

- [Cog Migration](/tutorials/migrations/cog/overview): Migrate your Cog model from [Replicate.com](https://www.replicate.com) to RunPod by following this step-by-step guide, covering setup, model identification, Docker image building, and serverless endpoint creation.

### Banana

- [Banana migration](/tutorials/migrations/banana/overview): Quickly migrate from Banana to RunPod with Docker, leveraging a bridge between the two environments for a seamless transition. Utilize a Dockerfile to encapsulate your environment and deploy existing projects to RunPod with minimal adjustments.

---

# build-docker-images.md

File: tutorials/pods/build-docker-images.md

# Build Docker Images on RunPod with Bazel

RunPod's GPU Pods use custom Docker images to run your code.
This means you can't directly spin up your own Docker instance or build Docker containers on a GPU Pod.
Tools like Docker Compose are also unavailable.

This limitation can be frustrating when you need to create custom Docker images for your RunPod templates.

Fortunately, many use cases can be addressed by creating a custom template with the desired Docker image.

In this tutorial, you'll learn how to use the [Bazel](https://bazel.build) build tool to build and push Docker images from inside a RunPod container.

By the end of this tutorial, you’ll be able to build custom Docker images on RunPod and push them to Docker Hub for use in your own templates.

## Prerequisites

Before you begin this guide you'll need the following:

- A Docker Hub account and access token for authenticating the docker login command
- Enough volume for your image to be built

## Create a Pod

1. Navigate to [Pods](https://www.runpod.io/console/pods) and select **+ Deploy**.
2. Choose between **GPU** and **CPU**.
3. Customize your an instance by setting up the following:
   1. (optional) Specify a Network volume.
   2. Select an instance type. For example, **A40**.
   3. (optional) Provide a template. For example, **RunPod Pytorch**.
   4. (GPU only) Specify your compute count.
4. Review your configuration and select **Deploy On-Demand**.

For more information, see [Manage Pods](/pods/manage-pods#start-a-pod).

Wait for the Pod to spin up then connect to your Pod through the Web Terminal:

1. Select **Connect**.
2. Choose **Start Web Terminal** and then **Connect to Web Terminal**.
3. Enter your username and password.

Now you can clone the example GitHub repository

## Clone the example GitHub repository

Clone the example code repository that demonstrates building Docker images with Bazel:

```command
git clone https://github.com/therealadityashankar/build-docker-in-runpod.git && cd build-docker-in-runpod
```

## Install dependencies

Install the required dependencies inside the Runpod container:

Update packages and install sudo:

```command
apt update && apt install -y sudo
```

Install Docker using the convenience script:

```command
curl -fsSL https://get.docker.com -o get-docker.sh && sudo sh get-docker.sh
```

Log in to Docker using an access token:

1. Go to https://hub.docker.com/settings/security and click "New Access Token".
2. Enter a description like "Runpod Token" and select "Read/Write" permissions.
3. Click "Generate" and copy the token that appears.
4. In the terminal, run:

```command
docker login -u <your-username>
```

When prompted, paste in the access token you copied instead of your password.

Install Bazel via the Bazelisk version manager:

```command
wget https://github.com/bazelbuild/bazelisk/releases/download/v1.20.0/bazelisk-linux-amd64
chmod +x bazelisk-linux-amd64  
sudo cp ./bazelisk-linux-amd64 /usr/local/bin/bazel
```

## Configure the Bazel Build

First, install nano if it’s not already installed and open the `BUILD.bazel` file for editing:

```
sudo apt install nano
nano BUILD.bazel
```

Replace the `{YOUR_USERNAME}` placeholder with your Docker Hub username in the `BUILD.bazel` file:

```starlark
[label BUILD.bazel]
oci_push(
    name = "push_custom_image",
    image = ":custom_image",
    repository = "index.docker.io/{YOUR_USERNAME}/custom_image",
    remote_tags = ["latest"]
)
```

## Build and Push the Docker Image

Run the bazel command to build the Docker image and push it to your Docker Hub account:

```command
bazel run //:push_custom_image
```

Once the command completes, go to https://hub.docker.com/ and log in. You should see a new repository called `custom_image` containing the Docker image you just built.

You can now reference this custom image in your own Runpod templates.

## Conclusion

In this tutorial, you learned how to use Bazel to build and push Docker images from inside RunPod containers.
By following the steps outlined, you can now create and utilize custom Docker images for your RunPod templates.
The techniques demonstrated can be further expanded to build more complex images, providing a flexible solution for your containerization needs on RunPod.

---

# connect-to-vscode.md

File: tutorials/pods/connect-to-vscode.md

This tutorial explains how to connect directly to your Pod instance through VSCode, allowing you to work within your volume directory as if the files were stored on your local machine. By following this guide, you will learn to create a Pod instance and establish a connection from the Pod to your VSCode editor.

When you're finished, you'll be able to seamlessly develop and manage files on your Pod instance using VSCode.

## Prerequisites

To complete this tutorial, you will need:

- A local development environment with VSCode installed. Download it from [Visual Studio Code](https://code.visualstudio.com/download).
- Familiarity with basic command-line operations and SSH.
- Your SSH key setup with RunPod, see [Use SSH](https://docs.runpod.io/pods/configuration/use-ssh).
- A RunPod account. Sign up at [RunPod](https://www.runpod.io/).
- (Optional) A GitHub account for code management.

## Create a Pod instance

In this step, you will create a Pod instance.

1. Navigate to [Pods](https://www.runpod.io/console/pods) and select **+ Deploy**.
2. Choose between **GPU** and **CPU** based on your requirements.
3. Customize your instance by setting up the following:
   1. (Optional) Specify a Network volume.
   2. Select an instance type. For example, **A40**.
   3. (Optional) Provide a template. For example, **RunPod Pytorch**.
   4. (GPU only) Specify your compute count.
4. Review your configuration and select **Deploy On-Demand**.

For more information, see [Manage Pods](https://www.runpod.io/pods/manage-pods#start-a-pod).

Next, establish a connection with your Pod.

## Establish a connection

In this step, you will connect to your Pod instance using SSH.

1. From the [Pods](https://www.runpod.io/console/pods) page, select the Pod you just deployed.
2. Select **Connect** and copy the **SSH over exposed TCP: (Supports SCP & SFTP)** command. For example:

   ```command
   ssh root@123.456.789.80 -p 12345 -i ~/.ssh/id_ed12345
   ```

Next, you will set up the connection in VSCode.

## Configuring VSCode for remote development

To connect your VSCode editor to the Pod instance, follow these steps:

1. Open VSCode and install the [Dev Container extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers).
2. Open the **Command Palette** (`CTRL+SHIFT+P`) and choose **Remote-SSH: Add New SSH Host**.
3. Enter the copied SSH command from the previous step and paste it into the text box.
4. Update your `.ssh/config` file by saving the configuration. For example:

   ```plaintext
   Host your_pod_instance
       HostName 123.456.789.80
       User root
       Port 12345
       IdentityFile ~/.ssh/id_ed12345
   ```

5. In the **Command Palette**, select **Remote-SSH: Connect to Host** and choose your newly added host.

You are now connected to your Pod instance within VSCode and can work on your volume directory as if it were local.

---

# fine-tune-llm-axolotl.md

File: tutorials/pods/fine-tune-llm-axolotl.md


:::note

RunPod provides an easier method to fine tune an LLM.
For more information, see [Fine tune a model](/docs/fine-tune).

:::

[axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) is a tool that simplifies the process of training large language models (LLMs).
It provides a streamlined workflow that makes it easier to fine-tune AI models on various configurations and architectures.
When combined with RunPod's GPU resources, Axolotl enables you to harness the power needed to efficiently train LLMs.

In addition to its user-friendly interface, Axolotl offers a comprehensive set of YAML examples covering a wide range of LLM families, such as LLaMA2, Gemma, LLaMA3, and Jamba.
These examples serve as valuable references, helping users understand the role of each parameter and guiding them in making appropriate adjustments for their specific use cases.
It is highly recommended to explore [these examples](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples) to gain a deeper understanding of the fine-tuning process and optimize the model's performance according to your requirements.

In this tutorial, we'll walk through the steps of training an LLM using Axolotl on RunPod and uploading your model to Hugging Face.

### Setting up the environment

Fine-tuning a large language model (LLM) can take up a lot of compute power.
Because of this, we recommend fine-tuning using RunPod's GPUs.

To do this, you'll need to create a Pod, specify a container, then you can begin training.
A Pod is an instance on a GPU or multiple GPUs that you can use to run your training job.
You also specify a Docker image like `axolotlai/axolotl-cloud:main-latest` that you want installed on your Pod.

1. Login to [RunPod](https://www.runpod.io/console/console/home) and deploy your Pod.
   1. Select **Deploy**.
   2. Select a GPU instance.
   3. Specify the `axolotlai/axolotl-cloud:main-latest` image as your Template image.
   4. Select your GPU count.
   5. Select **Deploy**.

Now that you have your Pod set up and running, connect to it over secure SSH.

2. Wait for the Pod to startup, then connect to it using secure SSH.
   1. On your Pod page, select **Connect**.
   2. Copy the secure SSH string and paste it into your terminal on your machine.
   ```bash
   ssh <username>@<pod-ip-address> -p <ssh-port> -i <path-to-ssh-key>  string
   ```
   Follow the on-screen prompts to SSH into your Pod.

:::note

You should use the SSH connection to your Pod as it is a persistent connection.
The Web UI terminal shouldn't be relied on for long-running processes, as it will be disconnected after a period of inactivity.

:::

With the Pod deployed and connected via SSH, we're ready to move on to preparing our dataset.

### Preparing the dataset

The dataset you provide to your LLM is crucial, as it's the data your model will learn from during fine-tuning.
You can make your own dataset that will then be used to fine-tune your own model, or you can use a pre-made one.

To continue, use either a [local dataset](#using-a-local-dataset) or one [stored on Hugging Face](#using-a-hugging-face-dataset).

#### Using a local dataset

To use a local dataset, you'll need to transfer it to your RunPod instance.
You can do this using RunPod CLI to securely transfer files from your local machine to the one hosted by RunPod.
All Pods automatically come with `runpodctl` installed with a Pod-scoped API key.
**To send a file**

<Tabs>
```bash
```
```bash
```
```bash
```
```bash
```
```command
```
```command
```
```command
```
```command
```
```command
```
```command
```
```command
```
```command
```
```command
```

---

# run-fooocus.md

File: tutorials/pods/run-fooocus.md

## Overview

Fooocus is an open-source image generating model.

In this tutorial, you'll run Fooocus in a Jupyter Notebook and then launch the Gradio-based interface to generate images.

Time to complete: ~5 minutes

## Prerequisites

The minimal requirement to run Fooocus is:

- 4GB Nvidia GPU memory (4GB VRAM)
- 8GB system memory (8GB RAM)

## RunPod infrastructure

1. Select **Pods** and choose **+ GPU Pod**.
2. Choose a GPU instance with at least 4GB VRAM and 8GB RAM by selecting **Deploy**.
3. Search for a template that includes **Jupyter Notebook** and select **Deploy**.
   - Select **RunPod Pytorch 2**.
   - Ensure **Start Jupyter Notebook** is selected.
4. Select **Choose** and then **Deploy**.

## Run the notebook

1. Select **Connect to Jupyter Lab**.
2. In the Jupyter Lab file browser, select **File > New > Notebook**.
3. In the first cell, paste the following and then run the Notebook.

```bash
!pip install pygit2==1.12.2
!pip install opencv-python==4.9.0.80
%cd /workspace
!git clone https://github.com/lllyasviel/Fooocus.git
%cd /workspace/Fooocus
!python entry_with_update.py --share
```

## Launch UI

Look for the line:

```text
App started successful. Use the app with ....
```

And select the link.

## Explore the model

Explore and run the model.

---

# run-ollama.md

File: tutorials/pods/run-ollama.md


This tutorial will guide you through setting up [Ollama](https://ollama.com), a powerful platform serving large language model, on a GPU Pod using RunPod.
Ollama makes it easy to run, create, and customize models.

However, not everyone has access to the compute power needed to run these models.
With RunPod, you can spin up and manage GPUs in the Cloud.
RunPod offers templates with preinstalled libraries, which makes it quick to run Ollama.

In the following tutorial, you'll set up a Pod on a GPU, install and serve the Ollama model, and interact with it on the CLI.

## Prerequisites

The tutorial assumes you have a RunPod account with credits.
No other prior knowledge is needed to complete this tutorial.

## Step 1: Start a PyTorch Template on RunPod

You will create a new Pod with the PyTorch template.
In this step, you will set overrides to configure Ollama.

1. Log in to your [RunPod account](https://www.runpod.io/console/pods) and choose **+ GPU Pod**.
2. Choose a GPU Pod like `A40`.
3. From the available templates, select the lastet PyTorch template.
4. Select **Customize Deployment**.
   1. Add the port `11434` to the list of exposed ports. This port is used by Ollama for HTTP API requests.
   2. Add the following environment variable to your Pod to allow Ollama to bind to the HTTP port:
      - Key: `OLLAMA_HOST`
      - Value: `0.0.0.0`
5. Select **Set Overrides**, **Continue**, then **Deploy**.

This setting configures Ollama to listen on all network interfaces, enabling external access through the exposed port.
For detailed instructions on setting environment variables, refer to the [Ollama FAQ documentation](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux).

Once the Pod is up and running, you'll have access to a terminal within the RunPod interface.

## Step 2: Install Ollama

Now that your Pod is running, you can Log in to the web terminal.
The web terminal is a powerful way to interact with your Pod.

1. Select **Connect** and choose **Start Web Terminal**.
2. Make note of the **Username** and **Password**, then select **Connect to Web Terminal**.
3. Enter your username and password.
4. To ensure Ollama can automatically detect and utilize your GPU, run the following commands.

```sh
apt update
apt install lshw
```

5. Run the following command to install Ollama and send to the background:

```bash
(curl -fsSL https://ollama.com/install.sh | sh && ollama serve > ollama.log 2>&1) &
```

This command fetches the Ollama installation script and executes it, setting up Ollama on your Pod.
The `ollama serve` part starts the Ollama server, making it ready to serve AI models.

Now that your Ollama server is running on your Pod, add a model.

## Step 3: Run an AI Model with Ollama

To run an AI model using Ollama, pass the model name to the `ollama run` command:

```bash
ollama run [model name]
# ollama run llama2
# ollama run mistral
```

Replace `[model name]` with the name of the AI model you wish to deploy.
For a complete list of models, see the [Ollama Library](https://ollama.com/library).

This command pulls the model and runs it, making it accessible for inference.
You can begin interacting with the model directly from your web terminal.

Optionally, you can set up an HTTP API request to interact with Ollama.
This is covered in the [next step](#step-4-interact-with-ollama-via-http-api).

## Step 4: Interact with Ollama via HTTP API

With Ollama set up and running, you can now interact with it using HTTP API requests.
In step 1.4, you configured Ollama to listen on all network interfaces.
This means you can use your Pod as a server to receive requests.

**Get a list of models**

To list the local models available in Ollama, you can use the following GET request:

<Tabs>
```
```
```json
```
```command
```

---

# run-your-first.md

File: tutorials/pods/run-your-first.md

## Overview

By the end of this tutorial, you’ll have deployed a Jupyter Notebook to RunPod, deployed an instance of Stable Diffusion, and generated your first image.

Time to complete: ~20 minutes

## Prerequisites

- [Hugging Face user access token](https://huggingface.co/docs/hub/security-tokens)

## RunPod infrastructure

- Select **RunPod Fast Stable Diffusion**
- Choose 1x RTX A5000 or 1x RTX 3090
- Select **Start Jupyter Notebook**
- Deploy.

## Run the notebook

- Select **RNPD-A1111.ipynb**
- Enter Hugging Face user access token
- Select the model you want to run:
  - `v.1.5` | `v2-512` | `v2-768`

## Launch Automatic1111 on your pod

- The cell labeled **Start Stable-Diffusion** will launch your pod.
  - (optional) Provide login credentials for this instance.
- Select the blue link ending in `.proxy.runpod.net`

## Explore Stable-Diffusion

Now that your pod is up and running Stable-Diffusion.

Explore and run the model.

---

# 00_hello.md

File: tutorials/sdks/python/101/00_hello.md

RunPod's serverless library enables you to create and deploy scalable functions without managing infrastructure.
This tutorial will walk you through creating a simple serverless function that determines whether a number is even.

## Creating a Basic Serverless Function

Let's start by building a function that checks if a given number is even.

### Import the RunPod library

Create a new python file called `is_even.py`.

Import the RunPod library:

```python
import runpod
```

### Define your function

Create a function that takes a `job` argument:

```python
def is_even(job):
    job_input = job["input"]
    the_number = job_input["number"]

    if not isinstance(the_number, int):
        return {"error": "Please provide an integer."}

    return the_number % 2 == 0
```

This function:

1. Extracts the input from the `job` dictionary
2. Checks if the input is an integer
3. Returns an error message if it's not an integer
4. Determines if the number is even and returns the result

### Start the Serverless function

Wrap your function with `runpod.serverless.start()`:

```python
runpod.serverless.start({"handler": is_even})
```

This line initializes the serverless function with your specified handler.

## Complete code example

Here's the full code for our serverless function:

```python
import runpod


def is_even(job):
    job_input = job["input"]
    the_number = job_input["number"]

    if not isinstance(the_number, int):
        return {"error": "Please provide an integer."}

    return the_number % 2 == 0


runpod.serverless.start({"handler": is_even})
```

## Testing your Serverless Function

To test your function locally, use the following command:

```bash
python is_even.py --test_input '{"input": {"number": 2}}'
```

When you run the test, you'll see output similar to this:

```plaintext
--- Starting Serverless Worker |  Version 1.6.2 ---
INFO   | test_input set, using test_input as job input.
DEBUG  | Retrieved local job: {'id': 'some-id', 'input': {'number': 2}}
INFO   | some-id | Started.
DEBUG  | some-id | Handler output: True
DEBUG  | some-id | run_job return: {'output': True}
INFO   | Job some-id completed successfully.
INFO   | Job result: {'output': True}
INFO   | Local testing complete, exiting.
```

This output indicates that:

1. The serverless worker started successfully
2. It received the test input
3. The function processed the input and returned `True` (as 2 is even)
4. The job completed successfully

## Conclusion

You've now created a basic serverless function using RunPod's Python SDK. This approach allows for efficient, scalable deployment of functions without the need to manage infrastructure.

To further explore RunPod's serverless capabilities, consider:

- Creating more complex functions
- Implementing error handling and input validation
- Exploring RunPod's documentation for advanced features and best practices

RunPod's serverless library provides a powerful tool for a wide range of applications, from simple utilities to complex data processing tasks.

---

# 01_local-server-testing.md

File: tutorials/sdks/python/101/01_local-server-testing.md

This tutorial will guide you through creating a basic serverless function using RunPod's Python SDK.
We'll build a function that reverses a given string, demonstrating the simplicity and flexibility of RunPod's serverless architecture.

## Setting up your Serverless Function

Let's break down the process of creating our string reversal function into steps.

### Import RunPod Library

First, import the RunPod library:

```python
import runpod
```

### Define utility function

Create a utility function to reverse the string:

```python
def reverse_string(s):
    return s[::-1]
```

This function uses Python's slicing feature to efficiently reverse the input string.

### Create the Handler Function

The handler function is the core of our serverless application:

```python
def handler(job):
    print(f"string-reverser | Starting job {job['id']}")
    job_input = job["input"]

    input_string = job_input.get("text", "")

    if not input_string:
        return {"error": "No input text provided"}

    reversed_string = reverse_string(input_string)

    job_output = {"original_text": input_string, "reversed_text": reversed_string}

    return job_output
```

This handler:

1. Logs the start of each job
2. Extracts the input string from the job data
3. Validates the input
4. Reverses the string using our utility function
5. Prepares and returns the output

### Start the Serverless Function

Finally, start the RunPod serverless worker:

```python
runpod.serverless.start({"handler": handler})
```

This line registers our handler function with RunPod's serverless infrastructure.

## Complete code example

Here's the full code for our serverless string reversal function:

```python
import runpod


def reverse_string(s):
    return s[::-1]


def handler(job):
    print(f"string-reverser | Starting job {job['id']}")
    job_input = job["input"]

    input_string = job_input.get("text", "")

    if not input_string:
        return {"error": "No input text provided"}

    reversed_string = reverse_string(input_string)

    job_output = {"original_text": input_string, "reversed_text": reversed_string}

    return job_output


runpod.serverless.start({"handler": handler})
```

## Testing Your Serverless Function

RunPod provides multiple ways to test your serverless function locally before deployment. We'll explore two methods: using command-line arguments and running a local test server.

### Method 1: Command-line Testing

To quickly test your function using command-line arguments, use this command:

```bash
python your_script.py --test_input '{"input": {"text": "Hello, RunPod!"}}'
```

When you run this test, you'll see output similar to:

```plaintext
--- Starting Serverless Worker |  Version 1.6.2 ---
INFO   | test_input set, using test_input as job input.
DEBUG  | Retrieved local job: {'input': {'text': 'Hello, RunPod!'}, 'id': 'local_test'}
INFO   | local_test | Started.
string-reverser | Starting job local_test
DEBUG  | local_test | Handler output: {'original_text': 'Hello, RunPod!', 'reversed_text': '!doPnuR ,olleH'}
DEBUG  | local_test | run_job return: {'output': {'original_text': 'Hello, RunPod!', 'reversed_text': '!doPnuR ,olleH'}}
INFO   | Job local_test completed successfully.
INFO   | Job result: {'output': {'original_text': 'Hello, RunPod!', 'reversed_text': '!doPnuR ,olleH'}}
INFO   | Local testing complete, exiting.
```

This output shows the serverless worker starting, processing the job, and returning the result.

### Method 2: Local Test Server

For more comprehensive testing, especially when you want to simulate HTTP requests to your serverless function, you can launch a local test server. This server provides an endpoint that you can send requests to, mimicking the behavior of a deployed serverless function.

To start the local test server, use the `--rp_serve_api` flag:

```bash
python your_script.py --rp_serve_api
```

This command starts a FastAPI server on your local machine, accessible at `http://localhost:8000`.

#### Sending Requests to the Local Server

Once your local server is running, you can send HTTP POST requests to test your function. Use tools like `curl` or Postman, or write scripts to automate your tests.

Example using `curl`:

```bash
curl -X POST http://localhost:8000/run \
     -H "Content-Type: application/json" \
     -d '{"input": {"text": "Hello, RunPod!"}}'
```

This will send a POST request to your local server with the input data, simulating how your function would be called in a production environment.

#### Understanding the Server Output

When you send a request to the local server, you'll see output in your terminal similar to:

```plaintext
INFO:     127.0.0.1:52686 - "POST /run HTTP/1.1" 200 OK
DEBUG    | Retrieved local job: {'input': {'text': 'Hello, RunPod!'}, 'id': 'local_test'}
INFO     | local_test | Started.
string-reverser | Starting job local_test
DEBUG    | local_test | Handler output: {'original_text': 'Hello, RunPod!', 'reversed_text': '!doPnuR ,olleH'}
DEBUG    | local_test | run_job return: {'output': {'original_text': 'Hello, RunPod!', 'reversed_text': '!doPnuR ,olleH'}}
INFO     | Job local_test completed successfully.
```

This output provides detailed information about how your function processes the request, which can be invaluable for debugging and optimizing your serverless function.

## Conclusion

You've now created a basic serverless function using RunPod's Python SDK that reverses input strings and learned how to test it using both command-line arguments and a local test server. This example demonstrates how easy it is to deploy and validate simple text processing tasks as serverless functions.

To further explore RunPod's serverless capabilities, consider:

- Adding more complex string manipulations
- Implementing error handling for different input types
- Writing automated test scripts to cover various input scenarios
- Using the local server to integrate your function with other parts of your application during development
- Exploring RunPod's documentation for advanced features like concurrent processing or GPU acceleration

RunPod's serverless library provides a powerful foundation for building scalable, efficient text processing applications without the need to manage infrastructure.

---

# 02_generator.md

File: tutorials/sdks/python/101/02_generator.md

This tutorial will guide you through creating a serverless function using RunPod's Python SDK that simulates a text-to-speech (TTS) process.
We'll use a generator handler to stream results incrementally, demonstrating how to handle long-running tasks efficiently in a serverless environment.

A generator in the RunPod's Python SDK is a special type of function that allows you to iterate over a sequence of values lazily. Instead of returning a single value and exiting, a generator yields multiple values, one at a time, pausing the function's state between each yield.
This is particularly useful for handling large data streams or long-running tasks, as it allows the function to produce and return results incrementally, rather than waiting until the entire process is complete.

## Setting up your Serverless Function

Let's break down the process of creating our TTS simulator into steps.

### Import required libraries

First, import the necessary libraries:

```python
import runpod
import time
import re
import json
import sys
```

### Create the TTS Simulator

Define a function that simulates the text-to-speech process:

```python
def text_to_speech_simulator(text, chunk_size=5, delay=0.5):
    words = re.findall(r'\w+', text)
    
    for i in range(0, len(words), chunk_size):
        chunk = words[i:i+chunk_size]
        audio_chunk = f"Audio chunk {i//chunk_size + 1}: {' '.join(chunk)}"
        time.sleep(delay)  # Simulate processing time
        yield audio_chunk
```

This function:

1. Splits the input text into words
2. Processes the words in chunks
3. Simulates a delay for each chunk
4. Yields each "audio chunk" as it's processed

### Create the Generator Handler

Now, let's create the main handler function:

```python
def generator_handler(job):
    job_input = job['input']
    text = job_input.get('text', "Welcome to RunPod's text-to-speech simulator!")
    chunk_size = job_input.get('chunk_size', 5)
    delay = job_input.get('delay', 0.5)
    
    print(f"TTS Simulator | Starting job {job['id']}")
    print(f"Processing text: {text}")
    
    for audio_chunk in text_to_speech_simulator(text, chunk_size, delay):
        yield {"status": "processing", "chunk": audio_chunk}
    
    yield {"status": "completed", "message": "Text-to-speech conversion completed"}
```

This handler:

1. Extracts parameters from the job input
2. Logs the start of the job
3. Calls the TTS simulator and yields each chunk as it's processed
4. Yields a completion message when finished

### Set up the main function

Finally, set up the main execution block:

```python
if __name__ == "__main__":
    if "--test_input" in sys.argv:
        # Code for local testing (see full example)
    else:
        runpod.serverless.start({"handler": generator_handler})
```

This block allows for both local testing and deployment as a RunPod serverless function.

## Complete code example

Here's the full code for our serverless TTS simulator:

```python
import runpod
import time
import re
import json
import sys

def text_to_speech_simulator(text, chunk_size=5, delay=0.5):
    words = re.findall(r'\w+', text)
    
    for i in range(0, len(words), chunk_size):
        chunk = words[i:i+chunk_size]
        audio_chunk = f"Audio chunk {i//chunk_size + 1}: {' '.join(chunk)}"
        time.sleep(delay)  # Simulate processing time
        yield audio_chunk

def generator_handler(job):
    job_input = job['input']
    text = job_input.get('text', "Welcome to RunPod's text-to-speech simulator!")
    chunk_size = job_input.get('chunk_size', 5)
    delay = job_input.get('delay', 0.5)
    
    print(f"TTS Simulator | Starting job {job['id']}")
    print(f"Processing text: {text}")
    
    for audio_chunk in text_to_speech_simulator(text, chunk_size, delay):
        yield {"status": "processing", "chunk": audio_chunk}
    
    yield {"status": "completed", "message": "Text-to-speech conversion completed"}

if __name__ == "__main__":
    if "--test_input" in sys.argv:
        test_input_index = sys.argv.index("--test_input")
        if test_input_index + 1 < len(sys.argv):
            test_input_json = sys.argv[test_input_index + 1]
            try:
                job = json.loads(test_input_json)
                gen = generator_handler(job)
                for item in gen:
                    print(json.dumps(item))
            except json.JSONDecodeError:
                print("Error: Invalid JSON in test_input")
        else:
            print("Error: --test_input requires a JSON string argument")
    else:
        runpod.serverless.start({"handler": generator_handler})
```

## Testing your Serverless Function

To test your function locally, use this command:

```bash
python your_script.py --test_input '
{
  "input": {
    "text": "This is a test of the RunPod text-to-speech simulator. It processes text in chunks and simulates audio generation.",
    "chunk_size": 4,
    "delay": 1
  },
  "id": "local_test"
}'
```

### Understanding the output

When you run the test, you'll see output similar to this:

```json
{"status": "processing", "chunk": "Audio chunk 1: This is a test"}
{"status": "processing", "chunk": "Audio chunk 2: of the RunPod"}
{"status": "processing", "chunk": "Audio chunk 3: text to speech"}
{"status": "processing", "chunk": "Audio chunk 4: simulator It processes"}
{"status": "processing", "chunk": "Audio chunk 5: text in chunks"}
{"status": "processing", "chunk": "Audio chunk 6: and simulates audio"}
{"status": "processing", "chunk": "Audio chunk 7: generation"}
{"status": "completed", "message": "Text-to-speech conversion completed"}
```

This output demonstrates:

1. The incremental processing of text chunks
2. Real-time status updates for each chunk
3. A completion message when the entire text is processed

## Conclusion

You've now created a serverless function using RunPod's Python SDK that simulates a streaming text-to-speech process. This example showcases how to handle long-running tasks and stream results incrementally in a serverless environment.

To further enhance this application, consider:

- Implementing a real text-to-speech model
- Adding error handling for various input types
- Exploring RunPod's documentation for advanced features like GPU acceleration for audio processing

RunPod's serverless library provides a powerful foundation for building scalable, efficient applications that can process and stream data in real-time without the need to manage infrastructure.

---

# 03_async.md

File: tutorials/sdks/python/101/03_async.md

This tutorial will guide you through creating a serverless function using RunPod's Python SDK that simulates fetching weather data for multiple cities concurrently.

Use asynchronous functions to handle multiple concurrent operations efficiently, especially when dealing with tasks that involve waiting for external resources, such as network requests or I/O operations.
Asynchronous programming allows your code to perform other tasks while waiting, rather than blocking the entire program. This is particularly useful in a serverless environment where you want to maximize resource utilization and minimize response times.

We'll use an async generator handler to stream results incrementally, demonstrating how to manage multiple concurrent operations efficiently in a serverless environment.

## Setting up your Serverless Function

Let's break down the process of creating our weather data simulator into steps.

### SImport required libraries

First, import the necessary libraries:

```python
import runpod
import asyncio
import random
import json
import sys
```

### Create the Weather Data Fetcher

Define an asynchronous function that simulates fetching weather data:

```python
async def fetch_weather_data(city, delay):
    await asyncio.sleep(delay)
    temperature = random.uniform(-10, 40)
    humidity = random.uniform(0, 100)
    return {
        "city": city,
        "temperature": round(temperature, 1),
        "humidity": round(humidity, 1)
    }
```

This function:

1. Simulates a network delay using `asyncio.sleep()`
2. Generates random temperature and humidity data
3. Returns a dictionary with the weather data for a city

### Create the Async Generator Handler

Now, let's create the main handler function:

```python
async def async_generator_handler(job):
    job_input = job['input']
    cities = job_input.get('cities', ['New York', 'London', 'Tokyo', 'Sydney', 'Moscow'])
    update_interval = job_input.get('update_interval', 2)
    duration = job_input.get('duration', 10)

    print(f"Weather Data Stream | Starting job {job['id']}")
    print(f"Monitoring cities: {', '.join(cities)}")

    start_time = asyncio.get_event_loop().time()

    while asyncio.get_event_loop().time() - start_time < duration:
        tasks = [fetch_weather_data(city, random.uniform(0.5, 2)) for city in cities]
        for completed_task in asyncio.as_completed(tasks):
            weather_data = await completed_task
            yield {
                "timestamp": round(asyncio.get_event_loop().time() - start_time, 2),
                "data": weather_data
            }
        
        await asyncio.sleep(update_interval)

    yield {"status": "completed", "message": "Weather monitoring completed"}
```

This handler:

1. Extracts parameters from the job input
2. Logs the start of the job
3. Creates tasks for fetching weather data for each city
4. Uses `asyncio.as_completed()` to yield results as they become available
5. Continues fetching data at specified intervals for the given duration

### Set up the Main Execution

Finally, Set up the main execution block:

```python
async def run_test(job):
    async for item in async_generator_handler(job):
        print(json.dumps(item))

if __name__ == "__main__":
    if "--test_input" in sys.argv:
        # Code for local testing (see full example)
    else:
        runpod.serverless.start({
            "handler": async_generator_handler,
            "return_aggregate_stream": True
        })
```

This block allows for both local testing and deployment as a RunPod serverless function.

## Complete code example

Here's the full code for our serverless weather data simulator:

```python
import runpod
import asyncio
import random
import json
import sys

async def fetch_weather_data(city, delay):
    await asyncio.sleep(delay)
    temperature = random.uniform(-10, 40)
    humidity = random.uniform(0, 100)
    return {
        "city": city,
        "temperature": round(temperature, 1),
        "humidity": round(humidity, 1)
    }

async def async_generator_handler(job):
    job_input = job['input']
    cities = job_input.get('cities', ['New York', 'London', 'Tokyo', 'Sydney', 'Moscow'])
    update_interval = job_input.get('update_interval', 2)
    duration = job_input.get('duration', 10)

    print(f"Weather Data Stream | Starting job {job['id']}")
    print(f"Monitoring cities: {', '.join(cities)}")

    start_time = asyncio.get_event_loop().time()

    while asyncio.get_event_loop().time() - start_time < duration:
        tasks = [fetch_weather_data(city, random.uniform(0.5, 2)) for city in cities]
        for completed_task in asyncio.as_completed(tasks):
            weather_data = await completed_task
            yield {
                "timestamp": round(asyncio.get_event_loop().time() - start_time, 2),
                "data": weather_data
            }
        
        await asyncio.sleep(update_interval)

    yield {"status": "completed", "message": "Weather monitoring completed"}

async def run_test(job):
    async for item in async_generator_handler(job):
        print(json.dumps(item))

if __name__ == "__main__":
    if "--test_input" in sys.argv:
        test_input_index = sys.argv.index("--test_input")
        if test_input_index + 1 < len(sys.argv):
            test_input_json = sys.argv[test_input_index + 1]
            try:
                job = json.loads(test_input_json)
                asyncio.run(run_test(job))
            except json.JSONDecodeError:
                print("Error: Invalid JSON in test_input")
        else:
            print("Error: --test_input requires a JSON string argument")
    else:
        runpod.serverless.start({
            "handler": async_generator_handler,
            "return_aggregate_stream": True
        })
```

## Testing Your Serverless Function

To test your function locally, use this command:

```bash
python your_script.py --test_input '
{
  "input": {
    "cities": ["New York", "London", "Tokyo", "Paris", "Sydney"],
    "update_interval": 3,
    "duration": 15
  },
  "id": "local_test"
}'
```

### Understanding the output

When you run the test, you'll see output similar to this:

```
Weather Data Stream | Starting job local_test
Monitoring cities: New York, London, Tokyo, Paris, Sydney
{"timestamp": 0.84, "data": {"city": "London", "temperature": 11.0, "humidity": 7.3}}
{"timestamp": 0.99, "data": {"city": "Paris", "temperature": -5.9, "humidity": 59.3}}
{"timestamp": 1.75, "data": {"city": "Tokyo", "temperature": 18.4, "humidity": 34.1}}
{"timestamp": 1.8, "data": {"city": "Sydney", "temperature": 26.8, "humidity": 91.0}}
{"timestamp": 1.99, "data": {"city": "New York", "temperature": 35.9, "humidity": 27.5}}
{"status": "completed", "message": "Weather monitoring completed"}
```

This output demonstrates:

1. The concurrent processing of weather data for multiple cities
2. Real-time updates with timestamps
3. A completion message when the monitoring duration is reached

## Conclusion

You've now created a serverless function using RunPod's Python SDK that simulates concurrent weather data fetching for multiple cities. This example showcases how to handle multiple asynchronous operations and stream results incrementally in a serverless environment.

To further enhance this application, consider:

- Implementing real API calls to fetch actual weather data
- Adding error handling for network failures or API limits
- Exploring RunPod's documentation for advanced features like scaling for high-concurrency scenarios

RunPod's serverless library provides a powerful foundation for building scalable, efficient applications that can process and stream data concurrently in real-time without the need to manage infrastructure.

---

# 04_error.md

File: tutorials/sdks/python/101/04_error.md

This tutorial will guide you through implementing effective error handling and logging in your RunPod serverless functions.

Proper error handling ensures that your serverless functions can handle unexpected situations gracefully. This prevents crashes and ensures that your application can continue running smoothly, even if some parts encounter issues.

We'll create a simulated image classification model to demonstrate these crucial practices, ensuring your serverless deployments are robust and maintainable.

## Setting up your Serverless Function

Let's break down the process of creating our error-aware image classifier into steps.

### Import required libraries and Set Up Logging

First, import the necessary libraries and Set up the RunPod logger:

```python
import runpod
from runpod import RunPodLogger
import time
import random

log = RunPodLogger()
```

### Create Helper Functions

Define functions to simulate various parts of the image classification process:

```python
def load_model():
    """Simulate loading a machine learning model."""
    log.info("Loading image classification model...")
    time.sleep(2)  # Simulate model loading time
    return "ImageClassifier"


def preprocess_image(image_url):
    """Simulate image preprocessing."""
    log.debug(f"Preprocessing image: {image_url}")
    time.sleep(0.5)  # Simulate preprocessing time
    return f"Preprocessed_{image_url}"


def classify_image(model, preprocessed_image):
    """Simulate image classification."""
    classes = ["cat", "dog", "bird", "fish", "horse"]
    confidence = random.uniform(0.7, 0.99)
    predicted_class = random.choice(classes)
    return predicted_class, confidence
```

These functions:

1. Simulate model loading, logging the process
2. Preprocess images, with debug logging
3. Classify images, returning random results for demonstration

### Create the Main Handler Function

Now, let's create the main handler function with error handling and logging:

```python
def handler(job):
    job_input = job["input"]
    images = job_input.get("images", [])

    # Process mock logs if provided
    for job_log in job_input.get("mock_logs", []):
        log_level = job_log.get("level", "info").lower()
        if log_level == "debug":
            log.debug(job_log["message"])
        elif log_level == "info":
            log.info(job_log["message"])
        elif log_level == "warn":
            log.warn(job_log["message"])
        elif log_level == "error":
            log.error(job_log["message"])

    try:
        # Load model
        model = load_model()
        log.info("Model loaded successfully")

        results = []
        for i, image_url in enumerate(images):
            # Preprocess image
            preprocessed_image = preprocess_image(image_url)

            # Classify image
            predicted_class, confidence = classify_image(model, preprocessed_image)

            result = {
                "image": image_url,
                "predicted_class": predicted_class,
                "confidence": round(confidence, 2),
            }
            results.append(result)

            # Log progress
            progress = (i + 1) / len(images) * 100
            log.info(f"Progress: {progress:.2f}%")

            # Simulate some processing time
            time.sleep(random.uniform(0.5, 1.5))

        log.info("Classification completed successfully")

        # Simulate error if mock_error is True
        if job_input.get("mock_error", False):
            raise Exception("Mock error")

        return {"status": "success", "results": results}

    except Exception as e:
        log.error(f"An error occurred: {str(e)}")
        return {"status": "error", "message": str(e)}
```

This handler:

1. Processes mock logs to demonstrate different logging levels
2. Uses a try-except block to handle potential errors
3. Simulates image classification with progress logging
4. Returns results or an error message based on the execution

### Start the Serverless Function

Finally, start the RunPod serverless function:

```python
runpod.serverless.start({"handler": handler})
```

## Complete code example

Here's the full code for our error-aware image classification simulator:

```python
import runpod
from runpod import RunPodLogger
import time
import random

log = RunPodLogger()


def load_model():
    """Simulate loading a machine learning model."""
    log.info("Loading image classification model...")
    time.sleep(2)  # Simulate model loading time
    return "ImageClassifier"


def preprocess_image(image_url):
    """Simulate image preprocessing."""
    log.debug(f"Preprocessing image: {image_url}")
    time.sleep(0.5)  # Simulate preprocessing time
    return f"Preprocessed_{image_url}"


def classify_image(model, preprocessed_image):
    """Simulate image classification."""
    classes = ["cat", "dog", "bird", "fish", "horse"]
    confidence = random.uniform(0.7, 0.99)
    predicted_class = random.choice(classes)
    return predicted_class, confidence


def handler(job):
    job_input = job["input"]
    images = job_input.get("images", [])

    # Process mock logs if provided
    for job_log in job_input.get("mock_logs", []):
        log_level = job_log.get("level", "info").lower()
        if log_level == "debug":
            log.debug(job_log["message"])
        elif log_level == "info":
            log.info(job_log["message"])
        elif log_level == "warn":
            log.warn(job_log["message"])
        elif log_level == "error":
            log.error(job_log["message"])

    try:
        # Load model
        model = load_model()
        log.info("Model loaded successfully")

        results = []
        for i, image_url in enumerate(images):
            # Preprocess image
            preprocessed_image = preprocess_image(image_url)

            # Classify image
            predicted_class, confidence = classify_image(model, preprocessed_image)

            result = {
                "image": image_url,
                "predicted_class": predicted_class,
                "confidence": round(confidence, 2),
            }
            results.append(result)

            # Log progress
            progress = (i + 1) / len(images) * 100
            log.info(f"Progress: {progress:.2f}%")

            # Simulate some processing time
            time.sleep(random.uniform(0.5, 1.5))

        log.info("Classification completed successfully")

        # Simulate error if mock_error is True
        if job_input.get("mock_error", False):
            raise Exception("Mock error")

        return {"status": "success", "results": results}

    except Exception as e:
        log.error(f"An error occurred: {str(e)}")
        return {"status": "error", "message": str(e)}


runpod.serverless.start({"handler": handler})
```

## Testing Your Serverless Function

To test your function locally, use this command:

```bash
python your_script.py --test_input '{
    "input": {
        "images": ["image1.jpg", "image2.jpg", "image3.jpg"],
        "mock_logs": [
            {"level": "info", "message": "Starting job"},
            {"level": "debug", "message": "Debug information"},
            {"level": "warn", "message": "Warning: low disk space"},
            {"level": "error", "message": "Error: network timeout"}
        ],
        "mock_error": false
    }
}'
```

### Understanding the output

When you run the test, you'll see output similar to this:

```json
{
  "status": "success",
  "results": [
    {
      "image": "image1.jpg",
      "predicted_class": "cat",
      "confidence": 0.85
    },
    {
      "image": "image2.jpg",
      "predicted_class": "dog",
      "confidence": 0.92
    },
    {
      "image": "image3.jpg",
      "predicted_class": "bird",
      "confidence": 0.78
    }
  ]
}
```

This output demonstrates:

1. Successful processing of all images
2. Random classification results for each image
3. The overall success status of the job

## Conclusion

You've now created a serverless function using RunPod's Python SDK that demonstrates effective error handling and logging practices. This approach ensures that your serverless functions are robust, maintainable, and easier to debug.

To further enhance this application, consider:

- Implementing more specific error types and handling
- Adding more detailed logging for each step of the process
- Exploring RunPod's advanced logging features and integrations

RunPod's serverless library provides a powerful foundation for building reliable, scalable applications with comprehensive error management and logging capabilities.

---

# 05_aggregate.md

File: tutorials/sdks/python/101/05_aggregate.md

This tutorial will guide you through using the `return_aggregate_stream` feature in RunPod to simplify result handling in your serverless functions.
Using `return_aggregate_stream` allows you to automatically collect and aggregate all yielded results from a generator handler into a single response.
This simplifies result handling, making it easier to manage and return a consolidated set of results from asynchronous tasks, such as concurrent sentiment analysis or object detection, without needing additional code to collect and format the results manually.

We'll create a multi-purpose analyzer that can perform sentiment analysis on text and object detection in images, demonstrating how to aggregate outputs efficiently.

## Setting up your Serverless Function

Let's break down the process of creating our multi-purpose analyzer into steps.

### Import required libraries

First, import the necessary libraries:

```python
import runpod
import time
import random
```

### Create Helper Functions

Define functions to simulate sentiment analysis and object detection:

```python
def analyze_sentiment(text):
    """Simulate sentiment analysis of text."""
    sentiments = ["Positive", "Neutral", "Negative"]
    score = random.uniform(-1, 1)
    sentiment = random.choice(sentiments)
    return f"Sentiment: {sentiment}, Score: {score:.2f}"


def detect_objects(image_url):
    """Simulate object detection in an image."""
    objects = ["person", "car", "dog", "cat", "tree", "building"]
    detected = random.sample(objects, random.randint(1, 4))
    confidences = [random.uniform(0.7, 0.99) for _ in detected]
    return [f"{obj}: {conf:.2f}" for obj, conf in zip(detected, confidences)]
```

These functions:

1. Simulate sentiment analysis, returning a random sentiment and score
2. Simulate object detection, returning a list of detected objects with confidence scores

### Create the main Handler Function

Now, let's create the main handler function that processes jobs and yields results:

```python
def handler(job):
    job_input = job["input"]
    task_type = job_input.get("task_type", "sentiment")
    items = job_input.get("items", [])

    results = []
    for item in items:
        time.sleep(random.uniform(0.5, 2))  # Simulate processing time

        if task_type == "sentiment":
            result = analyze_sentiment(item)
        elif task_type == "object_detection":
            result = detect_objects(item)
        else:
            result = f"Unknown task type: {task_type}"

        results.append(result)
        yield result

    return results
```

This handler:

1. Determines the task type (sentiment analysis or object detection)
2. Processes each item in the input
3. Yields results incrementally
4. Returns the complete list of results

### Set up the Serverless Function starter

Create a function to start the serverless handler with proper configuration:

```python
def start_handler():
    def wrapper(job):
        generator = handler(job)
        if job.get("id") == "local_test":
            return list(generator)
        return generator

    runpod.serverless.start({"handler": wrapper, "return_aggregate_stream": True})


if __name__ == "__main__":
    start_handler()
```

This setup:

1. Creates a wrapper to handle both local testing and RunPod environments
2. Uses `return_aggregate_stream=True` to automatically aggregate yielded results

## Complete code example

Here's the full code for our multi-purpose analyzer with output aggregation:

```python
import runpod
import time
import random


def analyze_sentiment(text):
    """Simulate sentiment analysis of text."""
    sentiments = ["Positive", "Neutral", "Negative"]
    score = random.uniform(-1, 1)
    sentiment = random.choice(sentiments)
    return f"Sentiment: {sentiment}, Score: {score:.2f}"


def detect_objects(image_url):
    """Simulate object detection in an image."""
    objects = ["person", "car", "dog", "cat", "tree", "building"]
    detected = random.sample(objects, random.randint(1, 4))
    confidences = [random.uniform(0.7, 0.99) for _ in detected]
    return [f"{obj}: {conf:.2f}" for obj, conf in zip(detected, confidences)]


def handler(job):
    job_input = job["input"]
    task_type = job_input.get("task_type", "sentiment")
    items = job_input.get("items", [])

    results = []
    for item in items:
        time.sleep(random.uniform(0.5, 2))  # Simulate processing time

        if task_type == "sentiment":
            result = analyze_sentiment(item)
        elif task_type == "object_detection":
            result = detect_objects(item)
        else:
            result = f"Unknown task type: {task_type}"

        results.append(result)
        yield result

    return results


def start_handler():
    def wrapper(job):
        generator = handler(job)
        if job.get("id") == "local_test":
            return list(generator)
        return generator

    runpod.serverless.start({"handler": wrapper, "return_aggregate_stream": True})


if __name__ == "__main__":
    start_handler()
```

## Testing your Serverless Function

To test your function locally, use these commands:

For sentiment analysis:

```bash
python your_script.py --test_input '
{
  "input": {
    "task_type": "sentiment",
    "items": [
      "I love this product!",
      "The service was terrible.",
      "It was okay, nothing special."
    ]
  }
}'
```

For object detection:

```bash
python your_script.py --test_input '
{
  "input": {
    "task_type": "object_detection",
    "items": [
      "image1.jpg",
      "image2.jpg",
      "image3.jpg"
    ]
  }
}'
```

### Understanding the output

When you run the sentiment analysis test, you'll see output similar to this:

```
--- Starting Serverless Worker |  Version 1.6.2 ---
INFO   | test_input set, using test_input as job input.
DEBUG  | Retrieved local job: {'input': {'task_type': 'sentiment', 'items': ['I love this product!', 'The service was terrible.', 'It was okay, nothing special.']}, 'id': 'local_test'}
INFO   | local_test | Started.
DEBUG  | local_test | Handler output: ['Sentiment: Positive, Score: 0.85', 'Sentiment: Negative, Score: -0.72', 'Sentiment: Neutral, Score: 0.12']
DEBUG  | local_test | run_job return: {'output': ['Sentiment: Positive, Score: 0.85', 'Sentiment: Negative, Score: -0.72', 'Sentiment: Neutral, Score: 0.12']}
INFO   | Job local_test completed successfully.
INFO   | Job result: {'output': ['Sentiment: Positive, Score: 0.85', 'Sentiment: Negative, Score: -0.72', 'Sentiment: Neutral, Score: 0.12']}
INFO   | Local testing complete, exiting.
```

This output demonstrates:

1. The serverless worker starting and processing the job
2. The handler generating results for each input item
3. The aggregation of results into a single list

## Conclusion

You've now created a serverless function using RunPod's Python SDK that demonstrates efficient output aggregation for both local testing and production environments. This approach simplifies result handling and ensures consistent behavior across different execution contexts.

To further enhance this application, consider:

- Implementing real sentiment analysis and object detection models
- Adding error handling and logging for each processing step
- Exploring RunPod's advanced features for handling larger datasets or parallel processing

RunPod's serverless library, with features like `return_aggregate_stream`, provides a powerful foundation for building scalable, efficient applications that can process and aggregate data seamlessly.

---

# 07-huggingface-models.md

File: tutorials/sdks/python/102/07-huggingface-models.md

Artificial Intelligence (AI) has revolutionized how applications analyze and interact with data. One powerful aspect of AI is sentiment analysis, which allows machines to interpret and categorize emotions expressed in text. In this tutorial, you will learn how to integrate pre-trained Hugging Face models into your RunPod Serverless applications to perform sentiment analysis. By the end of this guide, you will have a fully functional AI-powered sentiment analysis function running in a serverless environment.

### Install Required Libraries

To begin, we need to install the necessary Python libraries.
Hugging Face's `transformers` library provides state-of-the-art machine learning models, while the `torch` library supports these models.

Execute the following command in your terminal to install the required libraries:

```command
pip install torch transformers
```

This command installs the `torch` and `transformers` libraries. `torch` is used for creating and running models, and `transformers` provides pre-trained models.

### Import libraries

Next, we need to import the libraries into our Python script. Create a new Python file named `sentiment_analysis.py` and include the following import statements:

```python
import runpod
from transformers import pipeline
```

These imports bring in the `runpod` SDK for serverless functions and the `pipeline` method from `transformers`, which allows us to use pre-trained models.

### Load the Model

Loading the model in a function ensures that the model is only loaded once when the worker starts, optimizing the performance of our application. Add the following code to your `sentiment_analysis.py` file:

```python
def load_model():
    return pipeline(
        "sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english"
    )
```

In this function, we use the `pipeline` method from `transformers` to load a pre-trained sentiment analysis model. The `distilbert-base-uncased-finetuned-sst-2-english` model is a distilled version of BERT fine-tuned for sentiment analysis tasks.

### Define the Handler Function

We will now define the handler function that will process incoming events and use the model for sentiment analysis. Add the following code to your script:

```python
def sentiment_analysis_handler(event):
    global model

    # Ensure the model is loaded
    if "model" not in globals():
        model = load_model()

    # Get the input text from the event
    text = event["input"].get("text")

    # Validate input
    if not text:
        return {"error": "No text provided for analysis."}

    # Perform sentiment analysis
    result = model(text)[0]

    return {"sentiment": result["label"], "score": float(result["score"])}
```

This function performs the following steps:

1. Ensures the model is loaded.
2. Retrieves the input text from the incoming event.
3. Validates the input to ensure text is provided.
4. Uses the loaded model to perform sentiment analysis.
5. Returns the sentiment label and score as a dictionary.

### Start the Serverless Worker

To run our sentiment analysis function as a serverless worker, we need to start the worker using RunPod's SDK. Add the following line at the end of your `sentiment_analysis.py` file:

```python
runpod.serverless.start({"handler": sentiment_analysis_handler})
```

This command starts the serverless worker and specifies `sentiment_analysis_handler` as the handler function for incoming requests.

### Complete Code

Here is the complete code for our sentiment analysis serverless function:

```python
import runpod
from transformers import pipeline


def load_model():
    return pipeline(
        "sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english"
    )


def sentiment_analysis_handler(event):
    global model

    if "model" not in globals():
        model = load_model()

    text = event["input"].get("text")

    if not text:
        return {"error": "No text provided for analysis."}

    result = model(text)[0]

    return {"sentiment": result["label"], "score": float(result["score"])}


runpod.serverless.start({"handler": sentiment_analysis_handler})
```

### Testing Locally

To test this function locally, create a file named `test_input.json` with the following content:

```json
{
  "input": {
    "text": "I love using RunPod for serverless machine learning!"
  }
}
```

Run the following command in your terminal to test the function:

```command
python sentiment_analysis.py --rp_server_api
```

You should see output similar to the following, indicating that the sentiment analysis function is working correctly:

```
--- Starting Serverless Worker |  Version 1.6.2 ---
INFO   | Using test_input.json as job input.
DEBUG  | Retrieved local job: {'input': {'text': 'I love using RunPod for serverless machine learning!'}, 'id': 'local_test'}
INFO   | local_test | Started.
model.safetensors: 100%|█████████████████████████| 268M/268M [00:02<00:00, 94.9MB/s]
tokenizer_config.json: 100%|██████████████████████| 48.0/48.0 [00:00<00:00, 631kB/s]
vocab.txt: 100%|█████████████████████████████████| 232k/232k [00:00<00:00, 1.86MB/s]
Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.
DEBUG  | local_test | Handler output: {'sentiment': 'POSITIVE', 'score': 0.9889019727706909}
DEBUG  | local_test | run_job return: {'output': {'sentiment': 'POSITIVE', 'score': 0.9889019727706909}}
INFO   | Job local_test completed successfully.
INFO   | Job result: {'output': {'sentiment': 'POSITIVE', 'score': 0.9889019727706909}}
INFO   | Local testing complete, exiting.
```

## Conclusion

In this tutorial, you learned how to integrate a pre-trained Hugging Face model into a RunPod serverless function to perform sentiment analysis on text input.

This powerful combination enables you to create advanced AI applications in a serverless environment.

You can extend this concept to use more complex models or perform different types of inference tasks as needed.

In our final lesson, we will explore a more complex AI task: text-to-image generation.

---

# 08-stable-diffusion-text-to-image.md

File: tutorials/sdks/python/102/08-stable-diffusion-text-to-image.md

Text-to-image generation using advanced AI models offers a unique way to bring textual descriptions to life as images.
Stable Diffusion is a powerful model capable of generating high-quality images from text inputs, and RunPod is a serverless computing platform that can manage resource-intensive tasks effectively.
This tutorial will guide you through setting up a serverless application that utilizes Stable Diffusion for generating images from text prompts on RunPod.

By the end of this guide, you will have a fully functional text-to-image generation system deployed on a RunPod serverless environment.

## Prerequisites

Before diving into the setup, ensure you have the following:

- Access to a RunPod account
- A GPU instance configured on RunPod
- Basic knowledge of Python programming

## Import required libraries

To start, we need to import several essential libraries. These will provide the functionalities required for serverless operation and image generation.

```python
import runpod
import torch
from diffusers import StableDiffusionPipeline
from io import BytesIO
import base64
```

Here’s a breakdown of the imports:

- `runpod`: The SDK used to interact with RunPod's serverless environment.
- `torch`: PyTorch library, necessary for running deep learning models and ensuring they utilize the GPU.
- `diffusers`: Provides methods to work with diffusion models like Stable Diffusion.
- `BytesIO` and `base64`: Used to handle image data conversions.

Next, confirm that CUDA is available, as the model requires a GPU to function efficiently.

```python
assert (
    torch.cuda.is_available()
), "CUDA is not available. Make sure you have a GPU instance."
```

This assertion checks whether a compatible NVIDIA GPU is available for PyTorch to use.

## Load the Stable Diffusion Model

We'll load the Stable Diffusion model in a separate function. This ensures that the model is only loaded once when the worker process starts, which is more efficient.

```python
def load_model():
    model_id = "runwayml/stable-diffusion-v1-5"
    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
    pipe = pipe.to("cuda")
    return pipe
```

Here's what this function does:

- `model_id` specifies the model identifier for Stable Diffusion version 1.5.
- `StableDiffusionPipeline.from_pretrained` loads the model weights into memory with a specified tensor type.
- `pipe.to("cuda")` moves the model to the GPU for faster computation.

## Define Helper Functions

We need a helper function to convert the generated image into a base64 string. This encoding allows the image to be easily transmitted over the web in textual form.

```python
def image_to_base64(image):
    buffered = BytesIO()
    image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")
```

Explanation:

- `BytesIO`: Creates an in-memory binary stream to which the image is saved.
- `base64.b64encode`: Encodes the binary data to a base64 format, which is then decoded to a UTF-8 string.

## Define the Handler Function

The handler function will be responsible for managing image generation requests. It includes loading the model (if not already loaded), validating inputs, generating images, and converting them to base64 strings.

```python
def stable_diffusion_handler(event):
    global model

    # Ensure the model is loaded
    if "model" not in globals():
        model = load_model()

    # Get the input prompt from the event
    prompt = event["input"].get("prompt")

    # Validate input
    if not prompt:
        return {"error": "No prompt provided for image generation."}

    try:
        # Generate the image
        image = model(prompt).images[0]

        # Convert the image to base64
        image_base64 = image_to_base64(image)

        return {"image": image_base64, "prompt": prompt}

    except Exception as e:
        return {"error": str(e)}
```

Key steps in the function:

- Checks if the model is loaded globally, and loads it if not.
- Extracts the `prompt` from the input event.
- Validates that a prompt has been provided.
- Uses the `model` to generate an image.
- Converts the image to base64 and prepares the response.

## Start the Serverless Worker

Now, we'll start the serverless worker using the RunPod SDK.

```python
runpod.serverless.start({"handler": stable_diffusion_handler})
```

This command starts the serverless worker and specifies the `stable_diffusion_handler` function to handle incoming requests.

## Complete Code

For your convenience, here is the entire code consolidated:

```python
import runpod
import torch
from diffusers import StableDiffusionPipeline
from io import BytesIO
import base64

assert (
    torch.cuda.is_available()
), "CUDA is not available. Make sure you have a GPU instance."


def load_model():
    model_id = "runwayml/stable-diffusion-v1-5"
    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
    # to run on cpu change `cuda` to `cpu`
    pipe = pipe.to("cuda")
    return pipe


def image_to_base64(image):
    buffered = BytesIO()
    image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")


def stable_diffusion_handler(event):
    global model

    if "model" not in globals():
        model = load_model()

    prompt = event["input"].get("prompt")

    if not prompt:
        return {"error": "No prompt provided for image generation."}

    try:
        image = model(prompt).images[0]
        image_base64 = image_to_base64(image)

        return {"image": image_base64, "prompt": prompt}

    except Exception as e:
        return {"error": str(e)}


runpod.serverless.start({"handler": stable_diffusion_handler})
```

## Testing Locally

Before deploying on RunPod, you might want to test the script locally. Create a `test_input.json` file with the following content:

```json
{
  "input": {
    "prompt": "A serene landscape with mountains and a lake at sunset"
  }
}
```

Run the script with the following command:

```
python stable_diffusion.py --rp_server_api
```

Note: Local testing may not work optimally without a suitable GPU. If issues arise, proceed to deploy and test on RunPod.

## Important Notes:

1. This example requires significant computational resources, particularly GPU memory. Ensure your RunPod configuration has sufficient GPU capabilities.
2. The model is loaded only once when the worker starts, optimizing performance.
3. We've used Stable Diffusion v1.5; you can replace it with other versions or models as required.
4. The handler includes error handling for missing input and exceptions during processing.
5. Ensure necessary dependencies (like `torch`, `diffusers`) are included in your environment or requirements file when deploying.
6. The generated image is returned as a base64-encoded string. For practical applications, consider saving it to a file or cloud storage.

### Conclusion

In this tutorial, you learned how to use the RunPod serverless platform with Stable Diffusion to create a text-to-image generation system.
This project showcases the potential for deploying resource-intensive AI models in a serverless architecture using the RunPod Python SDK.
You now have the skills to create and deploy sophisticated AI applications on RunPod. What will you create next?

---

# _handler-basics.md

File: tutorials/sdks/python/102/_handler-basics.md

This guide will review the things you learned in the 101 courses.

This is brief.

In this guide, we'll show you how to run your RunPod serverless applications right on your own machine.
We'll cover both synchronous and asynchronous handlers.

By the end of this, you'll be a pro at testing, debugging, and making sure your serverless code is ship-shape, all from the comfort of your local environment.

### Setting up your Handler Functions

Let's set up both a synchronous and an asynchronous handler function. We'll create two separate Python files for this.

1. **Synchronous Handler:**

   Create a file named `sync_handler.py` with the following content:

   ```python
   import runpod

   def custom_name(job):
       for count in range(3):
           result = f"This is the {count} generated output."
           yield result

   runpod.serverless.start(
       {
           "handler": custom_name  # Required
       }
   )
   ```

2. **Asynchronous Handler:**

   Create another file named `async_handler.py` with this content:

   ```python
   import runpod
   import asyncio

   async def async_generator_handler(job):
       for i in range(5):
           output = f"Generated async token output {i}"
           yield output

           await asyncio.sleep(1)

   runpod.serverless.start(
       {
           "handler": async_generator_handler,  # Required: Specify the async handler
           "return_aggregate_stream": True,  # Optional: Aggregate results are accessible via /run endpoint
       }
   )
   ```

### Running Your Code Locally

Now, let's walk through how to run these serverless functions locally using the RunPod Python SDK.

#### Using a JSON file

First, we'll create a JSON file to feed into our functions. This mimics how your function would get data in the real cloud environment.

1. **Create a JSON file:**

   Open up your favorite text editor and create a new file called `test_input.json`. Pop this content into it:

   ```json
   {
     "input": {
       "dummy": "data"
     }
   }
   ```

   This JSON is just a placeholder since our handlers don't actually use any specific input data.

2. **Run the synchronous handler:**

   Fire up your `sync_handler.py` script. We'll use the `--rp_server_api` flag to pretend we're in a serverless environment:

   ```python
   python sync_handler.py --rp_server_api
   ```

   You should see output showing the three generated results.

3. **Run the asynchronous handler:**

   Now, let's run our `async_handler.py` script:

   ```python
   python async_handler.py --rp_server_api
   ```

   You should see output showing the five generated results, with a one-second delay between each.

#### Using inline JSON

If you prefer, you can also feed your function data directly from the command line. Here's how:

1. **Run the synchronous handler with inline JSON:**

   ```python
   python sync_handler.py --test_input '{"input": {"dummy": "data"}}'
   ```

2. **Run the asynchronous handler with inline JSON:**

   ```python
   python async_handler.py --test_input '{"input": {"dummy": "data"}}'
   ```

This does the same thing as using the JSON file, but now you're passing the input data right in the command.

### Understanding the output

When you run these handlers, you'll see output similar to this:

```python
--- Starting Serverless Worker |  Version 1.6.2 ---
INFO   | Using test_input.json as job input.
DEBUG  | Retrieved local job: {'input': {'dummy': 'data'}, 'id': 'local_test'}
INFO   | local_test | Started.
DEBUG  | local_test | Handler output: This is the 0 generated output.
DEBUG  | local_test | Handler output: This is the 1 generated output.
DEBUG  | local_test | Handler output: This is the 2 generated output.
DEBUG  | local_test | run_job return: {'output': ['This is the 0 generated output.', 'This is the 1 generated output.', 'This is the 2 generated output.']}
INFO   | Job local_test completed successfully.
INFO   | Job result: {'output': ['This is the 0 generated output.', 'This is the 1 generated output.', 'This is the 2 generated output.']}
INFO   | Local testing complete, exiting.
```

For the asynchronous handler, you'll see similar output but with five results and a delay between each.

### Key Takeaways

- You can test both synchronous and asynchronous RunPod serverless functions right on your own computer.
- Synchronous handlers are great for quick, straightforward tasks.
- Asynchronous handlers allow you to simulate long-running tasks and test streaming outputs.
- You can feed your function data using a JSON file or by passing it directly in the command line.
- Local testing lets you catch and fix issues quickly, without having to deploy to the cloud each time.

And there you have it! You've successfully run both synchronous and asynchronous serverless applications using the RunPod Python SDK. This local testing is like having a practice arena where you can try out your code, fix any issues, and make sure everything's working perfectly before you send it off to the cloud.

Next up, we'll take a deeper dive into more advanced handler techniques. This will help you create even more powerful and complex serverless functions. Stay tuned!

---

# _random-number-generator.md

File: tutorials/sdks/python/102/_random-number-generator.md

Understanding how to create simple serverless applications can unlock many possibilities in modern software development. In this tutorial, we will build a random number generator using the RunPod Python SDK. This project will introduce you to input validation, error handling, and generating efficient output, reinforcing essential concepts for more advanced AI tasks.

When you're finished, you'll know how to set up a basic serverless application with RunPod that generates random numbers within a specified range. You will also understand the importance of input validation and basic error handling.

## Import required libraries

To get started, we need to import the necessary libraries that will help us build our random number generator. Run the following code to import these libraries:

```python
import runpod
import random
```

## Define the Handler Function

The handler function is the core of our serverless application. It processes input, performs actions, and returns output. In this case, it will generate a random number within a specified range.

```python
def random_number_handler(job):
    job_input = job["input"]

    # Extract the minimum and maximum values from the input
    min_value = job_input.get("min", 1)
    max_value = job_input.get("max", 100)

    # Validate input
    if not isinstance(min_value, int) or not isinstance(max_value, int):
        return {"error": "Both min and max must be integers."}

    if min_value >= max_value:
        return {"error": "min must be less than max."}

    # Generate the random number
    random_num = random.randint(min_value, max_value)

    return {"random_number": random_num}
```

Explanation:

- `job["input"]`: Retrieves the input data from the job.
- `get("min", 1)` and `get("max", 100)`: Extract minimum and maximum values from the input; set default values if not present.
- Input validation ensures `min_value` and `max_value` are integers and `min_value` is less than `max_value`.
- `random.randint(min_value, max_value)`: Generates a random number within the specified range and returns it.

You have now created the handler function that will process input and generate the random number. Let's move on to starting the serverless worker.

## Start the Serverless Worker

To start the serverless worker, which will run the handler function, use the `runpod.serverless.start` method:

```python
runpod.serverless.start({"handler": random_number_handler})
```

Now your serverless worker is ready to process jobs and generate random numbers.

## Complete code example

Here is the complete code for the Random Number Generator:

```python
import runpod
import random


def random_number_handler(job):
    job_input = job["input"]

    min_value = job_input.get("min", 1)
    max_value = job_input.get("max", 100)

    if not isinstance(min_value, int) or not isinstance(max_value, int):
        return {"error": "Both min and max must be integers."}

    if min_value >= max_value:
        return {"error": "min must be less than max."}

    random_num = random.randint(min_value, max_value)

    return {"random_number": random_num}


runpod.serverless.start({"handler": random_number_handler})
```

To test your code locally, create a file named `test_input.json` with the following content:

```json
{
  "input": {
    "min": 1,
    "max": 10
  }
}
```

Run the following command to test the script:

```command
python random_number_generator.py --rp_server_api
```

You should see output similar to this:

```
[secondary_label Output]
--- Starting Serverless Worker |  Version 1.6.2 ---
INFO   | Using test_input.json as job input.
DEBUG  | Retrieved local job: {'input': {'min': 1, 'max': 10}, 'id': 'local_test'}
INFO   | local_test | Started.
DEBUG  | local_test | Handler output: {'random_number': 7}
DEBUG  | local_test | run_job return: {'output': {'random_number': 7}}
INFO   | Job local_test completed successfully.
INFO   | Job result: {'output': {'random_number': 7}}
INFO   | Local testing complete, exiting.
```

The random number will vary each time you run the script.

## Error Handling

Our handler includes basic error handling to manage incorrect input:

1. If the input types are incorrect (not integers), it returns an error message.
2. If the minimum value is greater than or equal to the maximum value, it returns an error message.

These error messages will be part of the job output, allowing the client to address these issues appropriately.

## Conclusion

In this tutorial, you learned how to create a random number generator as a serverless application using the RunPod Python SDK. This included input validation and error handling to ensure robust performance. With this foundation, you can build more complex serverless applications and integrate AI tasks into your projects.

Excellent work on completing your random number generator! Up next, you'll explore how to integrate pre-trained AI models into your RunPod functions, taking your skills to the next level.

---

# 01-introduction.md

File: tutorials/sdks/python/get-started/01-introduction.md

Welcome to the world of Serverless AI development with the [RunPod Python SDK](https://github.com/runpod/runpod-python).

The RunPod Python SDK helps you develop Serverless AI applications so that you can build and deploy scalable AI solutions efficiently.

This set of tutorials will deepen your understanding of serverless principles and the practical knowledge to use the RunPod Python SDK in your AI applications.

## Prerequisites

To follow along with this guide, you should have:

- Basic programming knowledge in Python.
- An understanding of AI and machine learning concepts.
- [An account on the RunPod platform](https://www.runpod.io/console/signup).

## What is the RunPod Python SDK?

The [RunPod Python SDK](https://github.com/runpod/runpod-python) is a toolkit designed to facilitate the creation and deployment of serverless applications on the RunPod platform.

It is optimized for AI and machine learning workloads, simplifying the development of scalable, cloud-based AI applications.
The SDK allows you to define handler functions, conduct local testing, and utilize GPU support.

Acting as a bridge between your Python code and RunPod's cloud infrastructure, the SDK enables you to execute complex AI tasks without managing underlying hardware.

To start using RunPod Python SDK, see the [prerequisites](/tutorials/sdks/python/get-started/prerequisites) section or if, you're already setup proceed to the [Hello World](/tutorials/sdks/python/get-started/hello-world) tutorial, where we will guide you through creating, deploying, and running your first serverless AI application.

You can also see a library of complete RunPod samples in the [Worker library](https://github.com/runpod-workers) on GitHub.
These samples are complete Python libraries for common use cases.

## Learn more

Continue your journey by following our sequenced lessons designed to deepen your understanding and skills:

Here's a brief overview of each tutorial:

1. [Prerequisites and setup](/tutorials/sdks/python/get-started/prerequisites):
   - Installing Python and setting up a virtual environment
   - Installing the RunPod SDK
   - Configuring your RunPod account

2. [Hello World: Your first RunPod function](/tutorials/sdks/python/get-started/hello-world):
   - Creating a basic handler function
   - Understanding job input and output
   - Starting the serverless worker

3. [Running and testing locally](/tutorials/sdks/python/get-started/running-locally):
   - Testing with JSON input files
   - Interpreting local test output

4. [RunPod functions](/tutorials/sdks/python/101/hello):
   - Creating a basic handler function
   - Understanding job input and output
   - Starting the serverless worker
   - Testing with command-line arguments

5. [Using a Local Server](/tutorials/sdks/python/101/local-server-testing):
   - Setting up a local test server
   - Sending HTTP requests to your local function
   - Understanding server output and debugging
   - Comparing command-line and server-based testing

6. [Building a Generator Handler for Streaming Results](/tutorials/sdks/python/101/generator):
   - Understanding generator functions in RunPod's SDK
   - Creating a text-to-speech simulator with streaming output
   - Implementing a generator handler for incremental processing
   - Testing and debugging generator-based serverless functions

7. [Advanced Handler Techniques](/tutorials/sdks/python/101/async):
   - Synchronous vs asynchronous handlers
   - Using generator functions for streaming output
   - Handling multiple inputs and complex data structures

8. [Error Handling and Logging](/tutorials/sdks/python/101/error):
   - Implementing try-except blocks in handlers
   - Using RunPod's logging system
   - Best practices for error management in serverless functions

9. [Hugging Face Integration](/tutorials/sdks/python/102/huggingface-models):
   - Installing and importing external libraries
   - Loading and using a Hugging Face model
   - Optimizing model loading for serverless environments

10. [Stable Diffusion](/tutorials/sdks/python/102/stable-diffusion-text-to-image):
    - Setting up a text-to-image generation function
    - Handling larger inputs and outputs

Now, move on to the [prerequisites](/tutorials/sdks/python/get-started/prerequisites) and then set up [your first “Hello World”](/tutorials/sdks/python/get-started/hello-world) application with RunPod Python SDK.

---

# 01-prerequisites.md

File: tutorials/sdks/python/get-started/01-prerequisites.md

Setting up a proper development environment is fundamental to effectively building serverless AI applications using RunPod.
This guide will take you through each necessary step to prepare your system for RunPod development, ensuring you have the correct tools and configurations.

In this guide, you will learn how to the RunPod library.

When you're finished, you'll have a fully prepared environment to begin developing your serverless AI applications with RunPod.

## Prerequisites

Before beginning, ensure your system meets the following requirements:

- **Python 3.8 or later**: This is the programming language in which you'll be writing your RunPod applications.
- **Access to a terminal or command prompt**: This will be used to run various commands throughout this tutorial.

## Install Python

First, you need to have Python installed on your system.
Python is a programming language that's widely used in various types of software development and what is used to develop with the RunPod Python SDK.

To install Python, follow these steps:

1. Visit the [official Python website](https://www.python.org/downloads/).
2. Download the latest stable version of Python (version is 3.8 or later).
3. Follow the installation instructions for your operating system.

Once Python is installed, you can move onto setting up a virtual environment.

## Set up a virtual environment

Using a virtual environment is a best practice in Python development.

It keeps project dependencies isolated, avoiding conflicts between packages used in different projects.

Here’s how you can set up a virtual environment:

1. Open your terminal or command prompt.
2. Navigate to your project directory using the `cd` command. For example:
   ```command
   cd path/to/your/project
   ```
3. Create a virtual environment named `venv` by running the following command:
   ```command
   python -m venv venv
   ```

This command uses Python's built-in `venv` module to create a virtual environment.

4. Activate the virtual environment:
   - On Windows, use:
     ```command
     venv\Scripts\activate
     ```
   - On macOS and Linux, use:
     ```command
     source venv/bin/activate
     ```

Activating the virtual environment ensures that any Python packages you install will be confined to this environment.

You have now set up and activated a virtual environment for your project.
The next step is to install the RunPod library within this virtual environment.

## Install the RunPod Library

With the virtual environment activated, you need to install the RunPod Python SDK. This library provides the tools necessary to develop serverless applications on the RunPod platform.

To install the RunPod library, execute:

```command
pip install runpod
```

This command uses `pip`, Python's package installer, to download and install the latest version of the RunPod SDK.

## Verify the Installation

It's essential to confirm that the RunPod library has been installed correctly.
You can do this by running the following Python command:

```command
python -c "import runpod; print(runpod.__version__)"
```

If everything is set up correctly, this command will output the version number of the installed RunPod SDK.

For example:

```output
1.6.2
```

You have now successfully set up your development environment.
Your system is equipped with Python, a virtual environment, and the RunPod library.

You will use the RunPod Python library for writing your serverless application.

Next, we'll proceed with creating a [Hello World application with RunPod](/tutorials/sdks/python/get-started/hello-world).

---

# 02-hello-world.md

File: tutorials/sdks/python/get-started/02-hello-world.md

Let's dive into creating your first RunPod Serverless application.
We're going to build a "Hello, World!" program that greets users with a custom message.
Don't worry about sending requests just yet - we'll cover that in the next tutorial, [running locally](/tutorials/sdks/python/get-started/running-locally).

This exercise will introduce you to the key parts of a RunPod application, giving you a solid foundation in serverless functions.
By the end, you'll have your very own RunPod serverless function up and running locally.

### Creating Your First Serverless Function

Let's write a Python script that defines a simple serverless function.
This function will say `Hello, World!`.

Create a new file called `hello_world.py` in your text editor and add the following code:

```python
import runpod


def handler(job):
    job_input = job["input"]

    return f"Hello {job_input['name']}!"


runpod.serverless.start({"handler": handler})
```

Let's break this down:

We start by importing the `runpod` library.
This gives us all the tools we need for creating and managing serverless applications.

Next, we define our `handler` function.
This function processes incoming requests.
It takes a `job` parameter, which contains all the info about the incoming job.

Inside the handler, we grab the input data from the job.
We're expecting a 'name' field in the input.

Then we create and return our greeting message, using the name we got from the input.

Finally, we call `runpod.serverless.start()`, telling it to use our `handler` function.
This kicks off the serverless worker and gets it ready to handle incoming jobs.

And there you have it!
You've just created your first RunPod serverless function.
It takes in a request with a name and returns a personalized greeting.

### Key Takeaways

- RunPod functions are built around a handler that processes incoming jobs.
- You can easily access input data from the job parameter.
- The `runpod.serverless.start()` function gets your serverless worker up and running.

## Next steps

You've now got a basic `Hello, World!` RunPod serverless function up and running.
You've learned how to handle input and output in a serverless environment and how to start your application.

These are the building blocks for creating more complex serverless applications with RunPod. As you get more comfortable with these concepts, you'll be able to create even more powerful and flexible serverless functions.

---

# 03-running-locally.md

File: tutorials/sdks/python/get-started/03-running-locally.md

Before deploying your serverless functions to the cloud, it's crucial to test them locally.
In the previous lesson, [Hello World with RunPod](/tutorials/sdks/python/get-started/hello-world), you created a Python file called `hello_world.py`.

In this guide, you'll learn how to run your RunPod serverless applications on your local machine using the RunPod Python SDK.

## Understanding RunPod's Local Testing Environment

When you run your code locally using the RunPod Python SDK, here's what happens behind the scenes:

- FastAPI Server: The SDK spins up a FastAPI server on your local machine. This server simulates the RunPod serverless environment.
- Request Handling: The FastAPI server receives and processes requests just like the cloud version would, allowing you to test your function's input handling and output generation.
- Environment Simulation: The local setup mimics key aspects of the RunPod serverless environment, helping ensure your code will behave similarly when deployed.

## Running Your Code Locally

Let's walk through how to run your serverless functions locally using the RunPod Python SDK.

**Options for Passing Information to Your API**

The RunPod Python SDK offers two main methods for sending data to your local FastAPI server:

1. Using a JSON file
2. Using inline JSON via command line

Both methods allow you to simulate how your function would receive data in the actual cloud environment.

### Using a JSON File

1. Create a JSON file:

   Create a file called `test_input.json` with your test data:

   ```json
   {
     "input": {
       "name": "World"
     }
   }
   ```

2. Run the serverless function:

   Execute your `hello_world.py` script with the `--rp_server_api` flag:

   ```bash
   python hello_world.py --rp_server_api
   ```

   The SDK will automatically look for and use the `test_input.json` file in the current directory.

### Using Inline JSON

You can also pass your test data directly via the command line:

```bash
python hello_world.py --test_input '{"input": {"name": "World"}}'
```

This method is useful for quick tests or when you want to vary the input without editing a file.

### Understanding the output

When you run your function locally, you'll see output similar to this:

```
--- Starting Serverless Worker |  Version 1.6.2 ---
INFO   | Using test_input.json as job input.
DEBUG  | Retrieved local job: {'input': {'name': 'World'}, 'id': 'local_test'}
INFO   | local_test | Started.
DEBUG  | local_test | Handler output: Hello World!
DEBUG  | local_test | run_job return: {'output': 'Hello World!'}
INFO   | Job local_test completed successfully.
INFO   | Job result: {'output': 'Hello World!'}
INFO   | Local testing complete, exiting.
```

This output provides valuable information:

- Confirmation that the Serverless Worker started successfully
- Details about the input data being used
- Step-by-step execution of your function
- The final output and job status

By analyzing this output, you can verify that your function is behaving as expected and debug any issues that arise.

### Key Takeaways

- Local testing with the RunPod Python SDK allows you to simulate the cloud environment on your machine.
- The SDK creates a FastAPI server to mock the serverless function execution.
- You can provide input data via a JSON file or inline JSON in the command line.
- Local testing accelerates development, reduces costs, and helps catch issues early.

Next, we'll explore the structure of RunPod handlers in more depth, enabling you to create more sophisticated serverless functions.

---

# run-ollama-inference.md

File: tutorials/serverless/cpu/run-ollama-inference.md

In this guide, you will learn how to run an Ollama server on your RunPod CPU for inference.
Although this tutorial focuses on CPU compute, you can also select a GPU type and follow the same steps.
By the end of this tutorial, you will have a fully functioning Ollama server ready to handle requests.

## Setting up your Endpoint

:::note

Use a [Network volume](/pods/storage/create-network-volumes) to attach to your Worker so that it can cache the LLM and decrease cold start times.
If you do not use a network volume, the Worker will have to download the model every time it spins back up, leading to increased latency and resource consumption.

:::

To begin, you need to set up a new endpoint on RunPod.

1. Log in to your [RunPod account](https://www.runpod.io/console/console/home).
2. Navigate to the **Serverless** section and select **New Endpoint**.
3. Choose **CPU** and provide a name for your Endpoint, for example 8 vCPUs 16 GB RAM.
4. Configure your Worker settings according to your needs.
5. In the **Container Image** field, enter the `pooyaharatian/runpod-ollama:0.0.8` container image.
6. In the **Container Start Command** field, specify the [Ollama supported model](https://ollama.com/library), such as `orca-mini` or `llama3.1`.
7. Allocate sufficient container disk space for your model. Typically, 20 GB should suffice for most models.
8. (optional) In **Environment Variables**, set a new key to `OLLAMA_MODELS` and its value to `/runpod-volume`. This will allow the model to be stored to your attached volume.
9. Click **Deploy** to initiate the setup.

Your model will start downloading. Once the Worker is ready, proceed to the next step.

## Sending a Run request

After your endpoint is deployed and the model is downloaded, you can send a run request to test the setup.

1. Go to the **Requests** section in the RunPod web UI.
2. In the input module, enter the following JSON object:

   ```json
   {
     "input": {
       "method_name": "generate",
       "input": {
         "prompt": "why the sky is blue?"
       }
     }
   }
   ```

3. Select **Run** to execute the request.
4. In a few seconds, you will receive a response. For example:

   ```json
   {
     "delayTime": 153,
     "executionTime": 4343,
     "id": "c2cb6af5-c822-4950-bca9-5349288c001d-u1",
     "output": {
       "context": [
         "omitted for brevity"
       ],
       "created_at": "2024-05-17T16:56:29.256938735Z",
       "done": true,
       "eval_count": 118,
       "eval_duration": 807433000,
       "load_duration": 3403140284,
       "model": "orca-mini",
       "prompt_eval_count": 46,
       "prompt_eval_duration": 38548000,
       "response": "The sky appears blue because of a process called scattering. When sunlight enters the Earth's atmosphere, it encounters molecules of air such as nitrogen and oxygen. These molecules scatter the light in all directions, but they scatter the shorter wavelengths of light (such as violet and blue) more than the longer wavelengths (such as red). This creates a reddish-orange sky that is less intense on the horizon than on the observer's position. As the sun gets lower in the sky, the amount of scattering increases and the sky appears to get brighter.",
       "total_duration": 4249684714
     },
     "status": "COMPLETED"
   }
   ```

With your Endpoint set up, you can now integrate it into your application just like any other request.

## Conclusion

In this tutorial, you have successfully set up and run an Ollama server on a RunPod CPU.
Now you can handle inference requests using your deployed model.

For further exploration, check out the following resources:

- [Runpod Ollama repository](https://github.com/pooyahrtn/)
- [RunPod Ollama container image](https://hub.docker.com/r/pooyaharatian/runpod-ollama)

---

# generate-sdxl-turbo.md

File: tutorials/serverless/gpu/generate-sdxl-turbo.md


When it comes to working with an AI image generator, the speed in which images are generated is often a compromise.
RunPod's Serverless Workers allows you to host [SDXL Turbo](https://huggingface.co/stabilityai/sdxl-turbo) from Stability AI, which is a fast text-to-image model.

In this tutorial, you'll build a web application, where you'll leverage RunPod's Serverless Worker and Endpoint to return an image from a text-based input.

By the end of this tutorial, you'll have an understanding of running a Serverless Worker on RunPod and sending requests to an Endpoint to receive a response.

You can proceed with the tutorial by following the build steps outlined here or skip directly to [Deploy a Serverless Endpoint](#deploy-a-serverless-endpoint) section.

## Prerequisites

This section presumes you have an understanding of the terminal and can execute commands from your terminal.

Before starting this tutorial, you'll need access to:

### RunPod

To continue with this quick start, you'll need access to the following from RunPod:

- RunPod account
- RunPod API Key

### Docker

To build your Docker image, you'll need access to the following:

- Docker installed
- Docker account

You can also use the prebuilt image from [runpod/sdxl-turbo](https://hub.docker.com/r/runpod/sdxl-turbo).

### GitHub

To clone the `worker-sdxl-turbo` repo, you'll need access to the following:

- Git installed
- Permissions to clone GitHub repos

With the prerequisites covered, get started by building and pushing a Docker image to a container registry.

## Build and push your Docker image

This step will walk you through building and pushing your Docker image to your container registry.
This is useful to building custom images for your use case.
If you prefer, you can use the prebuilt image from [runpod/sdxl-turbo](https://hub.docker.com/r/runpod/sdxl-turbo) instead of building your own.

Building a Docker image allows you to specify the container when creating a Worker.
The Docker image includes the [RunPod Handler](https://github.com/runpod-workers/worker-sdxl-turbo/blob/main/src/handler.py) which is how you provide instructions to Worker to perform some task.
In this example, the Handler is responsible for taking a Job and returning a base 64 instance of the image.

1. Clone the [RunPod Worker SDXL Turbo](https://github.com/runpod-workers/worker-sdxl-turbo) repository:

```command
gh repo clone runpod-workers/worker-sdxl-turbo
```

2. Navigate to the root of the cloned repo:

```command
cd worker-sdxl-turbo
```

3. Build the Docker image:

```command
docker build --tag <username>/<repo>:<tag> .
```

4. Push your container registry:

```command
docker push <username>/<repo>:<tag>
```

Now that you've pushed your container registry, you're ready to deploy your Serverless Endpoint to RunPod.

## Deploy a Serverless Endpoint

The container you just built will run on the Worker you're creating.
Here, you will configure and deploy the Endpoint.
This will include the GPU and the storage needed for your Worker.

This step will walk you through deploying a Serverless Endpoint to RunPod.

1. Log in to the [RunPod Serverless console](https://www.runpod.io/console/serverless).
2. Select **+ New Endpoint**.
3. Provide the following:
   1. Endpoint name.
   2. Select a GPU.
   3. Configure the number of Workers.
   4. (optional) Select **FlashBoot**.
   5. (optional) Select a template.
   6. Enter the name of your Docker image.
      - For example, `runpod/sdxl-turbo:dev`.
   7. Specify enough memory for your Docker image.
4. Select **Deploy**.

Now, let's send a request to your Endpoint.

## Send a request

Now that our Endpoint is deployed, you can begin interacting with and integrating it into an application.
Before writing the logic into the application, ensure that you can interact with the Endpoint by sending a request.

Run the following command:

<Tabs>
```bash
```
```json
```
```html
```
```javascript
```
    ```command
    ```

---

# run-gemma-7b.md

File: tutorials/serverless/gpu/run-gemma-7b.md

This tutorial walks you through running Google's Gemma model using RunPod's vLLM Worker.
Throughout this tutorial, you'll learn to set up a Serverless Endpoint with a gated large language model (LLM).

## Prerequisites

Before diving into the deployment process, gather the necessary tokens and accepting Google's terms.
This step ensures that you have access to the model and are in compliance with usage policies.

- [Hugging Face access token](https://huggingface.co/settings/tokens)
- [Accepting Google's terms of service](https://huggingface.co/google/gemma-7b)

The next section will guide you through Setting up your Serverless Endpoint with RunPod.

## Get started

To begin, we'll deploy a vLLM Worker as a Serverless Endpoint.
RunPod simplifies the process of running large language models, offering an alternative to the more complex Docker and Kubernetes deployment methods.

Follow these steps in the RunPod Serverless console to create your Endpoint.

1. Log in to the [RunPod Serverless console](https://www.runpod.io/console/serverless).
2. Select **+ New Endpoint**.
3. Provide the following:
   1. Endpoint name.
   2. Select a GPU.
   3. Configure the number of Workers.
   4. (optional) Select **FlashBoot**.
   5. Enter the vLLM Worker image: `runpod/worker-vllm:stable-cuda11.8.0` or `runpod/worker-vllm:stable-cuda12.1.0`.
   6. Specify enough storage for your model.
   7. Add the following environment variables:
      1. `MODEL_NAME`: `google/gemma-7b-it`.
      2. `HF_TOKEN`: your Hugging Face API token for private models.
4. Select **Deploy**.

Once the Endpoint initializes, you can send a request to your [Endpoint](/serverless/endpoints/get-started).
You've now successfully deployed your model, a significant milestone in utilizing Google's Gemma model.
As we move forward, the next section will focus on interacting with your model.

## Interact with your model

With the Endpoint up and running, it's time to leverage its capabilities by sending requests to interact with the model.
This section demonstrates how to use OpenAI APIs to communicate with your model.

In this example, you'll create a Python chat bot using the `OpenAI` library; however, you can use any programming language and any library that supports HTTP requests.

Here's how to get started:

Use the `OpenAI` class to interact with the model. The `OpenAI` class takes the following parameters:

- `base_url`: The base URL of the Serverless Endpoint.
- `api_key`: Your RunPod API key.

```python
from openai import OpenAI
import os

client = OpenAI(
    base_url=os.environ.get("RUNPOD_BASE_URL"),
    api_key=os.environ.get("RUNPOD_API_KEY"),
)
```

:::note

Set your environment variables `RUNPOD_BASE_URL` and `RUNPOD_API_KEY` to your RunPod API key and base URL.
Your `RUNPOD_BASE_URL` will be in the form of:

```bash
https://api.runpod.ai/v2/${RUNPOD_ENDPOINT_ID}/openai/v1
```

Where `${RUNPOD_ENDPOINT_ID}` is the ID of your Serverless Endpoint.

:::

Next, you can use the `client` to interact with the model. For example, you can use the `chat.completions.create` method to generate a response from the model.

Provide the following parameters to the `chat.completions.create` method:

- `model`: `The model name`.
- `messages`: A list of messages to send to the model.
- `max_tokens`: The maximum number of tokens to generate.
- `temperature`: The randomness of the generated text.
- `top_p`: The cumulative probability of the generated text.
- `max_tokens`: The maximum number of tokens to generate.

```python
messages = [
    {
        "role": "assistant",
        "content": "Hello, I'm your assistant. How can I help you today?",
    }
]


def display_chat_history(messages):
    for message in messages:
        print(f"{message['role'].capitalize()}: {message['content']}")


def get_assistant_response(messages):
    r = client.chat.completions.create(
        model="google/gemma-7b-it",
        messages=[{"role": m["role"], "content": m["content"]} for m in messages],
        temperature=0.7,
        top_p=0.8,
        max_tokens=100,
    )
    response = r.choices[0].message.content
    return response


while True:
    display_chat_history(messages)

    prompt = input("User: ")
    messages.append({"role": "user", "content": prompt})

    response = get_assistant_response(messages)
    messages.append({"role": "assistant", "content": response})
```

Congratulations!
You've successfully set up a Serverless Endpoint and interacted with Google's Gemma model.
This tutorial has shown you the essentials of deploying a model on RunPod and creating a simple application to communicate with it.
You've taken important steps towards integrating large language models into your projects.

---

# run-your-first.md

File: tutorials/serverless/gpu/run-your-first.md

:::note

Before we begin, ensure you have a RunPod API key, available under your user settings. This key is crucial for identification and billing purposes. Keep it secure! Also, remember to retrieve your results via the status endpoint within 30 minutes, as your inputs and outputs are not stored longer than this for privacy protection.

:::

### Overview

In this section, we'll explore how RunPod's API works. It's asynchronous, meaning that when you send a request, you get a job ID back almost instantly. Next, we'll show you how to use this job ID to check the status and retrieve your results.

Let's dive into an example using the Stable Diffusion v1 inference endpoint.

## Create a serverless worker

First, let's set up your serverless worker. Begin by selecting **Quick Deploy** on the RunPod interface. Then choose **Start** from the **Stable Diffusion v1.5** options. Pick a GPU, say a 24 GB GPU, and click **Deploy**. Here’s an example endpoint you might use: `https://api.runpod.ai/v2/{ID}/runsync`

### Start Your Job

Now, to initiate a job, you'll make a request like the one shown below. This sends your parameters to the API and starts the process.

```curl
curl -X POST https://api.runpod.ai/v2/{ID}/run \
    -H 'Content-Type: application/json'                             \
    -H 'Authorization: Bearer [Your API Key]'    \
    -d '{"input": {"prompt": "A cute fluffy white dog in the style of a Pixar animation 3D drawing."}}'
```

Upon doing this, you'll receive a response like this, containing your unique job ID:

```json
{
  "id": "c80ffee4-f315-4e25-a146-0f3d98cf024b",
  "status": "IN_QUEUE"
}
```

### Check the Status of Your Job

Since your initial response doesn't include the output, a subsequent call is necessary. Use your job ID to check the job's status as follows:

```curl
curl https://api.runpod.ai/v2/{ID}/status/c80ffee4-f315-4e25-a146-0f3d98cf024b \
-H 'Content-Type: application/json' \
-H 'Authorization: Bearer [Your API Key]'
```

If your job is still processing, the response will indicate that. Here's an example:

```json
{
  "delayTime": 2624,
  "id": "c80ffee4-f315-4e25-a146-0f3d98cf024b",
  "input": {
    "prompt": "A cute fluffy white dog in the style of a Pixar animation 3D drawing."
  },
  "status": "IN_PROGRESS"
}
```

### Get Completed Job Status

Once your job is complete, you'll receive a final response like this:

```json
{
  "delayTime": 17158,
  "executionTime": 4633,
  "id": "fb5a249d-12c7-48e5-a0e4-b813c3381262-22",
  "output": [
    {
      "image": "base64image",
      "seed": 40264
    }
  ],
  "status": "COMPLETED"
}
```

To save the output, use the following command:

```json
curl https://api.runpod.ai/v2/{ID}/status/c80ffee4-f315-4e25-a146-0f3d98cf024b \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer [Your API Key]' | jq . > output.json
```

:::note

Remember, you have up to 1 hour to retrieve your results via the status endpoint for privacy reasons.

:::

### Get Your Results

Finally, to view your results, decode the base64 image from the output. Here's how you can do it in Python:

```python
import json
import base64


def decode_and_save_image(json_file_path, output_image_path):
    try:
        # Reading the JSON file
        with open(json_file_path, "r") as file:
            data = json.load(file)

        # Extracting the base64 encoded image data
        base64_image = data["output"][0]["image"]

        # Decode the Base64 string
        decoded_image_data = base64.b64decode(base64_image)

        # Writing the decoded data to an image file
        with open(output_image_path, "wb") as image_file:
            image_file.write(decoded_image_data)

        print(f"Image successfully decoded and saved as '{output_image_path}'.")

    except FileNotFoundError:
        print(
            "File not found. Please ensure the JSON file exists in the specified path."
        )
    except KeyError as e:
        print(f"Error in JSON structure: {e}")
    except Exception as e:
        print(f"An error occurred: {e}")


# Usage
json_file_path = "output.json"  # Path to your JSON file
output_image_path = "decoded_image.png"  # Desired path for the output image

decode_and_save_image(json_file_path, output_image_path)
```

Congratulations! You've now successfully used RunPod's Stable Diffusion API to generate images.


---

